{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load dataset\n",
    "housingPrice = pd.read_excel(open('1553768847_housing.xlsx', 'rb'), sheet_name='housing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      "longitude             20640 non-null float64\n",
      "latitude              20640 non-null float64\n",
      "housing_median_age    20640 non-null int64\n",
      "total_rooms           20640 non-null int64\n",
      "total_bedrooms        20433 non-null float64\n",
      "population            20640 non-null int64\n",
      "households            20640 non-null int64\n",
      "median_income         20640 non-null float64\n",
      "ocean_proximity       20640 non-null object\n",
      "median_house_value    20640 non-null int64\n",
      "dtypes: float64(4), int64(5), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "housingPrice.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41</td>\n",
       "      <td>880</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322</td>\n",
       "      <td>126</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>452600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21</td>\n",
       "      <td>7099</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401</td>\n",
       "      <td>1138</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>358500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52</td>\n",
       "      <td>1467</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496</td>\n",
       "      <td>177</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>352100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52</td>\n",
       "      <td>1274</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558</td>\n",
       "      <td>219</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>341300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52</td>\n",
       "      <td>1627</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565</td>\n",
       "      <td>259</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>342200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                  41          880           129.0   \n",
       "1    -122.22     37.86                  21         7099          1106.0   \n",
       "2    -122.24     37.85                  52         1467           190.0   \n",
       "3    -122.25     37.85                  52         1274           235.0   \n",
       "4    -122.25     37.85                  52         1627           280.0   \n",
       "\n",
       "   population  households  median_income ocean_proximity  median_house_value  \n",
       "0         322         126         8.3252        NEAR BAY              452600  \n",
       "1        2401        1138         8.3014        NEAR BAY              358500  \n",
       "2         496         177         7.2574        NEAR BAY              352100  \n",
       "3         558         219         5.6431        NEAR BAY              341300  \n",
       "4         565         259         3.8462        NEAR BAY              342200  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print first few rows of this data.\n",
    "housingPrice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract features, label data from the dataset.\n",
    "features = housingPrice.iloc[:,:-1].values\n",
    "label = housingPrice.iloc[:,-1].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "bedroomsImpute = Imputer(missing_values=np.nan, strategy='mean', axis=0)\n",
    "features[:,[4]] = bedroomsImpute.fit_transform(features[:,[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 9 columns):\n",
      "0    20640 non-null object\n",
      "1    20640 non-null object\n",
      "2    20640 non-null object\n",
      "3    20640 non-null object\n",
      "4    20640 non-null object\n",
      "5    20640 non-null object\n",
      "6    20640 non-null object\n",
      "7    20640 non-null object\n",
      "8    20640 non-null object\n",
      "dtypes: object(9)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(features).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Encoding Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oceanEncoder = LabelEncoder()\n",
    "features[:,-1] = oceanEncoder.fit_transform(features[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oceanOHE = OneHotEncoder(categorical_features=[-1])\n",
    "features = oceanOHE.fit_transform(features).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4       5      6     7       8       9       10  \\\n",
       "0  0.0  0.0  0.0  1.0  0.0 -122.23  37.88  41.0   880.0   129.0   322.0   \n",
       "1  0.0  0.0  0.0  1.0  0.0 -122.22  37.86  21.0  7099.0  1106.0  2401.0   \n",
       "2  0.0  0.0  0.0  1.0  0.0 -122.24  37.85  52.0  1467.0   190.0   496.0   \n",
       "3  0.0  0.0  0.0  1.0  0.0 -122.25  37.85  52.0  1274.0   235.0   558.0   \n",
       "4  0.0  0.0  0.0  1.0  0.0 -122.25  37.85  52.0  1627.0   280.0   565.0   \n",
       "\n",
       "       11      12  \n",
       "0   126.0  8.3252  \n",
       "1  1138.0  8.3014  \n",
       "2   177.0  7.2574  \n",
       "3   219.0  5.6431  \n",
       "4   259.0  3.8462  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(features).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "                                                   label,\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.89115574, -0.68188905, -0.01556621, ..., -0.9744286 ,\n",
       "        -0.97703285,  2.34476576],\n",
       "       [-0.89115574, -0.68188905, -0.01556621, ...,  0.86143887,\n",
       "         1.66996103,  2.33223796],\n",
       "       [-0.89115574, -0.68188905, -0.01556621, ..., -0.82077735,\n",
       "        -0.84363692,  1.7826994 ],\n",
       "       ..., \n",
       "       [-0.89115574,  1.46651424, -0.01556621, ..., -0.3695372 ,\n",
       "        -0.17404163, -1.14259331],\n",
       "       [-0.89115574,  1.46651424, -0.01556621, ..., -0.60442933,\n",
       "        -0.39375258, -1.05458292],\n",
       "       [-0.89115574,  1.46651424, -0.01556621, ..., -0.03397701,\n",
       "         0.07967221, -0.78012947]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scScale = StandardScaler()\n",
    "ScaleStandardFeatures = scScale.fit_transform(features)\n",
    "ScaleStandardFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Perform Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.645027355367\n",
      "0.646447357609\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6531291879010583 Training: 0.6435167106972498 Seed: 3\n",
      "Testing: 0.6508315183818119 Training: 0.6439781856640889 Seed: 5\n",
      "Testing: 0.6547608901187603 Training: 0.6430210799659366 Seed: 8\n",
      "Testing: 0.6464473576092475 Training: 0.6450273553666998 Seed: 10\n",
      "Testing: 0.6479363974157099 Training: 0.6445968066973219 Seed: 11\n",
      "Testing: 0.6471315876091026 Training: 0.6449429894053635 Seed: 13\n",
      "Testing: 0.6513141582497535 Training: 0.6439543181681746 Seed: 22\n",
      "Testing: 0.6586265429816253 Training: 0.6419789625523029 Seed: 23\n",
      "Testing: 0.6576705542720351 Training: 0.6423483228072302 Seed: 24\n",
      "Testing: 0.6476653670230617 Training: 0.6448475924603088 Seed: 25\n",
      "Testing: 0.65182758436871 Training: 0.6438450103979694 Seed: 26\n",
      "Testing: 0.64858057844461 Training: 0.6446115524787028 Seed: 27\n",
      "Testing: 0.665587653009948 Training: 0.6400466833302726 Seed: 28\n",
      "Testing: 0.6470592926871662 Training: 0.6449480088829391 Seed: 29\n",
      "Testing: 0.6501438091345385 Training: 0.6438244198949823 Seed: 33\n",
      "Testing: 0.6609518550141411 Training: 0.6414278941825013 Seed: 34\n",
      "Testing: 0.6583268777872184 Training: 0.6422264810419899 Seed: 37\n",
      "Testing: 0.6546561167009485 Training: 0.6429770402245012 Seed: 38\n",
      "Testing: 0.6493382897153364 Training: 0.6444062796146276 Seed: 39\n",
      "Testing: 0.6528491497071752 Training: 0.6433224735396105 Seed: 40\n",
      "Testing: 0.6617825894767307 Training: 0.641118355533516 Seed: 44\n",
      "Testing: 0.6464957311728591 Training: 0.6451262194012624 Seed: 46\n",
      "Testing: 0.6506719335164637 Training: 0.6440054772633798 Seed: 48\n",
      "Testing: 0.6513260212078105 Training: 0.6437833113770046 Seed: 51\n",
      "Testing: 0.6600008720684972 Training: 0.6416492559589482 Seed: 59\n",
      "Testing: 0.6521600309503025 Training: 0.6435579516402415 Seed: 60\n",
      "Testing: 0.6596534220297728 Training: 0.6418607915692796 Seed: 61\n",
      "Testing: 0.6450949684179386 Training: 0.6447608262113688 Seed: 63\n",
      "Testing: 0.6486111671478875 Training: 0.6444889410316126 Seed: 64\n",
      "Testing: 0.6499917743937614 Training: 0.6438012522737229 Seed: 66\n",
      "Testing: 0.6626517452684824 Training: 0.6409148462837093 Seed: 70\n",
      "Testing: 0.6580254637308836 Training: 0.6421962010694839 Seed: 73\n",
      "Testing: 0.6485329069294229 Training: 0.6444871244487667 Seed: 74\n",
      "Testing: 0.6510199663901701 Training: 0.643990380242573 Seed: 78\n",
      "Testing: 0.6492663585461805 Training: 0.6443456459545939 Seed: 81\n",
      "Testing: 0.6513277092940514 Training: 0.6439236446070626 Seed: 82\n",
      "Testing: 0.6546440772125169 Training: 0.6429069682806421 Seed: 84\n",
      "Testing: 0.6505766096476095 Training: 0.6439397281859927 Seed: 87\n",
      "Testing: 0.6637168997327942 Training: 0.6406861867815867 Seed: 90\n",
      "Testing: 0.649669377981146 Training: 0.6441612346680652 Seed: 92\n",
      "Testing: 0.6502952979589393 Training: 0.6440901469370351 Seed: 93\n",
      "Testing: 0.6559411902650868 Training: 0.6426769233221794 Seed: 94\n",
      "Testing: 0.6480513177232086 Training: 0.6446503531877394 Seed: 95\n",
      "Testing: 0.6478896334869962 Training: 0.6446533870501251 Seed: 99\n",
      "Testing: 0.6653573565606985 Training: 0.6403184439877669 Seed: 100\n",
      "Testing: 0.6465981154221891 Training: 0.644742469762167 Seed: 104\n",
      "Testing: 0.6496031634067859 Training: 0.6443215808207324 Seed: 109\n",
      "Testing: 0.6614601581113368 Training: 0.6415055547666876 Seed: 110\n",
      "Testing: 0.651926844120179 Training: 0.6436754902542217 Seed: 112\n",
      "Testing: 0.6479879404455787 Training: 0.6447743220423253 Seed: 114\n",
      "Testing: 0.6590353814365769 Training: 0.6417361786836255 Seed: 118\n",
      "Testing: 0.6509877447625072 Training: 0.6439060300967362 Seed: 120\n",
      "Testing: 0.6647759508365446 Training: 0.6405427335523022 Seed: 121\n",
      "Testing: 0.6523126190653128 Training: 0.6431709379213502 Seed: 122\n",
      "Testing: 0.6510980056868192 Training: 0.6439643061002306 Seed: 125\n",
      "Testing: 0.6506807871596696 Training: 0.643962050727273 Seed: 127\n",
      "Testing: 0.662415872287124 Training: 0.6412747782460824 Seed: 132\n",
      "Testing: 0.657930358540564 Training: 0.6422010586463154 Seed: 133\n",
      "Testing: 0.6536636862409719 Training: 0.642997215641652 Seed: 135\n",
      "Testing: 0.6514003253055689 Training: 0.643891120673575 Seed: 139\n",
      "Testing: 0.647553699512068 Training: 0.6448635412149413 Seed: 144\n",
      "Testing: 0.6500295493786942 Training: 0.6442051864914399 Seed: 145\n",
      "Testing: 0.6619797582794099 Training: 0.640641732372328 Seed: 146\n",
      "Testing: 0.6469905219808435 Training: 0.6450178162681651 Seed: 151\n",
      "Testing: 0.655114271929887 Training: 0.6428795003454455 Seed: 152\n",
      "Testing: 0.6593016032014047 Training: 0.6418758742422745 Seed: 154\n",
      "Testing: 0.6461330061671375 Training: 0.645179023226309 Seed: 155\n",
      "Testing: 0.6469890430605 Training: 0.6449872965664496 Seed: 157\n",
      "Testing: 0.6550450950380338 Training: 0.6429370669385255 Seed: 159\n",
      "Testing: 0.6496297766059829 Training: 0.6443147826535945 Seed: 160\n",
      "Testing: 0.6533780730629322 Training: 0.6433919138432229 Seed: 161\n",
      "Testing: 0.6515340747065109 Training: 0.6439062111738417 Seed: 162\n",
      "Testing: 0.6487008183689782 Training: 0.6440677432959907 Seed: 163\n",
      "Testing: 0.6553976314480179 Training: 0.6427693853970305 Seed: 167\n",
      "Testing: 0.6628318557824104 Training: 0.6409580332524087 Seed: 168\n",
      "Testing: 0.6485237955687233 Training: 0.6445894868856412 Seed: 171\n",
      "Testing: 0.6630700445013769 Training: 0.641113028799137 Seed: 172\n",
      "Testing: 0.6593609211449766 Training: 0.6418428508346915 Seed: 173\n",
      "Testing: 0.6598387825278671 Training: 0.6416712364830451 Seed: 176\n",
      "Testing: 0.6476298758442856 Training: 0.6448182299083461 Seed: 177\n",
      "Testing: 0.65681876154475 Training: 0.6424737265627259 Seed: 186\n",
      "Testing: 0.6468966090566152 Training: 0.6446706796723927 Seed: 191\n",
      "Testing: 0.645146522763432 Training: 0.6448538501909039 Seed: 194\n",
      "Testing: 0.6474364774243666 Training: 0.6448158749714415 Seed: 197\n",
      "Testing: 0.6503581978584044 Training: 0.6440951510743701 Seed: 202\n",
      "Testing: 0.6545071480402224 Training: 0.6431300177622997 Seed: 205\n",
      "Testing: 0.6524145838675437 Training: 0.6435528829018408 Seed: 206\n",
      "Testing: 0.6511906385061597 Training: 0.643920113042909 Seed: 209\n",
      "Testing: 0.6585400591061847 Training: 0.6420320880673038 Seed: 210\n",
      "Testing: 0.659260028334576 Training: 0.6420021240932957 Seed: 212\n",
      "Testing: 0.6483841384805741 Training: 0.6446937469558176 Seed: 214\n",
      "Testing: 0.6525091175289686 Training: 0.6435265554171787 Seed: 215\n",
      "Testing: 0.6541396242573608 Training: 0.6430772458965333 Seed: 217\n",
      "Testing: 0.652325885568627 Training: 0.6437214936493788 Seed: 218\n",
      "Testing: 0.6545604213764535 Training: 0.6429943943118696 Seed: 219\n",
      "Testing: 0.6483227064863607 Training: 0.644675837361014 Seed: 220\n",
      "Testing: 0.6579356140490449 Training: 0.6420870904920004 Seed: 221\n",
      "Testing: 0.6465115069624481 Training: 0.6451426521906368 Seed: 222\n",
      "Testing: 0.6620434433083778 Training: 0.6412255853903033 Seed: 225\n",
      "Testing: 0.6494892231934619 Training: 0.6444077644653381 Seed: 226\n",
      "Testing: 0.6534310658946628 Training: 0.6431768129228008 Seed: 227\n",
      "Testing: 0.6610450874054481 Training: 0.641265496642557 Seed: 231\n",
      "Testing: 0.652412820186421 Training: 0.643573481920402 Seed: 234\n",
      "Testing: 0.6526944901179693 Training: 0.6434400069587431 Seed: 236\n",
      "Testing: 0.6540388933303745 Training: 0.6432241107002317 Seed: 240\n",
      "Testing: 0.6467804490888771 Training: 0.6450806779325322 Seed: 241\n",
      "Testing: 0.6502441529164789 Training: 0.6441046725156357 Seed: 244\n",
      "Testing: 0.6511252242821557 Training: 0.6440169612712676 Seed: 246\n",
      "Testing: 0.6487180761430376 Training: 0.644308291131325 Seed: 252\n",
      "Testing: 0.6570296805731831 Training: 0.6423188005047901 Seed: 254\n",
      "Testing: 0.6516070098466745 Training: 0.64382220851018 Seed: 256\n",
      "Testing: 0.6561672110555954 Training: 0.6426045219888967 Seed: 258\n",
      "Testing: 0.6486594977072369 Training: 0.6445456787035262 Seed: 259\n",
      "Testing: 0.6553759560009222 Training: 0.6427862618169269 Seed: 260\n",
      "Testing: 0.6461624472325115 Training: 0.645162564943319 Seed: 261\n",
      "Testing: 0.647951652023885 Training: 0.6447830896842115 Seed: 262\n",
      "Testing: 0.6634712145573959 Training: 0.6407398650014997 Seed: 263\n",
      "Testing: 0.6507637565864516 Training: 0.6440052137563907 Seed: 264\n",
      "Testing: 0.6488191063201912 Training: 0.6443331476983589 Seed: 266\n",
      "Testing: 0.6467834371756249 Training: 0.6450637589357711 Seed: 267\n",
      "Testing: 0.6482877154507762 Training: 0.6446458190929218 Seed: 268\n",
      "Testing: 0.652804741084743 Training: 0.6434924346758422 Seed: 269\n",
      "Testing: 0.6507825772599353 Training: 0.6440995749711115 Seed: 271\n",
      "Testing: 0.6461066490180649 Training: 0.6451973466364413 Seed: 272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6471695219402012 Training: 0.6448396947635954 Seed: 274\n",
      "Testing: 0.6608453727963957 Training: 0.6413802406302164 Seed: 275\n",
      "Testing: 0.6596724965763502 Training: 0.641863751462137 Seed: 276\n",
      "Testing: 0.6491181293795645 Training: 0.6444983294944923 Seed: 279\n",
      "Testing: 0.6545329979224945 Training: 0.642938148478768 Seed: 281\n",
      "Testing: 0.6603385614197836 Training: 0.641565999782445 Seed: 283\n",
      "Testing: 0.6533506141399044 Training: 0.6433799213665928 Seed: 284\n",
      "Testing: 0.6503006501115725 Training: 0.6438472438633338 Seed: 287\n",
      "Testing: 0.6481108482424828 Training: 0.6447339963286222 Seed: 288\n",
      "Testing: 0.6559940667827148 Training: 0.6428823986117083 Seed: 289\n",
      "Testing: 0.6480605762392078 Training: 0.6447002668550215 Seed: 290\n",
      "Testing: 0.6495522517198024 Training: 0.6441988035205692 Seed: 291\n",
      "Testing: 0.6507672431196834 Training: 0.6440315436985243 Seed: 292\n",
      "Testing: 0.6579453424156163 Training: 0.6420923722625119 Seed: 293\n",
      "Testing: 0.6641809324825894 Training: 0.6405767356867846 Seed: 294\n",
      "Testing: 0.6499111414115976 Training: 0.6441807036441427 Seed: 295\n",
      "Testing: 0.6589227264719743 Training: 0.6417036408689474 Seed: 296\n",
      "Testing: 0.6460379088699574 Training: 0.6451605720592047 Seed: 297\n",
      "Testing: 0.6550861807059587 Training: 0.642867313212028 Seed: 298\n",
      "Testing: 0.6564206414163931 Training: 0.642648101946368 Seed: 299\n",
      "Testing: 0.6578618024353418 Training: 0.6422100645914868 Seed: 303\n",
      "Testing: 0.660574178484775 Training: 0.6415410250460176 Seed: 305\n",
      "Testing: 0.6540466075365773 Training: 0.6432385771289559 Seed: 306\n",
      "Testing: 0.6510809105044114 Training: 0.6439075480619323 Seed: 309\n",
      "Testing: 0.6545350822953337 Training: 0.6431125759882873 Seed: 311\n",
      "Testing: 0.6520633917569875 Training: 0.6436474814211293 Seed: 315\n",
      "Testing: 0.6502764083555647 Training: 0.6440119200026342 Seed: 316\n",
      "Testing: 0.6588116369462969 Training: 0.6420510918537402 Seed: 318\n",
      "Testing: 0.6457781800938924 Training: 0.6452780310821676 Seed: 323\n",
      "Testing: 0.6589133573060804 Training: 0.6417095340167995 Seed: 326\n",
      "Testing: 0.6458458680032325 Training: 0.6451723795742372 Seed: 327\n",
      "Testing: 0.6571520820048228 Training: 0.6423982602527343 Seed: 329\n",
      "Testing: 0.6507821383832868 Training: 0.6438456132909389 Seed: 332\n",
      "Testing: 0.6468922368548082 Training: 0.6450761472724673 Seed: 335\n",
      "Testing: 0.6524364041557202 Training: 0.6435336733527408 Seed: 336\n",
      "Testing: 0.6561812331363309 Training: 0.6425427124934291 Seed: 338\n",
      "Testing: 0.6471741672985172 Training: 0.6447643093547508 Seed: 339\n",
      "Testing: 0.6694240397934419 Training: 0.6392046756767196 Seed: 340\n",
      "Testing: 0.6680589837299993 Training: 0.6400400156250771 Seed: 342\n",
      "Testing: 0.6456505118106494 Training: 0.6452865447139169 Seed: 345\n",
      "Testing: 0.6521461560227955 Training: 0.6437421724139796 Seed: 349\n",
      "Testing: 0.6498832730309871 Training: 0.644294085824918 Seed: 353\n",
      "Testing: 0.6520663909012157 Training: 0.6436996760315739 Seed: 354\n",
      "Testing: 0.6586573229518209 Training: 0.6419244239749943 Seed: 355\n",
      "Testing: 0.6520631733655269 Training: 0.6436112076301416 Seed: 356\n",
      "Testing: 0.6507195359599245 Training: 0.6439272156666933 Seed: 360\n",
      "Testing: 0.6530071081672743 Training: 0.6433841839971832 Seed: 361\n",
      "Testing: 0.6519266204380345 Training: 0.6438082607959801 Seed: 363\n",
      "Testing: 0.6472558813957998 Training: 0.6439955261642135 Seed: 364\n",
      "Testing: 0.6483503645008247 Training: 0.6446984672162129 Seed: 365\n",
      "Testing: 0.6519965639706123 Training: 0.6437186490398128 Seed: 367\n",
      "Testing: 0.6565201821931346 Training: 0.6425817471109568 Seed: 369\n",
      "Testing: 0.6537486035005774 Training: 0.6432242884144764 Seed: 371\n",
      "Testing: 0.6479941873152772 Training: 0.6447695465961579 Seed: 372\n",
      "Testing: 0.6588697568477203 Training: 0.6417213299899389 Seed: 374\n",
      "Testing: 0.6475404553752706 Training: 0.6447803952985209 Seed: 376\n",
      "Testing: 0.6532048344390955 Training: 0.6434242018261089 Seed: 377\n",
      "Testing: 0.6692445710878814 Training: 0.6389505081803881 Seed: 378\n",
      "Testing: 0.6580183096600161 Training: 0.6422172238019074 Seed: 380\n",
      "Testing: 0.6487558534134967 Training: 0.6444623380693666 Seed: 381\n",
      "Testing: 0.6559165290354163 Training: 0.6427679928199987 Seed: 384\n",
      "Testing: 0.6476913213721863 Training: 0.6445507953266425 Seed: 387\n",
      "Testing: 0.6521188507572984 Training: 0.6435396804087294 Seed: 391\n",
      "Testing: 0.6482879104651172 Training: 0.644526995385323 Seed: 392\n",
      "Testing: 0.6543903523751436 Training: 0.6429979208077614 Seed: 394\n",
      "Testing: 0.6504082256795243 Training: 0.6440406614921306 Seed: 397\n",
      "Testing: 0.6493313711243539 Training: 0.6443912316259354 Seed: 398\n",
      "Testing: 0.6458292222574837 Training: 0.6451880388732781 Seed: 400\n",
      "Testing: 0.6460932141817068 Training: 0.6451154177761529 Seed: 401\n",
      "Testing: 0.6512230022994592 Training: 0.6439114139072373 Seed: 402\n",
      "Testing: 0.6637046538705623 Training: 0.6406140077022078 Seed: 403\n",
      "Testing: 0.646184458727925 Training: 0.6447194183325522 Seed: 407\n",
      "Testing: 0.6456321989699385 Training: 0.6452485297761045 Seed: 409\n",
      "Testing: 0.6559038360510692 Training: 0.6427104315746168 Seed: 411\n",
      "Testing: 0.6545234115388827 Training: 0.6431135046077233 Seed: 412\n",
      "Testing: 0.6519835075389924 Training: 0.6437513177179263 Seed: 414\n",
      "Testing: 0.6472068111852776 Training: 0.6448230458455574 Seed: 416\n",
      "Testing: 0.6499995812928054 Training: 0.6442175255722528 Seed: 422\n",
      "Testing: 0.6512623168502117 Training: 0.6439793709075676 Seed: 423\n",
      "Testing: 0.6598307085095787 Training: 0.6411711418395074 Seed: 424\n",
      "Testing: 0.6686630114330768 Training: 0.6397534379952565 Seed: 425\n",
      "Testing: 0.6496683562436565 Training: 0.6443578045606758 Seed: 430\n",
      "Testing: 0.6455649206401317 Training: 0.6452504588149889 Seed: 432\n",
      "Testing: 0.6488007913740598 Training: 0.6445033929726833 Seed: 434\n",
      "Testing: 0.6463015952471486 Training: 0.6451201119001109 Seed: 436\n",
      "Testing: 0.6503015191690573 Training: 0.6442222048855893 Seed: 437\n",
      "Testing: 0.6488358045928835 Training: 0.6445096109727493 Seed: 439\n",
      "Testing: 0.6519589691269391 Training: 0.6436224598589223 Seed: 441\n",
      "Testing: 0.6528181822949364 Training: 0.6434287683733086 Seed: 446\n",
      "Testing: 0.6492630449050966 Training: 0.6442888348337797 Seed: 447\n",
      "Testing: 0.6644338759888977 Training: 0.640787974648867 Seed: 450\n",
      "Testing: 0.6632860592749671 Training: 0.6408689861620382 Seed: 451\n",
      "Testing: 0.6632607978671913 Training: 0.6408255262044515 Seed: 452\n",
      "Testing: 0.6665836846978994 Training: 0.6399014580607416 Seed: 456\n",
      "Testing: 0.6621346742893792 Training: 0.6410236517753453 Seed: 457\n",
      "Testing: 0.6588765264238315 Training: 0.6419283349259133 Seed: 459\n",
      "Testing: 0.6508687184269338 Training: 0.6432082231081058 Seed: 460\n",
      "Testing: 0.6506347367582009 Training: 0.644123177973513 Seed: 463\n",
      "Testing: 0.6594613433968873 Training: 0.6418921392126491 Seed: 464\n",
      "Testing: 0.652113978728427 Training: 0.6437494997634969 Seed: 469\n",
      "Testing: 0.660301445465442 Training: 0.6416086912750967 Seed: 470\n",
      "Testing: 0.6495251781958095 Training: 0.6443571107633456 Seed: 473\n",
      "Testing: 0.6486593958733453 Training: 0.6444877222866569 Seed: 476\n",
      "Testing: 0.6493856444284862 Training: 0.6442975287255839 Seed: 478\n",
      "Testing: 0.6520988009434782 Training: 0.6437039888847889 Seed: 479\n",
      "Testing: 0.657005579822372 Training: 0.64245332432956 Seed: 482\n",
      "Testing: 0.6620385504128415 Training: 0.6408564828052912 Seed: 485\n",
      "Testing: 0.6520174831303374 Training: 0.6437878117101142 Seed: 487\n",
      "Testing: 0.6491560324459761 Training: 0.6442637249464402 Seed: 488\n",
      "Testing: 0.6719459411980294 Training: 0.6379829274780104 Seed: 489\n",
      "Testing: 0.668019379559141 Training: 0.6397143185020759 Seed: 490\n",
      "Testing: 0.6546372388559077 Training: 0.6431263661740125 Seed: 491\n",
      "Testing: 0.6700636312069363 Training: 0.6390947381537735 Seed: 493\n",
      "Testing: 0.6518444633311457 Training: 0.6437754126385211 Seed: 496\n",
      "Testing: 0.6515775829878633 Training: 0.6438358359014877 Seed: 499\n",
      "Testing: 0.65225851445863 Training: 0.6435857927570063 Seed: 500\n",
      "Testing: 0.652325253173937 Training: 0.6434993116151271 Seed: 501\n",
      "Testing: 0.661106972805313 Training: 0.6414928705577589 Seed: 502\n",
      "Testing: 0.6588369914623082 Training: 0.6420300817450428 Seed: 504\n",
      "Testing: 0.6472543449039152 Training: 0.6449965113958218 Seed: 506\n",
      "Testing: 0.654407473195655 Training: 0.6431145653095736 Seed: 515\n",
      "Testing: 0.6498185416482256 Training: 0.6441925562698267 Seed: 516\n",
      "Testing: 0.6597267598287087 Training: 0.6416169585602683 Seed: 518\n",
      "Testing: 0.6496248340569126 Training: 0.644346737482869 Seed: 519\n",
      "Testing: 0.6588626629497922 Training: 0.6420357351718403 Seed: 521\n",
      "Testing: 0.6626153188948037 Training: 0.6410906295035723 Seed: 522\n",
      "Testing: 0.648211233709365 Training: 0.6446380119515087 Seed: 523\n",
      "Testing: 0.651312866575245 Training: 0.6439155580963902 Seed: 524\n",
      "Testing: 0.6546851059785946 Training: 0.6429965968936285 Seed: 525\n",
      "Testing: 0.656982606636485 Training: 0.6425115514422803 Seed: 527\n",
      "Testing: 0.6567211025640514 Training: 0.6426106916754183 Seed: 528\n",
      "Testing: 0.658914995585034 Training: 0.6420596739839296 Seed: 529\n",
      "Testing: 0.6511620884536462 Training: 0.6436592834241621 Seed: 530\n",
      "Testing: 0.6562289171103776 Training: 0.6427647609794452 Seed: 533\n",
      "Testing: 0.6651349561961379 Training: 0.6405492481360504 Seed: 536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6522504822291626 Training: 0.6436116732149921 Seed: 539\n",
      "Testing: 0.6626492418670435 Training: 0.6408925661445406 Seed: 541\n",
      "Testing: 0.6550000979855373 Training: 0.6429154331620037 Seed: 542\n",
      "Testing: 0.6540393231069223 Training: 0.6430946618581166 Seed: 543\n",
      "Testing: 0.6461123744765113 Training: 0.6450981784165235 Seed: 544\n",
      "Testing: 0.6499726071833634 Training: 0.6442341019254484 Seed: 545\n",
      "Testing: 0.6505539102984103 Training: 0.6439544376292536 Seed: 549\n",
      "Testing: 0.646342658357298 Training: 0.6450608062320734 Seed: 551\n",
      "Testing: 0.6519460131669603 Training: 0.6438069256960067 Seed: 556\n",
      "Testing: 0.6453764699814944 Training: 0.6453343870946983 Seed: 558\n",
      "Testing: 0.654648445080197 Training: 0.6430541989368315 Seed: 560\n",
      "Testing: 0.6522890170520772 Training: 0.6436856983576146 Seed: 562\n",
      "Testing: 0.6453622472302323 Training: 0.6453182653250609 Seed: 563\n",
      "Testing: 0.6465086934480484 Training: 0.6450495534354753 Seed: 566\n",
      "Testing: 0.6506837280177308 Training: 0.6439521762479815 Seed: 570\n",
      "Testing: 0.6568804731448326 Training: 0.642371031154396 Seed: 571\n",
      "Testing: 0.6514997730383556 Training: 0.643660942229139 Seed: 577\n",
      "Testing: 0.6564806684816522 Training: 0.6426707736613159 Seed: 578\n",
      "Testing: 0.6561643313901373 Training: 0.6427869610382502 Seed: 579\n",
      "Testing: 0.6587621037709772 Training: 0.6418103948893434 Seed: 580\n",
      "Testing: 0.6652615945896064 Training: 0.6402694080144029 Seed: 581\n",
      "Testing: 0.670278764227226 Training: 0.6389089668264959 Seed: 582\n",
      "Testing: 0.6484174153521484 Training: 0.6445130411407956 Seed: 585\n",
      "Testing: 0.6465923825468826 Training: 0.6451072751963831 Seed: 586\n",
      "Testing: 0.6459268904376534 Training: 0.6452572863885445 Seed: 588\n",
      "Testing: 0.6525351492178042 Training: 0.6436266608008692 Seed: 589\n",
      "Testing: 0.6596505209255799 Training: 0.6418481120346817 Seed: 591\n",
      "Testing: 0.6511044129520329 Training: 0.6439205325997401 Seed: 592\n",
      "Testing: 0.6572996144221939 Training: 0.6424010213240161 Seed: 593\n",
      "Testing: 0.6598740056130712 Training: 0.6419023809105099 Seed: 594\n",
      "Testing: 0.6603366154946327 Training: 0.6416029982410563 Seed: 596\n",
      "Testing: 0.6641009212991076 Training: 0.6406568967107469 Seed: 597\n",
      "Testing: 0.6537576013367355 Training: 0.6432582146801689 Seed: 598\n",
      "Testing: 0.6538626079954549 Training: 0.6432810626796671 Seed: 601\n",
      "Testing: 0.6541347247161171 Training: 0.643242437986282 Seed: 602\n",
      "Testing: 0.6575760551521674 Training: 0.6420087585097267 Seed: 606\n",
      "Testing: 0.6552974324681828 Training: 0.6428638723672114 Seed: 607\n",
      "Testing: 0.6511275882454279 Training: 0.6439084853192725 Seed: 611\n",
      "Testing: 0.6485631707237913 Training: 0.6445608993992944 Seed: 612\n",
      "Testing: 0.6567066940771538 Training: 0.6423266587280478 Seed: 614\n",
      "Testing: 0.6507527731953833 Training: 0.6439888809046387 Seed: 615\n",
      "Testing: 0.6494067771335741 Training: 0.6443862964344298 Seed: 617\n",
      "Testing: 0.6639382228817651 Training: 0.6407175450242224 Seed: 619\n",
      "Testing: 0.6472566313257273 Training: 0.644848531187747 Seed: 620\n",
      "Testing: 0.6490242265225765 Training: 0.6444311643520397 Seed: 621\n",
      "Testing: 0.6459214102018888 Training: 0.6452650261153181 Seed: 624\n",
      "Testing: 0.6639583327853348 Training: 0.6408612789914844 Seed: 625\n",
      "Testing: 0.656457662465233 Training: 0.642564245363143 Seed: 628\n",
      "Testing: 0.6517220405947776 Training: 0.6438443685910817 Seed: 629\n",
      "Testing: 0.6475855957493863 Training: 0.6446211365614709 Seed: 631\n",
      "Testing: 0.6571222401692167 Training: 0.6423520094849243 Seed: 634\n",
      "Testing: 0.659161986860193 Training: 0.6419130874896878 Seed: 641\n",
      "Testing: 0.6489547874495475 Training: 0.6443731130756585 Seed: 643\n",
      "Testing: 0.6520650295735237 Training: 0.6437893704289872 Seed: 650\n",
      "Testing: 0.6582144761966418 Training: 0.6420734758348953 Seed: 652\n",
      "Testing: 0.6539083487378079 Training: 0.6433214808990979 Seed: 660\n",
      "Testing: 0.6579332532215694 Training: 0.6423398967641956 Seed: 667\n",
      "Testing: 0.6609958779831504 Training: 0.6416072112144178 Seed: 669\n",
      "Testing: 0.6502754915227184 Training: 0.6438079309623432 Seed: 672\n",
      "Testing: 0.6549982826259234 Training: 0.6429912931458536 Seed: 673\n",
      "Testing: 0.6503988498409345 Training: 0.6440927710124684 Seed: 676\n",
      "Testing: 0.6473995305525805 Training: 0.6444643685045257 Seed: 677\n",
      "Testing: 0.6548745163918342 Training: 0.6430341192610144 Seed: 678\n",
      "Testing: 0.6532560898727627 Training: 0.6434378850652214 Seed: 679\n",
      "Testing: 0.6642000230031032 Training: 0.640802280652485 Seed: 680\n",
      "Testing: 0.6533899194972592 Training: 0.6434218666096927 Seed: 684\n",
      "Testing: 0.6583919015796252 Training: 0.6420870308123767 Seed: 685\n",
      "Testing: 0.6482237853997612 Training: 0.6447363222090947 Seed: 686\n",
      "Testing: 0.6553111071683032 Training: 0.6427063822538871 Seed: 689\n",
      "Testing: 0.6579857906635055 Training: 0.6421869906598165 Seed: 691\n",
      "Testing: 0.6548138100918921 Training: 0.6428582126314761 Seed: 692\n",
      "Testing: 0.6480542231711914 Training: 0.6444055093733885 Seed: 693\n",
      "Testing: 0.647857419513885 Training: 0.644518285539986 Seed: 694\n",
      "Testing: 0.6457176513423559 Training: 0.645305789439926 Seed: 695\n",
      "Testing: 0.6490799367991669 Training: 0.6444264035137361 Seed: 696\n",
      "Testing: 0.6508890943167179 Training: 0.6439814561885855 Seed: 700\n",
      "Testing: 0.6550930992743124 Training: 0.6429671680599708 Seed: 701\n",
      "Testing: 0.647688798621 Training: 0.6448026576524516 Seed: 705\n",
      "Testing: 0.6479354712481861 Training: 0.6446891843128724 Seed: 708\n",
      "Testing: 0.6497687837746178 Training: 0.6443401743296876 Seed: 709\n",
      "Testing: 0.6469418662657597 Training: 0.6449303451917884 Seed: 711\n",
      "Testing: 0.6521097248260643 Training: 0.643797817387129 Seed: 714\n",
      "Testing: 0.6487742705404489 Training: 0.6445928412739157 Seed: 716\n",
      "Testing: 0.6583540238365992 Training: 0.6420472694952523 Seed: 717\n",
      "Testing: 0.6777317560953586 Training: 0.6373514208619753 Seed: 719\n",
      "Testing: 0.6578055549840451 Training: 0.6421333818654212 Seed: 721\n",
      "Testing: 0.6494528905886329 Training: 0.6443603228626019 Seed: 729\n",
      "Testing: 0.6471557107829464 Training: 0.6447950120848753 Seed: 730\n",
      "Testing: 0.6454671415749886 Training: 0.645393951864091 Seed: 731\n",
      "Testing: 0.6542097552555047 Training: 0.6430949857525229 Seed: 732\n",
      "Testing: 0.6495025483621142 Training: 0.6444269107271101 Seed: 733\n",
      "Testing: 0.6527742108406751 Training: 0.6434507568268198 Seed: 734\n",
      "Testing: 0.6477099830081815 Training: 0.6444862815730321 Seed: 736\n",
      "Testing: 0.6651546352617451 Training: 0.640335937345285 Seed: 737\n",
      "Testing: 0.6569837943053094 Training: 0.6424651752299009 Seed: 742\n",
      "Testing: 0.6494479560727187 Training: 0.6442770667090879 Seed: 745\n",
      "Testing: 0.6461131954177688 Training: 0.6451803930355324 Seed: 749\n",
      "Testing: 0.646681183246929 Training: 0.645097290715076 Seed: 750\n",
      "Testing: 0.6468893366850927 Training: 0.6449669220967216 Seed: 757\n",
      "Testing: 0.6479834425868796 Training: 0.6446737106519912 Seed: 759\n",
      "Testing: 0.6496925804946772 Training: 0.6441643451006827 Seed: 764\n",
      "Testing: 0.6505126416009122 Training: 0.6438463146160027 Seed: 765\n",
      "Testing: 0.6524523290619846 Training: 0.6436450446314235 Seed: 768\n",
      "Testing: 0.6519673021595412 Training: 0.6437541463194904 Seed: 769\n",
      "Testing: 0.6476625667040035 Training: 0.6446514413770884 Seed: 775\n",
      "Testing: 0.655716873184516 Training: 0.6426906292319285 Seed: 783\n",
      "Testing: 0.6501882256487215 Training: 0.6442047247140592 Seed: 784\n",
      "Testing: 0.646141996503497 Training: 0.6451051246345415 Seed: 785\n",
      "Testing: 0.6465684722422476 Training: 0.6449794913719226 Seed: 787\n",
      "Testing: 0.6597530532868038 Training: 0.641626538351974 Seed: 795\n",
      "Testing: 0.6535215810162167 Training: 0.6434267912478138 Seed: 796\n",
      "Testing: 0.6550630755390154 Training: 0.642968369278202 Seed: 797\n",
      "Testing: 0.6598996512051266 Training: 0.6417468761166418 Seed: 798\n",
      "Testing: 0.6498071670083423 Training: 0.6442406811669922 Seed: 799\n",
      "Testing: 0.6556309349973671 Training: 0.6428565319471432 Seed: 800\n",
      "Testing: 0.6629827760869836 Training: 0.6409989969944576 Seed: 801\n",
      "Testing: 0.6542296702887802 Training: 0.643147090431671 Seed: 802\n",
      "Testing: 0.6470787541659804 Training: 0.6448505188272154 Seed: 803\n",
      "Testing: 0.6516928827870003 Training: 0.6438467267439847 Seed: 805\n",
      "Testing: 0.6473910076483075 Training: 0.644841039019473 Seed: 807\n",
      "Testing: 0.657199081314466 Training: 0.6424445832481764 Seed: 808\n",
      "Testing: 0.6459564387849013 Training: 0.6451455320045222 Seed: 809\n",
      "Testing: 0.6463714745618239 Training: 0.6451298584311599 Seed: 811\n",
      "Testing: 0.6475904834027569 Training: 0.6441518659593624 Seed: 812\n",
      "Testing: 0.6517054876268382 Training: 0.6438279919648164 Seed: 813\n",
      "Testing: 0.6599082712924651 Training: 0.6417942058500201 Seed: 814\n",
      "Testing: 0.6605499488506259 Training: 0.6411575109399469 Seed: 815\n",
      "Testing: 0.6481151657734963 Training: 0.6445339251233968 Seed: 816\n",
      "Testing: 0.6653462854575773 Training: 0.6404009170211251 Seed: 817\n",
      "Testing: 0.6474364523582439 Training: 0.6448756306788502 Seed: 822\n",
      "Testing: 0.6468905277042425 Training: 0.6450058142703016 Seed: 829\n",
      "Testing: 0.6702191416734691 Training: 0.6391237404104684 Seed: 830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6484167985477644 Training: 0.6446218271801396 Seed: 837\n",
      "Testing: 0.6526864650782659 Training: 0.6434809892125437 Seed: 838\n",
      "Testing: 0.6556320435659849 Training: 0.6427603296492762 Seed: 839\n",
      "Testing: 0.6583453317562484 Training: 0.6419447760461843 Seed: 840\n",
      "Testing: 0.6523042175546685 Training: 0.6435636969731293 Seed: 841\n",
      "Testing: 0.6534818671004923 Training: 0.6430712473777187 Seed: 842\n",
      "Testing: 0.6485791934864429 Training: 0.644616274594008 Seed: 844\n",
      "Testing: 0.651174167100073 Training: 0.643883432379126 Seed: 845\n",
      "Testing: 0.6655535442524637 Training: 0.6403369555223065 Seed: 846\n",
      "Testing: 0.6513042285436434 Training: 0.643868500371485 Seed: 848\n",
      "Testing: 0.6493325940357046 Training: 0.6443826954639844 Seed: 849\n",
      "Testing: 0.6458620524900114 Training: 0.645237647555299 Seed: 854\n",
      "Testing: 0.6491561629873166 Training: 0.6444014558267026 Seed: 859\n",
      "Testing: 0.6525585017715654 Training: 0.6435712554637985 Seed: 861\n",
      "Testing: 0.6555717004949229 Training: 0.6427168722433565 Seed: 862\n",
      "Testing: 0.654533381479087 Training: 0.6430652447677385 Seed: 864\n",
      "Testing: 0.6475607785299287 Training: 0.6446443316676713 Seed: 866\n",
      "Testing: 0.653737088992397 Training: 0.6433236306793975 Seed: 867\n",
      "Testing: 0.6495172746636996 Training: 0.6443964043908733 Seed: 868\n",
      "Testing: 0.667494170645182 Training: 0.6398881447459253 Seed: 870\n",
      "Testing: 0.6534818880529681 Training: 0.6433585976363123 Seed: 872\n",
      "Testing: 0.6523629919128967 Training: 0.643676766538329 Seed: 882\n",
      "Testing: 0.6486614625341689 Training: 0.644494447263507 Seed: 884\n",
      "Testing: 0.6477228287723702 Training: 0.6448031818571769 Seed: 885\n",
      "Testing: 0.648801020752364 Training: 0.6444730564735377 Seed: 886\n",
      "Testing: 0.6547133269837457 Training: 0.643085105576403 Seed: 888\n",
      "Testing: 0.6532282507080278 Training: 0.6433353031032275 Seed: 890\n",
      "Testing: 0.655586553601065 Training: 0.6428301859536156 Seed: 891\n",
      "Testing: 0.6533344649196433 Training: 0.6434056715263914 Seed: 892\n",
      "Testing: 0.6497878912841253 Training: 0.644240218847063 Seed: 893\n",
      "Testing: 0.6532694997024573 Training: 0.643250242483115 Seed: 895\n",
      "Testing: 0.6548995451861848 Training: 0.642953067185944 Seed: 896\n",
      "Testing: 0.6495633396134483 Training: 0.6442503703351241 Seed: 898\n",
      "Testing: 0.6515533536807501 Training: 0.6438329835943591 Seed: 899\n",
      "Testing: 0.6645861695035427 Training: 0.6405860765592923 Seed: 901\n",
      "Testing: 0.6492187548308144 Training: 0.6445025564541134 Seed: 902\n",
      "Testing: 0.6644471929641411 Training: 0.6404834814278835 Seed: 903\n",
      "Testing: 0.6480598344778112 Training: 0.6444963576643463 Seed: 904\n",
      "Testing: 0.6458536347568673 Training: 0.6452325587435647 Seed: 906\n",
      "Testing: 0.654315319113816 Training: 0.6431254766033367 Seed: 909\n",
      "Testing: 0.6625180975635154 Training: 0.6410378725013122 Seed: 912\n",
      "Testing: 0.6625448808199043 Training: 0.6412978116401931 Seed: 914\n",
      "Testing: 0.6457950299943731 Training: 0.6452964339960672 Seed: 915\n",
      "Testing: 0.6461907270160235 Training: 0.645173996575189 Seed: 917\n",
      "Testing: 0.6478642237598967 Training: 0.6447429299116278 Seed: 922\n",
      "Testing: 0.6476086152372602 Training: 0.6448360135154917 Seed: 925\n",
      "Testing: 0.6534045825607161 Training: 0.6432725852207841 Seed: 927\n",
      "Testing: 0.6873007735236039 Training: 0.6348224925701187 Seed: 928\n",
      "Testing: 0.6491274322387091 Training: 0.6443139351344015 Seed: 931\n",
      "Testing: 0.6497864967054463 Training: 0.644219576344647 Seed: 932\n",
      "Testing: 0.6456067638944516 Training: 0.6452671605666237 Seed: 935\n",
      "Testing: 0.6584963698293838 Training: 0.6411086801856678 Seed: 936\n",
      "Testing: 0.661321952735159 Training: 0.6412460673110578 Seed: 937\n",
      "Testing: 0.6517959928077188 Training: 0.6437773151804698 Seed: 938\n",
      "Testing: 0.6480674497079086 Training: 0.6447335282435522 Seed: 939\n",
      "Testing: 0.6536072521979358 Training: 0.6433477159203911 Seed: 940\n",
      "Testing: 0.6612407380590366 Training: 0.641267324184265 Seed: 942\n",
      "Testing: 0.6526774056041023 Training: 0.6436044458151369 Seed: 945\n",
      "Testing: 0.6501674944819925 Training: 0.6441753704249055 Seed: 947\n",
      "Testing: 0.6573480370098945 Training: 0.6422216066778015 Seed: 948\n",
      "Testing: 0.6464099042720423 Training: 0.6451416779252417 Seed: 950\n",
      "Testing: 0.6468045202992427 Training: 0.645061164668853 Seed: 953\n",
      "Testing: 0.6515432473807656 Training: 0.6435439132658948 Seed: 954\n",
      "Testing: 0.6541699659255283 Training: 0.6431517402497022 Seed: 956\n",
      "Testing: 0.6603855237045353 Training: 0.6414262883622126 Seed: 958\n",
      "Testing: 0.654221640897613 Training: 0.6431344509854939 Seed: 960\n",
      "Testing: 0.658462356641818 Training: 0.6421639730683402 Seed: 961\n",
      "Testing: 0.6531906037510378 Training: 0.6433992227806905 Seed: 962\n",
      "Testing: 0.6474055522613299 Training: 0.6448470556871047 Seed: 964\n",
      "Testing: 0.6545245849336955 Training: 0.6425921369442625 Seed: 965\n",
      "Testing: 0.6586625347547989 Training: 0.6420538961580905 Seed: 966\n",
      "Testing: 0.6472036747906278 Training: 0.6449333192815716 Seed: 968\n",
      "Testing: 0.6608514810940912 Training: 0.6414141036273964 Seed: 970\n",
      "Testing: 0.6566696811654985 Training: 0.642464167069754 Seed: 973\n",
      "Testing: 0.6479473312499736 Training: 0.6447599305740469 Seed: 974\n",
      "Testing: 0.6560349886165464 Training: 0.642781950901461 Seed: 977\n",
      "Testing: 0.64975836465644 Training: 0.6443480051964984 Seed: 982\n",
      "Testing: 0.6776589458467546 Training: 0.6369056569171953 Seed: 987\n",
      "Testing: 0.6507159761459036 Training: 0.6440511117407121 Seed: 992\n",
      "Testing: 0.6587258751462793 Training: 0.6420609984014295 Seed: 993\n",
      "Testing: 0.6568527540326019 Training: 0.6426275902500094 Seed: 994\n",
      "Testing: 0.6547265053210435 Training: 0.6430010536521192 Seed: 996\n",
      "Testing: 0.6511433498306572 Training: 0.644031454659209 Seed: 997\n",
      "Testing: 0.6620016088679987 Training: 0.6408688236769108 Seed: 999\n",
      "Testing: 0.6481368867424646 Training: 0.6445644328819509 Seed: 1000\n",
      "Testing: 0.6569854811882812 Training: 0.6422589512485659 Seed: 1004\n",
      "Testing: 0.6583014760674104 Training: 0.642173273548812 Seed: 1006\n",
      "Testing: 0.6480770899134263 Training: 0.6446493413864929 Seed: 1007\n",
      "Testing: 0.6454185887080872 Training: 0.6452456824590562 Seed: 1008\n",
      "Testing: 0.6593419655804038 Training: 0.6419774273185279 Seed: 1009\n",
      "Testing: 0.6526703926678005 Training: 0.643613024940672 Seed: 1012\n",
      "Testing: 0.6558184162891726 Training: 0.642356572337214 Seed: 1016\n",
      "Testing: 0.6624788933165755 Training: 0.641049407184646 Seed: 1017\n",
      "Testing: 0.6668797514948777 Training: 0.6398930179410882 Seed: 1019\n",
      "Testing: 0.6554510819865276 Training: 0.6427967560287707 Seed: 1021\n",
      "Testing: 0.6492819864978194 Training: 0.6442961761087207 Seed: 1022\n",
      "Testing: 0.6527117742268564 Training: 0.6435082517536016 Seed: 1023\n",
      "Testing: 0.6539267410870788 Training: 0.6431765846333803 Seed: 1024\n",
      "Testing: 0.652828471995336 Training: 0.6433018072458115 Seed: 1027\n",
      "Testing: 0.6526542976684924 Training: 0.6434279320597891 Seed: 1028\n",
      "Testing: 0.6543193884052512 Training: 0.6429193510068772 Seed: 1029\n",
      "Testing: 0.6532627609115107 Training: 0.643370982527679 Seed: 1031\n",
      "Testing: 0.6492579895023649 Training: 0.6443619444783943 Seed: 1034\n",
      "Testing: 0.6498295821338332 Training: 0.6443178485047132 Seed: 1035\n",
      "Testing: 0.6525472918596557 Training: 0.6435610182515863 Seed: 1036\n",
      "Testing: 0.657619209721886 Training: 0.642125280186373 Seed: 1039\n",
      "Testing: 0.6520909818567839 Training: 0.6436268285267162 Seed: 1040\n",
      "Testing: 0.6488049189563282 Training: 0.6444031658617712 Seed: 1042\n",
      "Testing: 0.6583391263362789 Training: 0.6420840626872625 Seed: 1043\n",
      "Testing: 0.6646367195784215 Training: 0.6405504577959176 Seed: 1044\n",
      "Testing: 0.6478275178815246 Training: 0.6447818045758651 Seed: 1045\n",
      "Testing: 0.6567353263196405 Training: 0.6425135340401973 Seed: 1046\n",
      "Testing: 0.6507846207405548 Training: 0.6440866712337817 Seed: 1049\n",
      "Testing: 0.6560898287197053 Training: 0.6427180694884993 Seed: 1051\n",
      "Testing: 0.6529262487459123 Training: 0.6435279919210914 Seed: 1052\n",
      "Testing: 0.6544451644355136 Training: 0.6431302514738325 Seed: 1053\n",
      "Testing: 0.6554368341038825 Training: 0.6429506078059639 Seed: 1054\n",
      "Testing: 0.6550469832241236 Training: 0.6428701100489611 Seed: 1055\n",
      "Testing: 0.6487252716323324 Training: 0.6445273691178872 Seed: 1056\n",
      "Testing: 0.6511224908019622 Training: 0.6439135210534851 Seed: 1057\n",
      "Testing: 0.648826619263156 Training: 0.6444493183049225 Seed: 1059\n",
      "Testing: 0.6507359641789885 Training: 0.6440895014316237 Seed: 1060\n",
      "Testing: 0.6520418812546348 Training: 0.6437320077747156 Seed: 1064\n",
      "Testing: 0.6549769624150904 Training: 0.6430643278216428 Seed: 1066\n",
      "Testing: 0.6460825104833843 Training: 0.6451767991668385 Seed: 1067\n",
      "Testing: 0.647459974039603 Training: 0.6443838796249557 Seed: 1068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6472209667838318 Training: 0.6449132840166055 Seed: 1071\n",
      "Testing: 0.6535526378453266 Training: 0.6434126627148522 Seed: 1072\n",
      "Testing: 0.6584986289994781 Training: 0.6419639741931078 Seed: 1074\n",
      "Testing: 0.6468138721397741 Training: 0.6450747113160114 Seed: 1075\n",
      "Testing: 0.6512248441248913 Training: 0.6439252458408463 Seed: 1076\n",
      "Testing: 0.664816234962514 Training: 0.6405832356679186 Seed: 1077\n",
      "Testing: 0.6593207652159172 Training: 0.6418877590556386 Seed: 1080\n",
      "Testing: 0.6595540678983063 Training: 0.6416867010935008 Seed: 1081\n",
      "Testing: 0.6473141341526103 Training: 0.6449111224834598 Seed: 1083\n",
      "Testing: 0.6630454877056412 Training: 0.6408172384670713 Seed: 1089\n",
      "Testing: 0.649346701647048 Training: 0.6444442876403467 Seed: 1092\n",
      "Testing: 0.6473247200879026 Training: 0.6441299433469275 Seed: 1094\n",
      "Testing: 0.6544664160142717 Training: 0.6431878423464112 Seed: 1095\n",
      "Testing: 0.6506372001064259 Training: 0.6439976963525265 Seed: 1098\n",
      "Testing: 0.6537258439315954 Training: 0.6433971824794307 Seed: 1099\n",
      "Testing: 0.6475674639875968 Training: 0.6447532330952677 Seed: 1100\n",
      "Testing: 0.6692811734618429 Training: 0.6394037814805482 Seed: 1103\n",
      "Testing: 0.6518727019280179 Training: 0.643748751909891 Seed: 1106\n",
      "Testing: 0.6514052469402005 Training: 0.643653794783422 Seed: 1109\n",
      "Testing: 0.6556007785277058 Training: 0.6427854549252721 Seed: 1110\n",
      "Testing: 0.6545728595031549 Training: 0.6429988749039296 Seed: 1114\n",
      "Testing: 0.6547553360133793 Training: 0.6427101949342865 Seed: 1119\n",
      "Testing: 0.6645788650373574 Training: 0.6406914758778046 Seed: 1120\n",
      "Testing: 0.6579139502435182 Training: 0.6421707346592739 Seed: 1121\n",
      "Testing: 0.6466154414370562 Training: 0.6447496332283789 Seed: 1126\n",
      "Testing: 0.6499123803583878 Training: 0.6441562633897318 Seed: 1128\n",
      "Testing: 0.6577031934132263 Training: 0.6423341188594031 Seed: 1129\n",
      "Testing: 0.6486338488125071 Training: 0.6444092041701522 Seed: 1133\n",
      "Testing: 0.6492173740716072 Training: 0.6443708103947332 Seed: 1136\n",
      "Testing: 0.6624139220944499 Training: 0.6409078784091896 Seed: 1138\n",
      "Testing: 0.6513085477868563 Training: 0.6439239587849499 Seed: 1140\n",
      "Testing: 0.6454504810028597 Training: 0.6453614555723997 Seed: 1142\n",
      "Testing: 0.6489384334578028 Training: 0.6445326841018755 Seed: 1146\n",
      "Testing: 0.6483136208247181 Training: 0.6444216396353726 Seed: 1148\n",
      "Testing: 0.6610098680602133 Training: 0.6416323955772316 Seed: 1150\n",
      "Testing: 0.6549678269306562 Training: 0.6430047547107081 Seed: 1156\n",
      "Testing: 0.6501259373664559 Training: 0.6442131909547725 Seed: 1158\n",
      "Testing: 0.6570234333532762 Training: 0.6423559346468413 Seed: 1159\n",
      "Testing: 0.6483231590943052 Training: 0.6446472402326102 Seed: 1160\n",
      "Testing: 0.6539172212124061 Training: 0.6433433168778283 Seed: 1161\n",
      "Testing: 0.6545337431921564 Training: 0.6430607374067421 Seed: 1162\n",
      "Testing: 0.6594828192473121 Training: 0.6417544653966274 Seed: 1163\n",
      "Testing: 0.6503067349090748 Training: 0.6441090692291074 Seed: 1165\n",
      "Testing: 0.6548612132816962 Training: 0.6429650993951883 Seed: 1166\n",
      "Testing: 0.6464205228803518 Training: 0.6451413052292609 Seed: 1167\n",
      "Testing: 0.6668797243104362 Training: 0.6397596520246507 Seed: 1169\n",
      "Testing: 0.6470065308655341 Training: 0.6448042659479176 Seed: 1170\n",
      "Testing: 0.6609093778283479 Training: 0.6416027111914366 Seed: 1172\n",
      "Testing: 0.6530068474628341 Training: 0.6434945549980456 Seed: 1174\n",
      "Testing: 0.6613263265885603 Training: 0.6412484354628205 Seed: 1179\n",
      "Testing: 0.653299810885285 Training: 0.6434772619523115 Seed: 1183\n",
      "Testing: 0.6625091130389408 Training: 0.6410343053314753 Seed: 1185\n",
      "Testing: 0.6548265472334347 Training: 0.6430544405243073 Seed: 1189\n",
      "Testing: 0.6634156184234378 Training: 0.6407119865959241 Seed: 1190\n",
      "Testing: 0.6514103204839563 Training: 0.643941215327645 Seed: 1191\n",
      "Testing: 0.6502684786711649 Training: 0.6440687398005966 Seed: 1193\n",
      "Testing: 0.6449689788615274 Training: 0.6449239216142253 Seed: 1196\n",
      "Testing: 0.6689667244203251 Training: 0.6393165257296975 Seed: 1197\n",
      "Testing: 0.6526232271643195 Training: 0.6434459172394417 Seed: 1198\n",
      "Testing: 0.65817668522832 Training: 0.6422597901555366 Seed: 1200\n",
      "Testing: 0.6516023316596848 Training: 0.6437012998214988 Seed: 1201\n",
      "Testing: 0.648651560865323 Training: 0.6445450956199376 Seed: 1202\n",
      "Testing: 0.6546446060013835 Training: 0.6430607479302994 Seed: 1204\n",
      "Testing: 0.6522689787587226 Training: 0.6437197295941133 Seed: 1206\n",
      "Testing: 0.6630584306507052 Training: 0.6412103394310058 Seed: 1207\n",
      "Testing: 0.6562580760507525 Training: 0.6426668060461515 Seed: 1210\n",
      "Testing: 0.6454315411239343 Training: 0.6451609516015141 Seed: 1212\n",
      "Testing: 0.6551968205855057 Training: 0.6428882558287513 Seed: 1213\n",
      "Testing: 0.6494548183363857 Training: 0.6443527733821265 Seed: 1217\n",
      "Testing: 0.6464353743337259 Training: 0.6451433657862821 Seed: 1219\n",
      "Testing: 0.6777101650797317 Training: 0.6370301600504048 Seed: 1225\n",
      "Testing: 0.6457775781045576 Training: 0.6452390549527371 Seed: 1229\n",
      "Testing: 0.6595633318290899 Training: 0.6417853228259811 Seed: 1230\n",
      "Testing: 0.6496111098101439 Training: 0.6441068653038063 Seed: 1233\n",
      "Testing: 0.6483351362609746 Training: 0.6446105193293675 Seed: 1235\n",
      "Testing: 0.656281700093902 Training: 0.6427706353290422 Seed: 1238\n",
      "Testing: 0.654829394528402 Training: 0.6427475453940065 Seed: 1240\n",
      "Testing: 0.6505694846566905 Training: 0.6440959698095043 Seed: 1243\n",
      "Testing: 0.6464959868295702 Training: 0.6451616160466377 Seed: 1248\n",
      "Testing: 0.651812122842141 Training: 0.643800364997841 Seed: 1251\n",
      "Testing: 0.6495611004966849 Training: 0.6443756368577723 Seed: 1252\n",
      "Testing: 0.6455184303005488 Training: 0.6452216037623807 Seed: 1254\n",
      "Testing: 0.6614210206765611 Training: 0.6413436673305153 Seed: 1256\n",
      "Testing: 0.6517730417138845 Training: 0.6435512552625755 Seed: 1257\n",
      "Testing: 0.6591828698945279 Training: 0.6417294049119109 Seed: 1258\n",
      "Testing: 0.6560389986835015 Training: 0.6427033856432636 Seed: 1259\n",
      "Testing: 0.6519206353461033 Training: 0.6436549072898184 Seed: 1261\n",
      "Testing: 0.6527562923677404 Training: 0.6434416396202994 Seed: 1262\n",
      "Testing: 0.6465170658625065 Training: 0.6447737980490675 Seed: 1264\n",
      "Testing: 0.6481219903194917 Training: 0.6447160543476752 Seed: 1265\n",
      "Testing: 0.6635155923902809 Training: 0.6409902799839333 Seed: 1266\n",
      "Testing: 0.6521454335048456 Training: 0.6438144141791707 Seed: 1267\n",
      "Testing: 0.66175967574304 Training: 0.6413578954512859 Seed: 1268\n",
      "Testing: 0.6504135470115794 Training: 0.6441215346990009 Seed: 1275\n",
      "Testing: 0.656753288817695 Training: 0.6424220969734988 Seed: 1276\n",
      "Testing: 0.6525709587387203 Training: 0.6431795334221292 Seed: 1280\n",
      "Testing: 0.6524260247644953 Training: 0.6436199731451596 Seed: 1283\n",
      "Testing: 0.6555698631942355 Training: 0.6427738241005947 Seed: 1284\n",
      "Testing: 0.6504027548446891 Training: 0.6441304217526126 Seed: 1285\n",
      "Testing: 0.6552628044795445 Training: 0.6428546944527802 Seed: 1286\n",
      "Testing: 0.6625363424632965 Training: 0.6408816821370358 Seed: 1287\n",
      "Testing: 0.6479177530255396 Training: 0.6448066706424147 Seed: 1289\n",
      "Testing: 0.6524788382468856 Training: 0.6435727698514753 Seed: 1290\n",
      "Testing: 0.6613844804074549 Training: 0.6413009555617083 Seed: 1291\n",
      "Testing: 0.659587611380994 Training: 0.64203273540576 Seed: 1292\n",
      "Testing: 0.6460288590251053 Training: 0.6451945732839542 Seed: 1293\n",
      "Testing: 0.6545855131962326 Training: 0.6431410675296022 Seed: 1299\n",
      "Testing: 0.6454631199180961 Training: 0.6452086943689521 Seed: 1303\n",
      "Testing: 0.6633088323396177 Training: 0.6408627635843769 Seed: 1305\n",
      "Testing: 0.6522524640521608 Training: 0.6435989972941106 Seed: 1306\n",
      "Testing: 0.6534087151355715 Training: 0.6434009098281229 Seed: 1307\n",
      "Testing: 0.6462524865847075 Training: 0.6451747713430613 Seed: 1308\n",
      "Testing: 0.6559339605727134 Training: 0.6427051609494605 Seed: 1309\n",
      "Testing: 0.64937426581402 Training: 0.6443058248737346 Seed: 1316\n",
      "Testing: 0.6532761637191836 Training: 0.6433403975286309 Seed: 1318\n",
      "Testing: 0.6587213776284644 Training: 0.6421342243733503 Seed: 1320\n",
      "Testing: 0.6456267213106297 Training: 0.6453343221048321 Seed: 1321\n",
      "Testing: 0.6524915773603864 Training: 0.6434739072849431 Seed: 1322\n",
      "Testing: 0.651500633122483 Training: 0.6437341977155453 Seed: 1323\n",
      "Testing: 0.6681049955998066 Training: 0.6397035620741613 Seed: 1324\n",
      "Testing: 0.6560228526049571 Training: 0.6425950565303615 Seed: 1329\n",
      "Testing: 0.6466759293828798 Training: 0.645069396237347 Seed: 1331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6475655264922037 Training: 0.6447212619573401 Seed: 1335\n",
      "Testing: 0.6521425410103155 Training: 0.6437755371106828 Seed: 1337\n",
      "Testing: 0.6538118307893536 Training: 0.6431685280672244 Seed: 1343\n",
      "Testing: 0.6466446888096246 Training: 0.6450234725262687 Seed: 1345\n",
      "Testing: 0.6577214110163517 Training: 0.6422596805793658 Seed: 1346\n",
      "Testing: 0.6509779505151544 Training: 0.6440264847076728 Seed: 1349\n",
      "Testing: 0.6582963802496317 Training: 0.6419601929997208 Seed: 1350\n",
      "Testing: 0.6585040082034119 Training: 0.6422537482576207 Seed: 1351\n",
      "Testing: 0.6606015984451767 Training: 0.6415292017098392 Seed: 1352\n",
      "Testing: 0.6528196970396334 Training: 0.6435071910456918 Seed: 1355\n",
      "Testing: 0.6616782175090022 Training: 0.6412489711412648 Seed: 1356\n",
      "Testing: 0.6523454735399563 Training: 0.6436857833789248 Seed: 1362\n",
      "Testing: 0.6523332308191757 Training: 0.6435774201645753 Seed: 1364\n",
      "Testing: 0.6528700659906467 Training: 0.643344846506583 Seed: 1368\n",
      "Testing: 0.6507839169103927 Training: 0.6440469162321121 Seed: 1370\n",
      "Testing: 0.6576888430095785 Training: 0.6423255150004207 Seed: 1373\n",
      "Testing: 0.6484295996190508 Training: 0.6444365239911828 Seed: 1375\n",
      "Testing: 0.6486913248472065 Training: 0.6444286933839578 Seed: 1377\n",
      "Testing: 0.6498756788928263 Training: 0.6442758629907729 Seed: 1382\n",
      "Testing: 0.6621204084585693 Training: 0.6413614115698066 Seed: 1385\n",
      "Testing: 0.654421898459018 Training: 0.6430421452272294 Seed: 1389\n",
      "Testing: 0.656902181209835 Training: 0.6425237434123359 Seed: 1390\n",
      "Testing: 0.6472832312925632 Training: 0.6448523022696757 Seed: 1391\n",
      "Testing: 0.66239438937759 Training: 0.6409874073755242 Seed: 1393\n",
      "Testing: 0.6460145806634052 Training: 0.6452161299490393 Seed: 1394\n",
      "Testing: 0.6634378875446071 Training: 0.6407139834433818 Seed: 1397\n",
      "Testing: 0.6572677622897887 Training: 0.6423236405761457 Seed: 1400\n",
      "Testing: 0.6547795868656345 Training: 0.6430327463348278 Seed: 1402\n",
      "Testing: 0.6534813204252122 Training: 0.6433468061658407 Seed: 1403\n",
      "Testing: 0.6621642706952574 Training: 0.6412761617820433 Seed: 1406\n",
      "Testing: 0.6698458822817763 Training: 0.6392894065991233 Seed: 1408\n",
      "Testing: 0.6483661084326771 Training: 0.6445806475968061 Seed: 1409\n",
      "Testing: 0.6526231982669726 Training: 0.643542783630628 Seed: 1412\n",
      "Testing: 0.6455951943614349 Training: 0.645293534332318 Seed: 1413\n",
      "Testing: 0.6567525052826998 Training: 0.6419247251815562 Seed: 1414\n",
      "Testing: 0.6539567832745583 Training: 0.6432405541490847 Seed: 1416\n",
      "Testing: 0.6728571887192397 Training: 0.6386755009419006 Seed: 1420\n",
      "Testing: 0.6642501576061912 Training: 0.640519899699338 Seed: 1422\n",
      "Testing: 0.6504077503100925 Training: 0.6441456030944402 Seed: 1428\n",
      "Testing: 0.6487325470079944 Training: 0.6445510503701378 Seed: 1429\n",
      "Testing: 0.6577162696468543 Training: 0.6420815713503905 Seed: 1430\n",
      "Testing: 0.6473185998880577 Training: 0.6449481295236373 Seed: 1431\n",
      "Testing: 0.6574017953572456 Training: 0.6423488251686418 Seed: 1432\n",
      "Testing: 0.6491861161653439 Training: 0.6443792357323852 Seed: 1433\n",
      "Testing: 0.6598807079802003 Training: 0.6416328876671471 Seed: 1435\n",
      "Testing: 0.6466932599292313 Training: 0.6449715117449631 Seed: 1437\n",
      "Testing: 0.6538846701612901 Training: 0.6432764009060679 Seed: 1440\n",
      "Testing: 0.6494340618238386 Training: 0.644165400613167 Seed: 1445\n",
      "Testing: 0.6538503317675789 Training: 0.6429872449201701 Seed: 1446\n",
      "Testing: 0.6477579174006126 Training: 0.643824007856861 Seed: 1447\n",
      "Testing: 0.6512640477500528 Training: 0.6440180915711279 Seed: 1449\n",
      "Testing: 0.658282110606502 Training: 0.6420351731102335 Seed: 1452\n",
      "Testing: 0.6491612297367819 Training: 0.6443545795550881 Seed: 1453\n",
      "Testing: 0.6468267726745229 Training: 0.6450741319610725 Seed: 1455\n",
      "Testing: 0.6576125934020911 Training: 0.6422422406023617 Seed: 1456\n",
      "Testing: 0.6489937907035243 Training: 0.644373017639527 Seed: 1461\n",
      "Testing: 0.6464439612121132 Training: 0.6450140272283315 Seed: 1463\n",
      "Testing: 0.6564957654931315 Training: 0.6425908449865181 Seed: 1464\n",
      "Testing: 0.6455345321035766 Training: 0.6453585682755139 Seed: 1468\n",
      "Testing: 0.6528997014037765 Training: 0.6435164320863896 Seed: 1469\n",
      "Testing: 0.647372409304814 Training: 0.6448378843811924 Seed: 1472\n",
      "Testing: 0.6599898089418091 Training: 0.641469937179731 Seed: 1475\n",
      "Testing: 0.6602672925518577 Training: 0.6412046361528767 Seed: 1476\n",
      "Testing: 0.645733456373606 Training: 0.6453381660823669 Seed: 1477\n",
      "Testing: 0.662583388021714 Training: 0.641088172248581 Seed: 1478\n",
      "Testing: 0.6672444952685335 Training: 0.6396286111498859 Seed: 1481\n",
      "Testing: 0.6457202338286282 Training: 0.6451468453483077 Seed: 1482\n",
      "Testing: 0.6576287952696829 Training: 0.6421488397836804 Seed: 1484\n",
      "Testing: 0.645379684057275 Training: 0.645378709449894 Seed: 1487\n",
      "Testing: 0.6533101525050279 Training: 0.6434402455716154 Seed: 1497\n",
      "Testing: 0.6489543540336913 Training: 0.6445244075788776 Seed: 1498\n",
      "Testing: 0.6475491238068952 Training: 0.6447171334331581 Seed: 1500\n",
      "Testing: 0.650704406285201 Training: 0.6440911181330811 Seed: 1504\n",
      "Testing: 0.6463625061973123 Training: 0.6450751727211946 Seed: 1505\n",
      "Testing: 0.6460182127087932 Training: 0.6451810243119979 Seed: 1506\n",
      "Testing: 0.6611541056285563 Training: 0.6414756045788366 Seed: 1508\n",
      "Testing: 0.6602008763685197 Training: 0.6415914453785295 Seed: 1509\n",
      "Testing: 0.6458432538216783 Training: 0.6452199193108901 Seed: 1510\n",
      "Testing: 0.6538645603306785 Training: 0.6431758931820765 Seed: 1511\n",
      "Testing: 0.6488576707372018 Training: 0.6442007848877158 Seed: 1513\n",
      "Testing: 0.6458745034255144 Training: 0.644946927631998 Seed: 1514\n",
      "Testing: 0.660859905108196 Training: 0.641317055765371 Seed: 1517\n",
      "Testing: 0.6530021641915631 Training: 0.6433487674195648 Seed: 1519\n",
      "Testing: 0.6567238973309026 Training: 0.6423606056718942 Seed: 1521\n",
      "Testing: 0.6628067027989811 Training: 0.6409359058208602 Seed: 1525\n",
      "Testing: 0.6515755641120691 Training: 0.6436500920660766 Seed: 1527\n",
      "Testing: 0.6489884554480486 Training: 0.6442842434054892 Seed: 1529\n",
      "Testing: 0.6537096177495101 Training: 0.6431798758605936 Seed: 1530\n",
      "Testing: 0.6620060501044386 Training: 0.6413354590562024 Seed: 1531\n",
      "Testing: 0.6554403144447121 Training: 0.6428716922474567 Seed: 1535\n",
      "Testing: 0.6572834408422905 Training: 0.6423695855346029 Seed: 1536\n",
      "Testing: 0.6588698662880726 Training: 0.6419297323375566 Seed: 1538\n",
      "Testing: 0.6548906527058488 Training: 0.6429416676719657 Seed: 1539\n",
      "Testing: 0.6514604716774569 Training: 0.6437393177328066 Seed: 1544\n",
      "Testing: 0.6480030465967695 Training: 0.6446762853589139 Seed: 1546\n",
      "Testing: 0.6500991163736698 Training: 0.6443184286268047 Seed: 1547\n",
      "Testing: 0.6671572616085416 Training: 0.6395417812138984 Seed: 1548\n",
      "Testing: 0.6487097732069386 Training: 0.6445584291123947 Seed: 1552\n",
      "Testing: 0.6613452426975557 Training: 0.6413005804522327 Seed: 1553\n",
      "Testing: 0.6533197722353565 Training: 0.6431993137021077 Seed: 1555\n",
      "Testing: 0.6667441415120369 Training: 0.6400014135388895 Seed: 1557\n",
      "Testing: 0.6585164190096402 Training: 0.6421694747142891 Seed: 1558\n",
      "Testing: 0.6477593639812724 Training: 0.6439111612455113 Seed: 1559\n",
      "Testing: 0.6645174155554738 Training: 0.6402939846213246 Seed: 1561\n",
      "Testing: 0.65773817277679 Training: 0.6422576892691021 Seed: 1563\n",
      "Testing: 0.657421819356568 Training: 0.6424173557078727 Seed: 1564\n",
      "Testing: 0.6468979523196545 Training: 0.6450092601617363 Seed: 1565\n",
      "Testing: 0.6540768077510687 Training: 0.6430633079870862 Seed: 1566\n",
      "Testing: 0.659335225499965 Training: 0.6419571285646527 Seed: 1569\n",
      "Testing: 0.6523043119420051 Training: 0.6435625430668022 Seed: 1570\n",
      "Testing: 0.6619185675558734 Training: 0.6411982178015503 Seed: 1571\n",
      "Testing: 0.6509449769616195 Training: 0.6438375455786995 Seed: 1579\n",
      "Testing: 0.6746254273679454 Training: 0.6377410575489786 Seed: 1580\n",
      "Testing: 0.6554570261369477 Training: 0.6426733104839915 Seed: 1581\n",
      "Testing: 0.6471578475697415 Training: 0.644879266589081 Seed: 1584\n",
      "Testing: 0.6675819886994384 Training: 0.6396025302873503 Seed: 1585\n",
      "Testing: 0.6472658815841543 Training: 0.6448718468858032 Seed: 1589\n",
      "Testing: 0.6587285256481469 Training: 0.6420379726460002 Seed: 1590\n",
      "Testing: 0.6572755178699292 Training: 0.6425079236549627 Seed: 1592\n",
      "Testing: 0.6473168398538246 Training: 0.6448682852231784 Seed: 1593\n",
      "Testing: 0.6549976968099906 Training: 0.6428536115556416 Seed: 1595\n",
      "Testing: 0.6603358834633495 Training: 0.6412946422484979 Seed: 1596\n",
      "Testing: 0.6580918504211706 Training: 0.6417660472845654 Seed: 1598\n",
      "Testing: 0.6567970073919978 Training: 0.6425763979810478 Seed: 1602\n",
      "Testing: 0.6545817844683143 Training: 0.642868615463406 Seed: 1604\n",
      "Testing: 0.6529544072583048 Training: 0.6434853071581375 Seed: 1605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6459486426441781 Training: 0.6451779521009203 Seed: 1607\n",
      "Testing: 0.6591425019810244 Training: 0.6419111709503349 Seed: 1608\n",
      "Testing: 0.6548758654676871 Training: 0.6429349950940881 Seed: 1609\n",
      "Testing: 0.6622260271917909 Training: 0.6410918773168696 Seed: 1614\n",
      "Testing: 0.6509818368992536 Training: 0.64388877227389 Seed: 1615\n",
      "Testing: 0.6599272065074694 Training: 0.6414565913108259 Seed: 1616\n",
      "Testing: 0.6589896679255338 Training: 0.6419357692700569 Seed: 1617\n",
      "Testing: 0.6507009350773094 Training: 0.6440901899630889 Seed: 1621\n",
      "Testing: 0.6504682422497841 Training: 0.6440377406576395 Seed: 1623\n",
      "Testing: 0.6480461828975738 Training: 0.6446861831937412 Seed: 1624\n",
      "Testing: 0.6587838774277617 Training: 0.6420425329857807 Seed: 1625\n",
      "Testing: 0.6635818331154433 Training: 0.6407563349166696 Seed: 1629\n",
      "Testing: 0.6463824808086367 Training: 0.6450773674681483 Seed: 1632\n",
      "Testing: 0.6592753481891641 Training: 0.6418544070647406 Seed: 1634\n",
      "Testing: 0.6536930028020906 Training: 0.6432967621232453 Seed: 1635\n",
      "Testing: 0.6474440180915287 Training: 0.6448502241228202 Seed: 1637\n",
      "Testing: 0.6560971853648043 Training: 0.642661772931854 Seed: 1641\n",
      "Testing: 0.6482899215549012 Training: 0.6443354581994029 Seed: 1642\n",
      "Testing: 0.6578808476732187 Training: 0.6422645381555523 Seed: 1650\n",
      "Testing: 0.6517116988971818 Training: 0.6438067971207128 Seed: 1651\n",
      "Testing: 0.6459800829551399 Training: 0.6452617734990063 Seed: 1653\n",
      "Testing: 0.6477840986551753 Training: 0.6447623804764793 Seed: 1656\n",
      "Testing: 0.6581155391444208 Training: 0.6420837955566314 Seed: 1657\n",
      "Testing: 0.6504491701345307 Training: 0.6441595266199394 Seed: 1658\n",
      "Testing: 0.659552484206184 Training: 0.6418763811171506 Seed: 1661\n",
      "Testing: 0.6652756417649945 Training: 0.6404057585754384 Seed: 1662\n",
      "Testing: 0.6522129232907257 Training: 0.6437275089517077 Seed: 1664\n",
      "Testing: 0.6468885077033775 Training: 0.6444680924388698 Seed: 1665\n",
      "Testing: 0.6481300866102606 Training: 0.6438399888984123 Seed: 1666\n",
      "Testing: 0.6500313516460967 Training: 0.6441982673432018 Seed: 1671\n",
      "Testing: 0.6591354703316084 Training: 0.6420348631754134 Seed: 1672\n",
      "Testing: 0.6581857390331175 Training: 0.6421857899292962 Seed: 1673\n",
      "Testing: 0.6504944385911134 Training: 0.6439879130903912 Seed: 1678\n",
      "Testing: 0.6519684748132116 Training: 0.6436804499051112 Seed: 1680\n",
      "Testing: 0.649364794794683 Training: 0.6442616740290157 Seed: 1691\n",
      "Testing: 0.6632264199138321 Training: 0.640709072126666 Seed: 1692\n",
      "Testing: 0.6523283561495369 Training: 0.6436742554757946 Seed: 1693\n",
      "Testing: 0.6500202187193828 Training: 0.6442458340389695 Seed: 1694\n",
      "Testing: 0.6519159386149854 Training: 0.6437021216063116 Seed: 1696\n",
      "Testing: 0.6530434177484157 Training: 0.6433451055413801 Seed: 1698\n",
      "Testing: 0.657136654263255 Training: 0.6424795073866194 Seed: 1699\n",
      "Testing: 0.6471194532963536 Training: 0.6447446267278587 Seed: 1701\n",
      "Testing: 0.6536640186599604 Training: 0.6433908107571327 Seed: 1703\n",
      "Testing: 0.646953264208429 Training: 0.6450274432471179 Seed: 1705\n",
      "Testing: 0.6645871999480351 Training: 0.6405824208881086 Seed: 1706\n",
      "Testing: 0.649555884863548 Training: 0.6443318269957645 Seed: 1708\n",
      "Testing: 0.6487725829959143 Training: 0.6445071285872367 Seed: 1711\n",
      "Testing: 0.6529116600812419 Training: 0.6434749097104063 Seed: 1714\n",
      "Testing: 0.6517574875071612 Training: 0.6437191454756994 Seed: 1715\n",
      "Testing: 0.6460597415558434 Training: 0.6452134727096168 Seed: 1716\n",
      "Testing: 0.6456320033233884 Training: 0.6452894761694083 Seed: 1718\n",
      "Testing: 0.6522549039974278 Training: 0.6436632133938182 Seed: 1719\n",
      "Testing: 0.6568653115143595 Training: 0.6423608809014781 Seed: 1720\n",
      "Testing: 0.6498967904629934 Training: 0.6443295808786864 Seed: 1722\n",
      "Testing: 0.6574289223254992 Training: 0.6423429263953844 Seed: 1725\n",
      "Testing: 0.6536786618378571 Training: 0.6433586499856349 Seed: 1729\n",
      "Testing: 0.6511326049106415 Training: 0.6439405162217807 Seed: 1734\n",
      "Testing: 0.6518298449149713 Training: 0.643542053520141 Seed: 1735\n",
      "Testing: 0.6495986466228271 Training: 0.6443099500621221 Seed: 1739\n",
      "Testing: 0.6601124519135445 Training: 0.6416722039011217 Seed: 1741\n",
      "Testing: 0.6489319212639106 Training: 0.6444497583797426 Seed: 1743\n",
      "Testing: 0.6496207795635061 Training: 0.643877863097527 Seed: 1744\n",
      "Testing: 0.6591045233594615 Training: 0.6415414175158076 Seed: 1745\n",
      "Testing: 0.6582703713615414 Training: 0.6421283139607488 Seed: 1746\n",
      "Testing: 0.6557619521957416 Training: 0.6427863974241601 Seed: 1748\n",
      "Testing: 0.6597336321912094 Training: 0.6417951598887762 Seed: 1751\n",
      "Testing: 0.6535934762908907 Training: 0.6433738375173741 Seed: 1752\n",
      "Testing: 0.6600312164700708 Training: 0.6415793999083356 Seed: 1753\n",
      "Testing: 0.6477939231925172 Training: 0.6446819188382212 Seed: 1754\n",
      "Testing: 0.6462169844926544 Training: 0.6452183934018382 Seed: 1757\n",
      "Testing: 0.6608734303946712 Training: 0.6414232253047834 Seed: 1758\n",
      "Testing: 0.6458034420979912 Training: 0.6453095677438602 Seed: 1761\n",
      "Testing: 0.6464869592036664 Training: 0.645061522292325 Seed: 1766\n",
      "Testing: 0.651825507871726 Training: 0.6437812746299687 Seed: 1770\n",
      "Testing: 0.6586919977629868 Training: 0.6420107675667952 Seed: 1771\n",
      "Testing: 0.6485870829319319 Training: 0.644636553051024 Seed: 1774\n",
      "Testing: 0.6500300981795919 Training: 0.6440853171604148 Seed: 1776\n",
      "Testing: 0.6560227613733736 Training: 0.6426580715762942 Seed: 1778\n",
      "Testing: 0.650481312702204 Training: 0.644151482571554 Seed: 1780\n",
      "Testing: 0.6455498498699774 Training: 0.645341625925451 Seed: 1781\n",
      "Testing: 0.6535953005234836 Training: 0.6432730782821637 Seed: 1782\n",
      "Testing: 0.6579585219810459 Training: 0.6422278565264691 Seed: 1783\n",
      "Testing: 0.6518774817310709 Training: 0.6436270099193636 Seed: 1785\n",
      "Testing: 0.6536700134719667 Training: 0.643277407288025 Seed: 1787\n",
      "Testing: 0.6470394511445895 Training: 0.6448989831100604 Seed: 1788\n",
      "Testing: 0.6523888616706555 Training: 0.6436560290775352 Seed: 1795\n",
      "Testing: 0.6477599186995 Training: 0.6445247969378278 Seed: 1796\n",
      "Testing: 0.6550081345747469 Training: 0.6430650695965714 Seed: 1799\n",
      "Testing: 0.6586285227926961 Training: 0.6421298228367492 Seed: 1800\n",
      "Testing: 0.66343156057471 Training: 0.6408268418399388 Seed: 1801\n",
      "Testing: 0.6604913743220131 Training: 0.6415203035991726 Seed: 1803\n",
      "Testing: 0.6480763593022024 Training: 0.644443037010964 Seed: 1804\n",
      "Testing: 0.6461031320930182 Training: 0.6452148355122796 Seed: 1807\n",
      "Testing: 0.649300508137049 Training: 0.6443404346207363 Seed: 1808\n",
      "Testing: 0.6554456921475071 Training: 0.6428778020512961 Seed: 1811\n",
      "Testing: 0.6501360966653681 Training: 0.64429789308211 Seed: 1815\n",
      "Testing: 0.6660965785562296 Training: 0.639940198458474 Seed: 1820\n",
      "Testing: 0.6584814359031299 Training: 0.6420577472421729 Seed: 1823\n",
      "Testing: 0.6482164538530585 Training: 0.6445581739774857 Seed: 1827\n",
      "Testing: 0.6585923112125938 Training: 0.6420187284761285 Seed: 1828\n",
      "Testing: 0.6595314396880279 Training: 0.6420071562824116 Seed: 1834\n",
      "Testing: 0.6594594873291562 Training: 0.6417048343784034 Seed: 1837\n",
      "Testing: 0.6484290146773248 Training: 0.6445573244170634 Seed: 1839\n",
      "Testing: 0.6619636805195518 Training: 0.6413396058129497 Seed: 1840\n",
      "Testing: 0.6563293170361442 Training: 0.6426148909134777 Seed: 1843\n",
      "Testing: 0.6474220917242557 Training: 0.6448332603006861 Seed: 1845\n",
      "Testing: 0.6563316392039766 Training: 0.6424857655361449 Seed: 1847\n",
      "Testing: 0.6491987321551846 Training: 0.6443346200005968 Seed: 1858\n",
      "Testing: 0.6513571789903909 Training: 0.6439227078402168 Seed: 1859\n",
      "Testing: 0.6503657210154045 Training: 0.6439853652313912 Seed: 1863\n",
      "Testing: 0.6576871383844306 Training: 0.6422468035674025 Seed: 1872\n",
      "Testing: 0.6476409748320116 Training: 0.6448562144322417 Seed: 1873\n",
      "Testing: 0.6559157341320306 Training: 0.6425630717922097 Seed: 1877\n",
      "Testing: 0.650918711178194 Training: 0.6439584615250322 Seed: 1878\n",
      "Testing: 0.6470879142727392 Training: 0.6449673880705077 Seed: 1880\n",
      "Testing: 0.6562082947360177 Training: 0.6427708424412519 Seed: 1881\n",
      "Testing: 0.6505405822586308 Training: 0.6439719345623887 Seed: 1883\n",
      "Testing: 0.6504801469302898 Training: 0.6441308901891987 Seed: 1890\n",
      "Testing: 0.6508625551322851 Training: 0.6440848544546802 Seed: 1893\n",
      "Testing: 0.656489655548675 Training: 0.6417481732124548 Seed: 1899\n",
      "Testing: 0.6527757931944183 Training: 0.6434543301493789 Seed: 1901\n",
      "Testing: 0.6504673384297395 Training: 0.6440751799820341 Seed: 1905\n",
      "Testing: 0.6513920712364119 Training: 0.643949131670039 Seed: 1908\n",
      "Testing: 0.6569843692795402 Training: 0.6425225150480615 Seed: 1909\n",
      "Testing: 0.6512045317674089 Training: 0.6439358527737562 Seed: 1911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6501772734604182 Training: 0.6441901810785062 Seed: 1912\n",
      "Testing: 0.6457221654785383 Training: 0.6453084675926183 Seed: 1913\n",
      "Testing: 0.6501545908146891 Training: 0.6440912312719996 Seed: 1916\n",
      "Testing: 0.6459699197307048 Training: 0.6450824305674208 Seed: 1920\n",
      "Testing: 0.649243320064146 Training: 0.644424027530655 Seed: 1929\n",
      "Testing: 0.6463735191008683 Training: 0.6450703820890258 Seed: 1937\n",
      "Testing: 0.6565152312026772 Training: 0.6425558497943169 Seed: 1938\n",
      "Testing: 0.6477171384142772 Training: 0.6446789638539226 Seed: 1939\n",
      "Testing: 0.6609364207633485 Training: 0.6414590147043813 Seed: 1940\n",
      "Testing: 0.6534840519659956 Training: 0.6434264713926849 Seed: 1943\n",
      "Testing: 0.6544748932357813 Training: 0.6431518089940325 Seed: 1945\n",
      "Testing: 0.646324462667251 Training: 0.6451869209911177 Seed: 1950\n",
      "Testing: 0.6581150051827191 Training: 0.6423167325022658 Seed: 1951\n",
      "Testing: 0.6507357504291978 Training: 0.6437459536387583 Seed: 1952\n",
      "Testing: 0.6510077579636455 Training: 0.6438844245174028 Seed: 1953\n",
      "Testing: 0.6561773432882576 Training: 0.6425922532583297 Seed: 1954\n",
      "Testing: 0.6468078838481265 Training: 0.6448875836831298 Seed: 1958\n",
      "Testing: 0.6462761943785656 Training: 0.6451635291231917 Seed: 1963\n",
      "Testing: 0.652137900909304 Training: 0.6430224263388196 Seed: 1964\n",
      "Testing: 0.6508618834483731 Training: 0.6439773258510616 Seed: 1965\n",
      "Testing: 0.6611787535130803 Training: 0.6414571938915592 Seed: 1966\n",
      "Testing: 0.645749219411301 Training: 0.6450965824934181 Seed: 1967\n",
      "Testing: 0.6459301738742207 Training: 0.6453282453144674 Seed: 1968\n",
      "Testing: 0.6500069254794155 Training: 0.64431846740467 Seed: 1969\n",
      "Testing: 0.6569258553033044 Training: 0.6423538444878093 Seed: 1971\n",
      "Testing: 0.6811064112810422 Training: 0.6362310620844351 Seed: 1973\n",
      "Testing: 0.6546744007290333 Training: 0.6430951780707479 Seed: 1976\n",
      "Testing: 0.6506309615224461 Training: 0.644024846620066 Seed: 1978\n",
      "Testing: 0.6498496581993718 Training: 0.6437849064700956 Seed: 1980\n",
      "Testing: 0.6456067536700844 Training: 0.6453807218327592 Seed: 1982\n",
      "Testing: 0.6482958101723661 Training: 0.6445906391429139 Seed: 1985\n",
      "Testing: 0.6646149882307274 Training: 0.6404530633741161 Seed: 1987\n",
      "Testing: 0.6526975311725035 Training: 0.643622915084639 Seed: 1989\n",
      "Testing: 0.6481484420879344 Training: 0.6445618229351763 Seed: 1991\n",
      "Testing: 0.6453925630589361 Training: 0.6453131206669384 Seed: 1992\n",
      "Testing: 0.6506215267629749 Training: 0.6440547448611753 Seed: 1993\n",
      "Testing: 0.6515922832553864 Training: 0.6438194067406922 Seed: 1994\n",
      "Testing: 0.6474266805310418 Training: 0.6446741597481164 Seed: 1998\n",
      "Testing: 0.6470912537909365 Training: 0.6450004669294989 Seed: 2000\n",
      "Testing: 0.6472294423933992 Training: 0.6445319898098284 Seed: 2001\n",
      "Testing: 0.6525623379435684 Training: 0.6435464597089715 Seed: 2002\n",
      "Testing: 0.6526881985241253 Training: 0.6434936843337262 Seed: 2003\n",
      "Testing: 0.6646118096930744 Training: 0.640416966677861 Seed: 2005\n",
      "Testing: 0.6483049896986555 Training: 0.6446940680699742 Seed: 2009\n",
      "Testing: 0.6617757042134151 Training: 0.6411543336729084 Seed: 2015\n",
      "Testing: 0.6541261119284871 Training: 0.6431826869869721 Seed: 2016\n",
      "Testing: 0.6534776119566105 Training: 0.643281863547982 Seed: 2017\n",
      "Testing: 0.6533492859517482 Training: 0.6433634356971454 Seed: 2018\n",
      "Testing: 0.6492717383903113 Training: 0.6442928532963943 Seed: 2019\n",
      "Testing: 0.6480933088543094 Training: 0.6446465093829199 Seed: 2020\n",
      "Testing: 0.663029676079662 Training: 0.6407881132206092 Seed: 2021\n",
      "Testing: 0.6461926348586708 Training: 0.6451776825083324 Seed: 2022\n",
      "Testing: 0.6512372839687371 Training: 0.6437750020147055 Seed: 2023\n",
      "Testing: 0.650505038381785 Training: 0.6440635241328458 Seed: 2024\n",
      "Testing: 0.6513097559165945 Training: 0.6438565039863072 Seed: 2028\n",
      "Testing: 0.6580705198462466 Training: 0.6421817133045474 Seed: 2030\n",
      "Testing: 0.6584934476590462 Training: 0.641942311793411 Seed: 2037\n",
      "Testing: 0.6454120860384993 Training: 0.6453994298702426 Seed: 2038\n",
      "Testing: 0.6464275006708495 Training: 0.6451741033566283 Seed: 2039\n",
      "Testing: 0.6511168647722283 Training: 0.6438588933506584 Seed: 2040\n",
      "Testing: 0.6523257260923372 Training: 0.6436824791152624 Seed: 2041\n",
      "Testing: 0.6541966845409637 Training: 0.6431923024520754 Seed: 2043\n",
      "Testing: 0.6478403494957312 Training: 0.6446311595114486 Seed: 2044\n",
      "Testing: 0.6712004989457925 Training: 0.6391520894986223 Seed: 2045\n",
      "Testing: 0.6553948422004363 Training: 0.6426757170293986 Seed: 2046\n",
      "Testing: 0.6490248351770082 Training: 0.6444471784612467 Seed: 2051\n",
      "Testing: 0.6464630427455814 Training: 0.6451325195340161 Seed: 2054\n",
      "Testing: 0.6479630860985248 Training: 0.6447565987926396 Seed: 2055\n",
      "Testing: 0.6513602165357635 Training: 0.643884102536993 Seed: 2057\n",
      "Testing: 0.6497139365547436 Training: 0.6443321994934895 Seed: 2058\n",
      "Testing: 0.650779917646711 Training: 0.6440948285706184 Seed: 2059\n",
      "Testing: 0.6546754682076288 Training: 0.6429337271980247 Seed: 2061\n",
      "Testing: 0.6470252286445319 Training: 0.6449333020601806 Seed: 2062\n",
      "Testing: 0.6570068060206621 Training: 0.6425847195831006 Seed: 2063\n",
      "Testing: 0.6484017503459355 Training: 0.6445836102324795 Seed: 2065\n",
      "Testing: 0.6495256781810744 Training: 0.6442699586495841 Seed: 2066\n",
      "Testing: 0.6531635734122028 Training: 0.6433884368736539 Seed: 2069\n",
      "Testing: 0.6531675726672383 Training: 0.6434353099373008 Seed: 2072\n",
      "Testing: 0.6722920164688032 Training: 0.6382471043016837 Seed: 2073\n",
      "Testing: 0.6518457572420944 Training: 0.6436650911189342 Seed: 2074\n",
      "Testing: 0.6456147721784119 Training: 0.6453616254656829 Seed: 2076\n",
      "Testing: 0.667298375328558 Training: 0.6399559334525534 Seed: 2077\n",
      "Testing: 0.6478998152896248 Training: 0.6447268970562019 Seed: 2078\n",
      "Testing: 0.6464863900447397 Training: 0.6450421295929902 Seed: 2080\n",
      "Testing: 0.653443700374693 Training: 0.6433309520881068 Seed: 2082\n",
      "Testing: 0.6579961372211582 Training: 0.6421547489797389 Seed: 2086\n",
      "Testing: 0.657258191943427 Training: 0.6422974607568277 Seed: 2088\n",
      "Testing: 0.6483729822595461 Training: 0.644622354149692 Seed: 2090\n",
      "Testing: 0.6656617453524875 Training: 0.6401805766059158 Seed: 2093\n",
      "Testing: 0.6656430325226594 Training: 0.639983014029859 Seed: 2094\n",
      "Testing: 0.6632464954674445 Training: 0.6406690271934966 Seed: 2096\n",
      "Testing: 0.6474541587500106 Training: 0.6448251430067113 Seed: 2099\n",
      "Testing: 0.6461904584736105 Training: 0.6451530537416083 Seed: 2100\n",
      "Testing: 0.646087179497312 Training: 0.6452521882279757 Seed: 2101\n",
      "Testing: 0.6522001519497511 Training: 0.6436608948931551 Seed: 2102\n",
      "Testing: 0.6487691303588556 Training: 0.6445445416051879 Seed: 2108\n",
      "Testing: 0.6511653883688062 Training: 0.6438901613256639 Seed: 2112\n",
      "Testing: 0.6533101522930381 Training: 0.6432473530004821 Seed: 2113\n",
      "Testing: 0.6498550397781915 Training: 0.6443292507937542 Seed: 2115\n",
      "Testing: 0.6467470312760043 Training: 0.6450051511020103 Seed: 2116\n",
      "Testing: 0.656514022502415 Training: 0.6424558920056123 Seed: 2117\n",
      "Testing: 0.6655610519981531 Training: 0.6403345773313407 Seed: 2121\n",
      "Testing: 0.6471818241033387 Training: 0.6449359502656562 Seed: 2124\n",
      "Testing: 0.6469712432087844 Training: 0.6450087626682073 Seed: 2127\n",
      "Testing: 0.6592890809387554 Training: 0.6419885865285463 Seed: 2131\n",
      "Testing: 0.6557832583043015 Training: 0.6428067171245169 Seed: 2139\n",
      "Testing: 0.6519911085625115 Training: 0.6437468607987554 Seed: 2140\n",
      "Testing: 0.6491395374850657 Training: 0.6443347387338721 Seed: 2141\n",
      "Testing: 0.6457378055100654 Training: 0.6452942306822766 Seed: 2144\n",
      "Testing: 0.6501806355182153 Training: 0.6441526617112201 Seed: 2145\n",
      "Testing: 0.650095028333894 Training: 0.6441037437590039 Seed: 2146\n",
      "Testing: 0.6563143177129468 Training: 0.6426093602676933 Seed: 2147\n",
      "Testing: 0.6545129305072839 Training: 0.6431888078328012 Seed: 2151\n",
      "Testing: 0.6537020492257737 Training: 0.6433333167903467 Seed: 2153\n",
      "Testing: 0.6466480970307673 Training: 0.64503987104837 Seed: 2158\n",
      "Testing: 0.662592172630103 Training: 0.6409768534186139 Seed: 2159\n",
      "Testing: 0.650512596698492 Training: 0.6440288493690924 Seed: 2160\n",
      "Testing: 0.6653235032887953 Training: 0.6402969466745276 Seed: 2161\n",
      "Testing: 0.6456496430135148 Training: 0.6446307606940865 Seed: 2162\n",
      "Testing: 0.6532433758341276 Training: 0.6435628433600072 Seed: 2163\n",
      "Testing: 0.6552265781121687 Training: 0.6430262747050082 Seed: 2165\n",
      "Testing: 0.6630632782921357 Training: 0.6409941292167411 Seed: 2166\n",
      "Testing: 0.6634848508459248 Training: 0.6407820836118369 Seed: 2171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6462022622037492 Training: 0.6451474366262608 Seed: 2174\n",
      "Testing: 0.6486481941197954 Training: 0.6445279623235449 Seed: 2178\n",
      "Testing: 0.6475718057305802 Training: 0.6448441818377082 Seed: 2180\n",
      "Testing: 0.6600519440715783 Training: 0.6416269442046473 Seed: 2182\n",
      "Testing: 0.6564771644849299 Training: 0.6426647611674511 Seed: 2185\n",
      "Testing: 0.6532214313808787 Training: 0.6433877631837016 Seed: 2187\n",
      "Testing: 0.6527815248742407 Training: 0.6435533627668978 Seed: 2189\n",
      "Testing: 0.6466096178240914 Training: 0.645066575373034 Seed: 2191\n",
      "Testing: 0.6602005750263559 Training: 0.6417877089842682 Seed: 2195\n",
      "Testing: 0.6602731006505917 Training: 0.6416354480572026 Seed: 2196\n",
      "Testing: 0.6499828597415174 Training: 0.6440988909727359 Seed: 2197\n",
      "Testing: 0.6687817496249073 Training: 0.6393531894143312 Seed: 2198\n",
      "Testing: 0.6490600491207497 Training: 0.6445357746865309 Seed: 2199\n",
      "Testing: 0.6512232260615709 Training: 0.6438392574189321 Seed: 2201\n",
      "Testing: 0.6697439399118672 Training: 0.6391840103613751 Seed: 2202\n",
      "Testing: 0.6502348813320108 Training: 0.6441518692762442 Seed: 2204\n",
      "Testing: 0.655526435975923 Training: 0.6428575153186907 Seed: 2205\n",
      "Testing: 0.6495096552418389 Training: 0.6443108763941343 Seed: 2206\n",
      "Testing: 0.6541338002525858 Training: 0.6432684756196382 Seed: 2207\n",
      "Testing: 0.6497466266139232 Training: 0.6442611055901739 Seed: 2209\n",
      "Testing: 0.6490314435009252 Training: 0.6444374765402743 Seed: 2210\n",
      "Testing: 0.6548878674667897 Training: 0.6430075578321646 Seed: 2213\n",
      "Testing: 0.6534389747879268 Training: 0.6434418295670573 Seed: 2218\n",
      "Testing: 0.6468027543833909 Training: 0.6449460895369157 Seed: 2219\n",
      "Testing: 0.6619952513422991 Training: 0.6412253146075494 Seed: 2220\n",
      "Testing: 0.6467156290910052 Training: 0.6450394983992295 Seed: 2221\n",
      "Testing: 0.6495671007232727 Training: 0.6441375275316359 Seed: 2224\n",
      "Testing: 0.6547287468191618 Training: 0.6430206285778928 Seed: 2225\n",
      "Testing: 0.6550995408405749 Training: 0.6429567491432651 Seed: 2226\n",
      "Testing: 0.6465257572512252 Training: 0.6450293286940404 Seed: 2228\n",
      "Testing: 0.6550817689730162 Training: 0.6429440089113737 Seed: 2230\n",
      "Testing: 0.6500856920813027 Training: 0.6441633110337583 Seed: 2231\n",
      "Testing: 0.6553479416989547 Training: 0.6427819155680942 Seed: 2232\n",
      "Testing: 0.6506115980312682 Training: 0.644111828465979 Seed: 2234\n",
      "Testing: 0.6497549290725075 Training: 0.644331413560024 Seed: 2235\n",
      "Testing: 0.6479232148835191 Training: 0.6446997042063222 Seed: 2241\n",
      "Testing: 0.6714498289249855 Training: 0.638835114267231 Seed: 2243\n",
      "Testing: 0.6517278655472409 Training: 0.6437330494083626 Seed: 2244\n",
      "Testing: 0.6579112240189755 Training: 0.6420047931899887 Seed: 2245\n",
      "Testing: 0.6451708697156258 Training: 0.6449172248484583 Seed: 2248\n",
      "Testing: 0.6458535989166168 Training: 0.6453440619119442 Seed: 2249\n",
      "Testing: 0.6508543702449157 Training: 0.6439169104132821 Seed: 2250\n",
      "Testing: 0.645582563498285 Training: 0.6451956660691782 Seed: 2252\n",
      "Testing: 0.6667140143400119 Training: 0.6397080802030851 Seed: 2254\n",
      "Testing: 0.6475045609134703 Training: 0.6448979013907218 Seed: 2255\n",
      "Testing: 0.6547083525894807 Training: 0.6429455735658522 Seed: 2257\n",
      "Testing: 0.6621118432730572 Training: 0.641146801320604 Seed: 2258\n",
      "Testing: 0.6489677599098322 Training: 0.6442479776031576 Seed: 2262\n",
      "Testing: 0.6497824691556758 Training: 0.6443366211170372 Seed: 2263\n",
      "Testing: 0.6458081738643145 Training: 0.6452766877766898 Seed: 2267\n",
      "Testing: 0.6598740637361589 Training: 0.6417883693386646 Seed: 2268\n",
      "Testing: 0.6511112616802431 Training: 0.6439214191535535 Seed: 2270\n",
      "Testing: 0.6464447740292963 Training: 0.6450339805521101 Seed: 2271\n",
      "Testing: 0.6594332226345503 Training: 0.64190942523456 Seed: 2273\n",
      "Testing: 0.6490391312048595 Training: 0.644398773362527 Seed: 2274\n",
      "Testing: 0.6557607962136581 Training: 0.6427993534354888 Seed: 2275\n",
      "Testing: 0.6559689044504043 Training: 0.6429015338227173 Seed: 2277\n",
      "Testing: 0.64621898673422 Training: 0.6452546927433828 Seed: 2278\n",
      "Testing: 0.6679214040222413 Training: 0.6395654640348438 Seed: 2282\n",
      "Testing: 0.6566688506900039 Training: 0.6424087002323062 Seed: 2283\n",
      "Testing: 0.6463126737830327 Training: 0.6451781048998984 Seed: 2286\n",
      "Testing: 0.657477613382415 Training: 0.642086550914751 Seed: 2290\n",
      "Testing: 0.6605182580347986 Training: 0.641572249369997 Seed: 2292\n",
      "Testing: 0.6513754087947161 Training: 0.6438606511152475 Seed: 2296\n",
      "Testing: 0.6544550008738068 Training: 0.6430258515041439 Seed: 2300\n",
      "Testing: 0.6624691806141579 Training: 0.6411413734792449 Seed: 2302\n",
      "Testing: 0.6514606774938252 Training: 0.6438569901992482 Seed: 2303\n",
      "Testing: 0.659769009520373 Training: 0.6414709869850617 Seed: 2306\n",
      "Testing: 0.6536698983353592 Training: 0.6429939072224656 Seed: 2308\n",
      "Testing: 0.6569087187290767 Training: 0.6424989551111229 Seed: 2309\n",
      "Testing: 0.6465807606234857 Training: 0.6450018354271845 Seed: 2310\n",
      "Testing: 0.6517982526728322 Training: 0.6439302988391341 Seed: 2313\n",
      "Testing: 0.6474782356994894 Training: 0.6448546389967948 Seed: 2316\n",
      "Testing: 0.6559550245105644 Training: 0.6427212205048641 Seed: 2323\n",
      "Testing: 0.6554657031058811 Training: 0.6428726125224439 Seed: 2325\n",
      "Testing: 0.6490596289197574 Training: 0.6444728407094783 Seed: 2328\n",
      "Testing: 0.6460614440363179 Training: 0.6451357451859265 Seed: 2330\n",
      "Testing: 0.6678091521241873 Training: 0.6395911288182929 Seed: 2334\n",
      "Testing: 0.6625327334172881 Training: 0.6408770743807688 Seed: 2336\n",
      "Testing: 0.6470142737329363 Training: 0.6450290299565777 Seed: 2341\n",
      "Testing: 0.6594580689075018 Training: 0.6417291912528075 Seed: 2342\n",
      "Testing: 0.6508872396800733 Training: 0.6439014853722673 Seed: 2343\n",
      "Testing: 0.6653534491155546 Training: 0.6403038799085055 Seed: 2345\n",
      "Testing: 0.6604205244987525 Training: 0.6416785115656212 Seed: 2351\n",
      "Testing: 0.6499799258239516 Training: 0.6439361130111942 Seed: 2352\n",
      "Testing: 0.6506922554928444 Training: 0.6440368040946987 Seed: 2353\n",
      "Testing: 0.6519002256472815 Training: 0.643822504478291 Seed: 2354\n",
      "Testing: 0.6536471341862821 Training: 0.6433188566785724 Seed: 2355\n",
      "Testing: 0.647086187532306 Training: 0.6448374552935918 Seed: 2357\n",
      "Testing: 0.6696504156832634 Training: 0.6392689365754872 Seed: 2358\n",
      "Testing: 0.6551207068129893 Training: 0.6428687906482168 Seed: 2359\n",
      "Testing: 0.6465284456647817 Training: 0.6445746318048735 Seed: 2361\n",
      "Testing: 0.6532282863960782 Training: 0.6434791272381297 Seed: 2362\n",
      "Testing: 0.6504613061115846 Training: 0.644085123044245 Seed: 2364\n",
      "Testing: 0.6465447709640681 Training: 0.6450249987149537 Seed: 2368\n",
      "Testing: 0.6526405478526628 Training: 0.6435771782443144 Seed: 2370\n",
      "Testing: 0.6528627344779824 Training: 0.6435617804837075 Seed: 2372\n",
      "Testing: 0.6535719852310985 Training: 0.6433575739114801 Seed: 2373\n",
      "Testing: 0.6654476380088752 Training: 0.6402718662878812 Seed: 2377\n",
      "Testing: 0.6572660907187415 Training: 0.6424410404549031 Seed: 2379\n",
      "Testing: 0.648440475763022 Training: 0.6446231383496729 Seed: 2382\n",
      "Testing: 0.6521937055713787 Training: 0.6436405233094332 Seed: 2384\n",
      "Testing: 0.6519938452764868 Training: 0.643489235338879 Seed: 2385\n",
      "Testing: 0.6475017620032257 Training: 0.6448572895648705 Seed: 2386\n",
      "Testing: 0.6464734601353099 Training: 0.645001290136592 Seed: 2390\n",
      "Testing: 0.6505291848828201 Training: 0.6441164920849756 Seed: 2391\n",
      "Testing: 0.6461687452350966 Training: 0.6451549859160921 Seed: 2392\n",
      "Testing: 0.6594645133444577 Training: 0.6418981746062646 Seed: 2394\n",
      "Testing: 0.6512358937322834 Training: 0.6438768363010341 Seed: 2395\n",
      "Testing: 0.6636147906205526 Training: 0.6409044520168816 Seed: 2396\n",
      "Testing: 0.6484757674316545 Training: 0.6446526048751551 Seed: 2397\n",
      "Testing: 0.6564156572123097 Training: 0.6426548036901003 Seed: 2400\n",
      "Testing: 0.6601138243674765 Training: 0.6414862087839939 Seed: 2401\n",
      "Testing: 0.6526737330190485 Training: 0.6435142286482426 Seed: 2403\n",
      "Testing: 0.6582028304887827 Training: 0.6422848780375499 Seed: 2405\n",
      "Testing: 0.6476899816309528 Training: 0.6447432210460765 Seed: 2406\n",
      "Testing: 0.64680780404099 Training: 0.645038469841819 Seed: 2407\n",
      "Testing: 0.6459786715902436 Training: 0.6451918432775011 Seed: 2408\n",
      "Testing: 0.650991666978436 Training: 0.6439303690830899 Seed: 2410\n",
      "Testing: 0.6508445734304766 Training: 0.6440475177795884 Seed: 2414\n",
      "Testing: 0.662282995505745 Training: 0.6412499452092567 Seed: 2415\n",
      "Testing: 0.6474147180092933 Training: 0.6449418425355872 Seed: 2416\n",
      "Testing: 0.6499202801281381 Training: 0.6442318254398317 Seed: 2420\n",
      "Testing: 0.6630835209064446 Training: 0.640938709185658 Seed: 2421\n",
      "Testing: 0.647686889653972 Training: 0.6447652038784994 Seed: 2422\n",
      "Testing: 0.6596032399106203 Training: 0.6416183066688095 Seed: 2424\n",
      "Testing: 0.648301318101085 Training: 0.6445243973541547 Seed: 2426\n",
      "Testing: 0.6603594239725538 Training: 0.6417278149471648 Seed: 2428\n",
      "Testing: 0.6622596730854661 Training: 0.6411339593642921 Seed: 2429\n",
      "Testing: 0.6690850279391855 Training: 0.6396115072607459 Seed: 2430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6466097409843908 Training: 0.6450459723259512 Seed: 2435\n",
      "Testing: 0.6537546034999028 Training: 0.6430036725656618 Seed: 2441\n",
      "Testing: 0.6545560756659217 Training: 0.6431444699096331 Seed: 2445\n",
      "Testing: 0.6509000031056799 Training: 0.6439732901027748 Seed: 2446\n",
      "Testing: 0.6461041496890341 Training: 0.6451391124163952 Seed: 2449\n",
      "Testing: 0.6514195226515851 Training: 0.6438775673942577 Seed: 2451\n",
      "Testing: 0.6460244459472966 Training: 0.6450390731672473 Seed: 2459\n",
      "Testing: 0.6508774752403984 Training: 0.6440319995899445 Seed: 2466\n",
      "Testing: 0.679348088244752 Training: 0.6366935501343439 Seed: 2467\n",
      "Testing: 0.672297797855001 Training: 0.638576167574855 Seed: 2468\n",
      "Testing: 0.6497850281880971 Training: 0.6443239446551224 Seed: 2469\n",
      "Testing: 0.6479997672300636 Training: 0.6447279664233184 Seed: 2470\n",
      "Testing: 0.657499990548956 Training: 0.6423802155469118 Seed: 2474\n",
      "Testing: 0.6574146076701456 Training: 0.6422230549172375 Seed: 2475\n",
      "Testing: 0.6484729245895086 Training: 0.6441915955536816 Seed: 2479\n",
      "Testing: 0.6490839720877619 Training: 0.6444227488547892 Seed: 2480\n",
      "Testing: 0.6462508850766213 Training: 0.6450699713600132 Seed: 2482\n",
      "Testing: 0.6454501391654537 Training: 0.6453192368429308 Seed: 2483\n",
      "Testing: 0.6478642306923201 Training: 0.6447524346298069 Seed: 2486\n",
      "Testing: 0.6597885234584411 Training: 0.6415149894081751 Seed: 2488\n",
      "Testing: 0.64732494985681 Training: 0.6449002926077274 Seed: 2489\n",
      "Testing: 0.6453858042594727 Training: 0.6452613162057246 Seed: 2490\n",
      "Testing: 0.6453765288860074 Training: 0.6453542013634839 Seed: 2492\n",
      "Testing: 0.6551156416261708 Training: 0.642916081480541 Seed: 2494\n",
      "Testing: 0.656278162609985 Training: 0.6426969349650127 Seed: 2495\n",
      "Testing: 0.652208698071862 Training: 0.6437182787022333 Seed: 2496\n",
      "Testing: 0.6594814524537295 Training: 0.6416518386419101 Seed: 2497\n",
      "Testing: 0.6494073783089261 Training: 0.6444550536758238 Seed: 2499\n",
      "Testing: 0.6555095118418959 Training: 0.6428163566084061 Seed: 2502\n",
      "Testing: 0.6454397042495831 Training: 0.6453914933162315 Seed: 2503\n",
      "Testing: 0.6595778745097202 Training: 0.6418322855792068 Seed: 2508\n",
      "Testing: 0.6493758281859501 Training: 0.6443588148084519 Seed: 2510\n",
      "Testing: 0.6572221512102613 Training: 0.6422686329186512 Seed: 2511\n",
      "Testing: 0.6465178459529166 Training: 0.6450501408813522 Seed: 2513\n",
      "Testing: 0.650999819603719 Training: 0.6437739453315862 Seed: 2515\n",
      "Testing: 0.6546868968811386 Training: 0.643009634057127 Seed: 2516\n",
      "Testing: 0.6523908995558035 Training: 0.6436609251604569 Seed: 2518\n",
      "Testing: 0.656916029614313 Training: 0.6425309717734363 Seed: 2520\n",
      "Testing: 0.647012667894743 Training: 0.6450361061046176 Seed: 2522\n",
      "Testing: 0.6461125072890435 Training: 0.645069121063965 Seed: 2523\n",
      "Testing: 0.6601833992324567 Training: 0.6416706337899447 Seed: 2524\n",
      "Testing: 0.654887205347539 Training: 0.642915563788306 Seed: 2526\n",
      "Testing: 0.6614703734549089 Training: 0.6412794136970028 Seed: 2527\n",
      "Testing: 0.6549191269493893 Training: 0.6429641840097515 Seed: 2528\n",
      "Testing: 0.647259917394776 Training: 0.6447870734007408 Seed: 2529\n",
      "Testing: 0.6579520112197349 Training: 0.642251936459779 Seed: 2530\n",
      "Testing: 0.6591767686792459 Training: 0.6418729116400319 Seed: 2531\n",
      "Testing: 0.6594296026997813 Training: 0.6418234520995424 Seed: 2533\n",
      "Testing: 0.6665915180546751 Training: 0.6400086254861378 Seed: 2535\n",
      "Testing: 0.6457000272052944 Training: 0.6452737116766633 Seed: 2537\n",
      "Testing: 0.6589017463327517 Training: 0.6420438257875575 Seed: 2538\n",
      "Testing: 0.6485198295025397 Training: 0.6445407512364565 Seed: 2540\n",
      "Testing: 0.6472897071041036 Training: 0.6448773541883829 Seed: 2541\n",
      "Testing: 0.6498036608158646 Training: 0.6442774730633749 Seed: 2542\n",
      "Testing: 0.6588870548275539 Training: 0.6420537471435785 Seed: 2545\n",
      "Testing: 0.6472751800906051 Training: 0.644929099911401 Seed: 2547\n",
      "Testing: 0.6461992648035575 Training: 0.6452331747128645 Seed: 2548\n",
      "Testing: 0.6493693767186237 Training: 0.6441628763746443 Seed: 2551\n",
      "Testing: 0.6554821773783127 Training: 0.6429129681128695 Seed: 2554\n",
      "Testing: 0.6669369388924287 Training: 0.6399188199662172 Seed: 2555\n",
      "Testing: 0.6484295200848155 Training: 0.6446028884759071 Seed: 2556\n",
      "Testing: 0.6484947156805845 Training: 0.6445790991667147 Seed: 2558\n",
      "Testing: 0.6553834490610477 Training: 0.6427609947959658 Seed: 2559\n",
      "Testing: 0.6584548736650798 Training: 0.6421668682244649 Seed: 2561\n",
      "Testing: 0.6478486620853354 Training: 0.6447564569821439 Seed: 2562\n",
      "Testing: 0.6572757388693014 Training: 0.6423440356507595 Seed: 2563\n",
      "Testing: 0.6603809582217084 Training: 0.6414813268793997 Seed: 2564\n",
      "Testing: 0.6478386071123013 Training: 0.6447880825878949 Seed: 2565\n",
      "Testing: 0.6494457018493303 Training: 0.644389211857206 Seed: 2566\n",
      "Testing: 0.6621315507937018 Training: 0.6411312352170104 Seed: 2570\n",
      "Testing: 0.6467006001440542 Training: 0.6449603917510958 Seed: 2571\n",
      "Testing: 0.6490804988104295 Training: 0.6441236768368035 Seed: 2574\n",
      "Testing: 0.6582181907910225 Training: 0.6419739526037036 Seed: 2575\n",
      "Testing: 0.6615780806902277 Training: 0.6413000494207373 Seed: 2576\n",
      "Testing: 0.6549514842314479 Training: 0.6430412799507477 Seed: 2577\n",
      "Testing: 0.6491301952044823 Training: 0.6444740137826244 Seed: 2578\n",
      "Testing: 0.6478117552597286 Training: 0.6448074361764868 Seed: 2581\n",
      "Testing: 0.6596686429051846 Training: 0.6415789326676766 Seed: 2583\n",
      "Testing: 0.665775230039278 Training: 0.6402021136928244 Seed: 2584\n",
      "Testing: 0.6599602230705495 Training: 0.6416775402991379 Seed: 2586\n",
      "Testing: 0.662201075587667 Training: 0.6410692024709383 Seed: 2588\n",
      "Testing: 0.6569752497368642 Training: 0.6424582236233984 Seed: 2589\n",
      "Testing: 0.6489834750817125 Training: 0.644438341237953 Seed: 2592\n",
      "Testing: 0.648484681234424 Training: 0.6445697717986936 Seed: 2594\n",
      "Testing: 0.6479774625332391 Training: 0.6447419768087594 Seed: 2599\n",
      "Testing: 0.6590104517822939 Training: 0.6418681262342221 Seed: 2600\n",
      "Testing: 0.6535514874825367 Training: 0.6432534485881741 Seed: 2601\n",
      "Testing: 0.6490408235301787 Training: 0.6441143555590769 Seed: 2604\n",
      "Testing: 0.661441367672897 Training: 0.6412761269043763 Seed: 2607\n",
      "Testing: 0.6456479898555592 Training: 0.6453098607982601 Seed: 2610\n",
      "Testing: 0.6661199236249951 Training: 0.6404038622109494 Seed: 2617\n",
      "Testing: 0.658918692140694 Training: 0.6419975326385038 Seed: 2618\n",
      "Testing: 0.6518706497230357 Training: 0.6438087875849179 Seed: 2620\n",
      "Testing: 0.6465025285801154 Training: 0.6450801988455632 Seed: 2622\n",
      "Testing: 0.6496819189393803 Training: 0.6443397435760458 Seed: 2623\n",
      "Testing: 0.6513527581541899 Training: 0.6434629678449955 Seed: 2624\n",
      "Testing: 0.649286690790408 Training: 0.6442929060030878 Seed: 2627\n",
      "Testing: 0.6467111768394939 Training: 0.6450116053319296 Seed: 2629\n",
      "Testing: 0.6496883544776417 Training: 0.6443352356211249 Seed: 2630\n",
      "Testing: 0.6687469767402104 Training: 0.6394290353089132 Seed: 2633\n",
      "Testing: 0.6469708720827105 Training: 0.6450130973868635 Seed: 2634\n",
      "Testing: 0.6483126454762831 Training: 0.6446629247180765 Seed: 2635\n",
      "Testing: 0.6510626485442214 Training: 0.6439529680362688 Seed: 2636\n",
      "Testing: 0.6604031358073464 Training: 0.6416212437716426 Seed: 2637\n",
      "Testing: 0.650013705906201 Training: 0.6441885728853514 Seed: 2644\n",
      "Testing: 0.6599986026706196 Training: 0.6414852220057714 Seed: 2645\n",
      "Testing: 0.661192937978725 Training: 0.6415319326694999 Seed: 2646\n",
      "Testing: 0.6508220167957091 Training: 0.6440271121879858 Seed: 2647\n",
      "Testing: 0.6608863947519185 Training: 0.641510427926101 Seed: 2649\n",
      "Testing: 0.6517258364232743 Training: 0.643802194077685 Seed: 2651\n",
      "Testing: 0.6462622690448632 Training: 0.6443608632524495 Seed: 2655\n",
      "Testing: 0.6513301375296895 Training: 0.6437954934731969 Seed: 2657\n",
      "Testing: 0.6546452634500217 Training: 0.6429660565252613 Seed: 2658\n",
      "Testing: 0.654144948242262 Training: 0.6427719300199952 Seed: 2666\n",
      "Testing: 0.6518348349492529 Training: 0.6437448030043071 Seed: 2670\n",
      "Testing: 0.6475990908622771 Training: 0.6448309780155669 Seed: 2672\n",
      "Testing: 0.6472300568830098 Training: 0.6449180117472914 Seed: 2674\n",
      "Testing: 0.6568850759417274 Training: 0.6424406093227186 Seed: 2676\n",
      "Testing: 0.6458167916161959 Training: 0.645201609896316 Seed: 2679\n",
      "Testing: 0.6561289212371509 Training: 0.6427614362674364 Seed: 2684\n",
      "Testing: 0.653603414879315 Training: 0.6432547949599435 Seed: 2685\n",
      "Testing: 0.6488945727147746 Training: 0.6445141313560092 Seed: 2686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6595308624325136 Training: 0.6418754217284843 Seed: 2687\n",
      "Testing: 0.6495939812778532 Training: 0.6443418727159065 Seed: 2689\n",
      "Testing: 0.6510820405988272 Training: 0.6440474064021446 Seed: 2694\n",
      "Testing: 0.6484357369622191 Training: 0.6446364152070493 Seed: 2695\n",
      "Testing: 0.6579969194985099 Training: 0.6420029649085264 Seed: 2696\n",
      "Testing: 0.6485698951634128 Training: 0.6442509975192215 Seed: 2699\n",
      "Testing: 0.6512153020406726 Training: 0.6437622428025733 Seed: 2700\n",
      "Testing: 0.6522046688333365 Training: 0.6436875651924943 Seed: 2701\n",
      "Testing: 0.6552983693063203 Training: 0.6428962043002852 Seed: 2702\n",
      "Testing: 0.6490055162847785 Training: 0.6445014893494815 Seed: 2704\n",
      "Testing: 0.651255652026228 Training: 0.6438559561363437 Seed: 2705\n",
      "Testing: 0.6502454006719948 Training: 0.6441303828002178 Seed: 2708\n",
      "Testing: 0.6516122008190984 Training: 0.6438257062167156 Seed: 2709\n",
      "Testing: 0.6463645091314677 Training: 0.6451251912862237 Seed: 2710\n",
      "Testing: 0.6481450761441503 Training: 0.6445944136513421 Seed: 2717\n",
      "Testing: 0.6481389565290717 Training: 0.6447351697411965 Seed: 2718\n",
      "Testing: 0.645784995244882 Training: 0.6453120090383342 Seed: 2720\n",
      "Testing: 0.6517413357412388 Training: 0.643868283562585 Seed: 2722\n",
      "Testing: 0.6547394999952203 Training: 0.6429979191772036 Seed: 2726\n",
      "Testing: 0.6638273445357277 Training: 0.6406199657544789 Seed: 2727\n",
      "Testing: 0.6582020679038619 Training: 0.6416347260777377 Seed: 2728\n",
      "Testing: 0.6462740853688869 Training: 0.6452144122811785 Seed: 2729\n",
      "Testing: 0.6612123219794349 Training: 0.6415041081976421 Seed: 2730\n",
      "Testing: 0.658906789994874 Training: 0.642087857900377 Seed: 2731\n",
      "Testing: 0.6476890890630728 Training: 0.6448320270198333 Seed: 2733\n",
      "Testing: 0.6568621334707844 Training: 0.6420025527331319 Seed: 2735\n",
      "Testing: 0.663969115071601 Training: 0.6404763435218079 Seed: 2739\n",
      "Testing: 0.6490297734095399 Training: 0.6445499421633372 Seed: 2740\n",
      "Testing: 0.6460045652882507 Training: 0.6452750970451476 Seed: 2741\n",
      "Testing: 0.6453050295911085 Training: 0.6452735001871474 Seed: 2742\n",
      "Testing: 0.6555175491986199 Training: 0.6426045414425035 Seed: 2743\n",
      "Testing: 0.6548056404936196 Training: 0.6430215188818218 Seed: 2747\n",
      "Testing: 0.6531166655134798 Training: 0.6434826265071376 Seed: 2748\n",
      "Testing: 0.645600331470927 Training: 0.6453763323152995 Seed: 2750\n",
      "Testing: 0.6487983276344775 Training: 0.6441413623742303 Seed: 2754\n",
      "Testing: 0.6505889169877377 Training: 0.6441222394572288 Seed: 2755\n",
      "Testing: 0.6488437830707643 Training: 0.6444491108571414 Seed: 2756\n",
      "Testing: 0.6454878959280128 Training: 0.6453934490609731 Seed: 2757\n",
      "Testing: 0.6507003754460381 Training: 0.6440292505021292 Seed: 2758\n",
      "Testing: 0.6547565217688232 Training: 0.6429752920245626 Seed: 2759\n",
      "Testing: 0.6469244700588115 Training: 0.6449859154557849 Seed: 2760\n",
      "Testing: 0.6467160133530565 Training: 0.6450404779088463 Seed: 2761\n",
      "Testing: 0.6471427881003066 Training: 0.6448903751386915 Seed: 2763\n",
      "Testing: 0.6599881971106278 Training: 0.6415809893552565 Seed: 2764\n",
      "Testing: 0.6461776191116333 Training: 0.6448578945505197 Seed: 2766\n",
      "Testing: 0.6465132526902979 Training: 0.6450606265874379 Seed: 2771\n",
      "Testing: 0.6493775607927983 Training: 0.6443619723737541 Seed: 2774\n",
      "Testing: 0.6579117493110448 Training: 0.6421405062981387 Seed: 2775\n",
      "Testing: 0.6497857044657246 Training: 0.6442873661440136 Seed: 2776\n",
      "Testing: 0.6481956383212033 Training: 0.6447238965463595 Seed: 2777\n",
      "Testing: 0.6508302535178578 Training: 0.6441101320175052 Seed: 2778\n",
      "Testing: 0.6509961102391861 Training: 0.6439447017068327 Seed: 2787\n",
      "Testing: 0.6636189798443372 Training: 0.6407616658171322 Seed: 2788\n",
      "Testing: 0.6460245866007237 Training: 0.6452286035053426 Seed: 2791\n",
      "Testing: 0.6522904291513064 Training: 0.6436921952227919 Seed: 2792\n",
      "Testing: 0.66723346559654 Training: 0.6400652601750295 Seed: 2793\n",
      "Testing: 0.653236607729438 Training: 0.6432063448902396 Seed: 2799\n",
      "Testing: 0.6507187044188656 Training: 0.6439858614819411 Seed: 2800\n",
      "Testing: 0.6525058186635218 Training: 0.6434985089247384 Seed: 2806\n",
      "Testing: 0.662287494548388 Training: 0.640966620965166 Seed: 2807\n",
      "Testing: 0.668537389433048 Training: 0.639484529113476 Seed: 2810\n",
      "Testing: 0.6560242339341121 Training: 0.6426862309574009 Seed: 2815\n",
      "Testing: 0.659280812488043 Training: 0.6417919151626099 Seed: 2817\n",
      "Testing: 0.6486637531732214 Training: 0.6445419506971675 Seed: 2818\n",
      "Testing: 0.6506480030324988 Training: 0.6439992054403122 Seed: 2819\n",
      "Testing: 0.6577558323304312 Training: 0.6423501132402125 Seed: 2820\n",
      "Testing: 0.6509534203780963 Training: 0.6438354910848254 Seed: 2821\n",
      "Testing: 0.6456610257536146 Training: 0.6453432318185887 Seed: 2822\n",
      "Testing: 0.6489755547049967 Training: 0.644225950245634 Seed: 2824\n",
      "Testing: 0.6488213884166325 Training: 0.6445888288731156 Seed: 2825\n",
      "Testing: 0.6470533045291217 Training: 0.644934948087579 Seed: 2829\n",
      "Testing: 0.654892256085553 Training: 0.6430331626909771 Seed: 2834\n",
      "Testing: 0.6467056296342768 Training: 0.6450169577532396 Seed: 2838\n",
      "Testing: 0.6521321973611647 Training: 0.6437784336770336 Seed: 2841\n",
      "Testing: 0.6707548639876344 Training: 0.6389183587217008 Seed: 2843\n",
      "Testing: 0.651131095750185 Training: 0.6438939898820446 Seed: 2849\n",
      "Testing: 0.6481872624177692 Training: 0.6447178312148645 Seed: 2851\n",
      "Testing: 0.6517015512757287 Training: 0.6437675786668342 Seed: 2852\n",
      "Testing: 0.6538035169746969 Training: 0.6431880665585675 Seed: 2853\n",
      "Testing: 0.6467840986468543 Training: 0.6450329853020387 Seed: 2856\n",
      "Testing: 0.6604789087684814 Training: 0.6415671598189356 Seed: 2857\n",
      "Testing: 0.65785958317734 Training: 0.6422656093360386 Seed: 2859\n",
      "Testing: 0.6502621497362346 Training: 0.6441586558208634 Seed: 2863\n",
      "Testing: 0.6608394096626307 Training: 0.6414769332795278 Seed: 2865\n",
      "Testing: 0.6561489940505096 Training: 0.6426838869698699 Seed: 2869\n",
      "Testing: 0.6503084036963539 Training: 0.6440389487860132 Seed: 2871\n",
      "Testing: 0.6474692403775243 Training: 0.6448459071208238 Seed: 2872\n",
      "Testing: 0.6486838568981022 Training: 0.6445342140064716 Seed: 2873\n",
      "Testing: 0.6468226611659927 Training: 0.6441580306768304 Seed: 2877\n",
      "Testing: 0.6497323645358947 Training: 0.6442724517605661 Seed: 2878\n",
      "Testing: 0.6533321292639012 Training: 0.6433431649084096 Seed: 2879\n",
      "Testing: 0.6534030636469212 Training: 0.6432320374957501 Seed: 2883\n",
      "Testing: 0.655004880720772 Training: 0.642241290313419 Seed: 2888\n",
      "Testing: 0.6484111162235464 Training: 0.6446576259849883 Seed: 2889\n",
      "Testing: 0.6521292308247459 Training: 0.6437093626549713 Seed: 2890\n",
      "Testing: 0.6536881713311349 Training: 0.6432121663241787 Seed: 2896\n",
      "Testing: 0.6576261206216766 Training: 0.6423417902797263 Seed: 2897\n",
      "Testing: 0.6515370460958845 Training: 0.6438380945856772 Seed: 2901\n",
      "Testing: 0.6460588913387042 Training: 0.645055355132567 Seed: 2902\n",
      "Testing: 0.6490963715793603 Training: 0.6444954951607424 Seed: 2903\n",
      "Testing: 0.6482570018995547 Training: 0.6446103324980834 Seed: 2904\n",
      "Testing: 0.6468135162004597 Training: 0.6449016397444722 Seed: 2905\n",
      "Testing: 0.6573270655458711 Training: 0.6424505826437068 Seed: 2906\n",
      "Testing: 0.6454885110147082 Training: 0.6454021960988556 Seed: 2911\n",
      "Testing: 0.654662711210425 Training: 0.6430222536479535 Seed: 2920\n",
      "Testing: 0.6542823824387377 Training: 0.6432371234235683 Seed: 2924\n",
      "Testing: 0.6543955025426019 Training: 0.6431010104888633 Seed: 2925\n",
      "Testing: 0.6471160684427701 Training: 0.6449440476309545 Seed: 2926\n",
      "Testing: 0.6480694018606534 Training: 0.6447065146260577 Seed: 2927\n",
      "Testing: 0.6490200880342205 Training: 0.6445449416014528 Seed: 2929\n",
      "Testing: 0.652780813040928 Training: 0.6436047970860527 Seed: 2931\n",
      "Testing: 0.6519086201998294 Training: 0.6437485553990429 Seed: 2934\n",
      "Testing: 0.663902906877145 Training: 0.6403720040076046 Seed: 2935\n",
      "Testing: 0.6468599760771936 Training: 0.6449686551075784 Seed: 2937\n",
      "Testing: 0.6507751361272553 Training: 0.6440351006254071 Seed: 2938\n",
      "Testing: 0.6501794200380845 Training: 0.6442091596182868 Seed: 2944\n",
      "Testing: 0.6528575910873256 Training: 0.6436335725841105 Seed: 2949\n",
      "Testing: 0.6544804048888474 Training: 0.6431278380840412 Seed: 2950\n",
      "Testing: 0.6581083832636987 Training: 0.6422468932664398 Seed: 2951\n",
      "Testing: 0.6595043921502848 Training: 0.6417433415819769 Seed: 2954\n",
      "Testing: 0.6486404741343073 Training: 0.6445254720523228 Seed: 2955\n",
      "Testing: 0.6549056645281199 Training: 0.642948981324523 Seed: 2956\n",
      "Testing: 0.6565742318385623 Training: 0.6423771310505992 Seed: 2959\n",
      "Testing: 0.6485031445097371 Training: 0.6445429363810948 Seed: 2960\n",
      "Testing: 0.6624412671240437 Training: 0.6412245203876914 Seed: 2962\n",
      "Testing: 0.6540684185894878 Training: 0.6431257124838774 Seed: 2965\n",
      "Testing: 0.6454658523066513 Training: 0.6450662528076835 Seed: 2967\n",
      "Testing: 0.6546083445414579 Training: 0.6428202614351551 Seed: 2969\n",
      "Testing: 0.6489752642153137 Training: 0.6444785071326052 Seed: 2971\n",
      "Testing: 0.6455246346613726 Training: 0.6453011665125733 Seed: 2972\n",
      "Testing: 0.6488531431670421 Training: 0.6444528704519743 Seed: 2975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6459605651864135 Training: 0.6452248916599009 Seed: 2976\n",
      "Testing: 0.66126857035359 Training: 0.6414319376336093 Seed: 2978\n",
      "Testing: 0.6515552649806359 Training: 0.6437285903767749 Seed: 2981\n",
      "Testing: 0.6630556509883385 Training: 0.6409989595901703 Seed: 2982\n",
      "Testing: 0.6508186529957886 Training: 0.6439763277958361 Seed: 2989\n",
      "Testing: 0.673929123761672 Training: 0.6382164645081235 Seed: 2990\n",
      "Testing: 0.6542679418220765 Training: 0.6427980956138079 Seed: 2992\n",
      "Testing: 0.6473706272634332 Training: 0.6448793688410677 Seed: 2993\n",
      "Testing: 0.6492402749345256 Training: 0.6444117261709248 Seed: 2995\n",
      "Testing: 0.6499912845128029 Training: 0.6442563118572203 Seed: 2997\n",
      "Testing: 0.666268959505919 Training: 0.6400642634885789 Seed: 2998\n",
      "Testing: 0.6579841985564404 Training: 0.6421998351084902 Seed: 2999\n",
      "Testing: 0.6535799039615902 Training: 0.6433111269946546 Seed: 3001\n",
      "Testing: 0.6667429588506141 Training: 0.6398094256809475 Seed: 3002\n",
      "Testing: 0.6549882322651503 Training: 0.6429552763669834 Seed: 3004\n",
      "Testing: 0.6486230224642306 Training: 0.6445362818961244 Seed: 3006\n",
      "Testing: 0.6492777873477247 Training: 0.6444184369120693 Seed: 3007\n",
      "Testing: 0.6496776199170704 Training: 0.6443555192824927 Seed: 3009\n",
      "Testing: 0.6520788470942122 Training: 0.6436113919264208 Seed: 3014\n",
      "Testing: 0.6728028523877935 Training: 0.6381947042445522 Seed: 3015\n",
      "Testing: 0.6480994386407932 Training: 0.6445432292126033 Seed: 3016\n",
      "Testing: 0.6597667865957199 Training: 0.6416754477853177 Seed: 3017\n",
      "Testing: 0.6559459213234647 Training: 0.6425815361968716 Seed: 3018\n",
      "Testing: 0.6567593920310517 Training: 0.6426188103817755 Seed: 3019\n",
      "Testing: 0.6531273964073087 Training: 0.6433682780318039 Seed: 3021\n",
      "Testing: 0.6526287211648012 Training: 0.6434918765359827 Seed: 3024\n",
      "Testing: 0.6519675377257962 Training: 0.643774246799536 Seed: 3026\n",
      "Testing: 0.654535457499171 Training: 0.6427562126422273 Seed: 3027\n",
      "Testing: 0.645996031665537 Training: 0.645209590478106 Seed: 3029\n",
      "Testing: 0.6484270410797 Training: 0.6446570914437382 Seed: 3031\n",
      "Testing: 0.6474317986521703 Training: 0.6448898327349787 Seed: 3032\n",
      "Testing: 0.6512653176931963 Training: 0.6439119590581714 Seed: 3036\n",
      "Testing: 0.6562260883202247 Training: 0.6424647753366733 Seed: 3037\n",
      "Testing: 0.6472481423265612 Training: 0.64492467434457 Seed: 3039\n",
      "Testing: 0.6647639692900102 Training: 0.6405465034374865 Seed: 3040\n",
      "Testing: 0.6652423841737285 Training: 0.6404174036186139 Seed: 3041\n",
      "Testing: 0.6578555454356437 Training: 0.642156535369577 Seed: 3044\n",
      "Testing: 0.649024475110445 Training: 0.6444854417934253 Seed: 3049\n",
      "Testing: 0.6559990689221622 Training: 0.6426891532666531 Seed: 3051\n",
      "Testing: 0.6469787437478738 Training: 0.6446624259216067 Seed: 3052\n",
      "Testing: 0.6485496179149328 Training: 0.6446115307234219 Seed: 3053\n",
      "Testing: 0.6519216818721971 Training: 0.6437548284694581 Seed: 3057\n",
      "Testing: 0.6551056705635918 Training: 0.6427643379541722 Seed: 3059\n",
      "Testing: 0.6539344235386184 Training: 0.643131853463405 Seed: 3060\n",
      "Testing: 0.6668359881042795 Training: 0.6400023132938756 Seed: 3061\n",
      "Testing: 0.656485769514928 Training: 0.642507375873679 Seed: 3062\n",
      "Testing: 0.6587718207394841 Training: 0.6421207537450623 Seed: 3066\n",
      "Testing: 0.6476823774859539 Training: 0.644523290784557 Seed: 3068\n",
      "Testing: 0.6474751147150568 Training: 0.6448567514090581 Seed: 3069\n",
      "Testing: 0.6529037408725417 Training: 0.6434503699999627 Seed: 3070\n",
      "Testing: 0.6454335728455945 Training: 0.64536301999338 Seed: 3071\n",
      "Testing: 0.6501236344628137 Training: 0.644130736517943 Seed: 3073\n",
      "Testing: 0.6542713730416403 Training: 0.6431193847091486 Seed: 3077\n",
      "Testing: 0.6516653503873868 Training: 0.643854429980156 Seed: 3078\n",
      "Testing: 0.6606346990977254 Training: 0.6415689665401871 Seed: 3079\n",
      "Testing: 0.6481995434738584 Training: 0.6446130222608388 Seed: 3080\n",
      "Testing: 0.6487544096510398 Training: 0.6445579585300397 Seed: 3082\n",
      "Testing: 0.6533751988037727 Training: 0.6434088862020739 Seed: 3085\n",
      "Testing: 0.6481648823867202 Training: 0.6446757245706667 Seed: 3087\n",
      "Testing: 0.6785920628918571 Training: 0.6367738385507198 Seed: 3088\n",
      "Testing: 0.6460782929360751 Training: 0.6451934936924635 Seed: 3089\n",
      "Testing: 0.6587273216732654 Training: 0.6420415000266969 Seed: 3091\n",
      "Testing: 0.6484010855738266 Training: 0.6444821748676202 Seed: 3092\n",
      "Testing: 0.6491621820411683 Training: 0.644479584433903 Seed: 3094\n",
      "Testing: 0.6485947344396489 Training: 0.64459239608493 Seed: 3096\n",
      "Testing: 0.6475390076679349 Training: 0.644847911723802 Seed: 3098\n",
      "Testing: 0.6565537373248184 Training: 0.6425861947060925 Seed: 3105\n",
      "Testing: 0.6546326110430586 Training: 0.6430307418132412 Seed: 3106\n",
      "Testing: 0.6489752656106977 Training: 0.6445122390751946 Seed: 3110\n",
      "Testing: 0.6560907638398362 Training: 0.6426612988719381 Seed: 3113\n",
      "Testing: 0.6481354046912904 Training: 0.6446624825892805 Seed: 3114\n",
      "Testing: 0.6494006458743664 Training: 0.6443610013798086 Seed: 3116\n",
      "Testing: 0.6647382496642198 Training: 0.6402752801244388 Seed: 3117\n",
      "Testing: 0.655623672911435 Training: 0.6428107556550862 Seed: 3125\n",
      "Testing: 0.6537654247055874 Training: 0.6432736826109753 Seed: 3126\n",
      "Testing: 0.6696261066159952 Training: 0.6389740980020593 Seed: 3127\n",
      "Testing: 0.6548504308900819 Training: 0.6430628734504825 Seed: 3128\n",
      "Testing: 0.652524084698866 Training: 0.6435082040468411 Seed: 3130\n",
      "Testing: 0.6477030384175585 Training: 0.6448330944326257 Seed: 3133\n",
      "Testing: 0.6496778955745508 Training: 0.6442312099652086 Seed: 3134\n",
      "Testing: 0.6577080874118436 Training: 0.6421295730051657 Seed: 3135\n",
      "Testing: 0.6746543832036175 Training: 0.6378730657777599 Seed: 3136\n",
      "Testing: 0.6569935723440288 Training: 0.6423555624027438 Seed: 3137\n",
      "Testing: 0.6723901045646955 Training: 0.6387078187322212 Seed: 3138\n",
      "Testing: 0.6488891770016899 Training: 0.6445275438875188 Seed: 3142\n",
      "Testing: 0.6483328904741132 Training: 0.6445942644951902 Seed: 3144\n",
      "Testing: 0.6477053667371206 Training: 0.6448441753664278 Seed: 3146\n",
      "Testing: 0.6477857194686766 Training: 0.6447671688759702 Seed: 3148\n",
      "Testing: 0.6523190823058325 Training: 0.6436344475768282 Seed: 3152\n",
      "Testing: 0.6567558840091304 Training: 0.6425793844791128 Seed: 3155\n",
      "Testing: 0.6482278649318265 Training: 0.6446971328680813 Seed: 3160\n",
      "Testing: 0.646258278794522 Training: 0.6451880280400863 Seed: 3161\n",
      "Testing: 0.6560725003005853 Training: 0.6427624692797672 Seed: 3162\n",
      "Testing: 0.6555524078170468 Training: 0.6427791826280657 Seed: 3164\n",
      "Testing: 0.6476759035021828 Training: 0.6448528091885706 Seed: 3165\n",
      "Testing: 0.6488058198203415 Training: 0.6444125837462505 Seed: 3166\n",
      "Testing: 0.6600783047759903 Training: 0.6416809932358692 Seed: 3169\n",
      "Testing: 0.6642736557267943 Training: 0.6406637109452046 Seed: 3173\n",
      "Testing: 0.6533503450700409 Training: 0.6433283176371478 Seed: 3175\n",
      "Testing: 0.6480011183564698 Training: 0.6447129197840269 Seed: 3177\n",
      "Testing: 0.6541313643227183 Training: 0.6431915488515233 Seed: 3178\n",
      "Testing: 0.6489186365185575 Training: 0.6444953087962443 Seed: 3179\n",
      "Testing: 0.6519609730824535 Training: 0.6437012577219188 Seed: 3182\n",
      "Testing: 0.6468565498091071 Training: 0.6450588821970847 Seed: 3184\n",
      "Testing: 0.6460936594798655 Training: 0.6452221053231164 Seed: 3187\n",
      "Testing: 0.65525961341847 Training: 0.6427165388372311 Seed: 3188\n",
      "Testing: 0.6478881873444169 Training: 0.6445359160070685 Seed: 3190\n",
      "Testing: 0.6542154981019385 Training: 0.6430640703779876 Seed: 3191\n",
      "Testing: 0.6737327510770773 Training: 0.6380722847110395 Seed: 3192\n",
      "Testing: 0.6455837889629429 Training: 0.6452066608681738 Seed: 3196\n",
      "Testing: 0.6472764894156628 Training: 0.6448500947798534 Seed: 3202\n",
      "Testing: 0.6545170512071368 Training: 0.6429467552380168 Seed: 3204\n",
      "Testing: 0.6530475140551711 Training: 0.6433257761484721 Seed: 3205\n",
      "Testing: 0.6529121099838562 Training: 0.6434711770374958 Seed: 3206\n",
      "Testing: 0.6535360633814147 Training: 0.6433975766985846 Seed: 3215\n",
      "Testing: 0.6499037125447629 Training: 0.6443087852493794 Seed: 3219\n",
      "Testing: 0.6454401963793185 Training: 0.6453698207825215 Seed: 3220\n",
      "Testing: 0.65634909217895 Training: 0.6425872693003616 Seed: 3224\n",
      "Testing: 0.6526530679070488 Training: 0.6432243801663076 Seed: 3231\n",
      "Testing: 0.6475385614706766 Training: 0.6447876535901931 Seed: 3232\n",
      "Testing: 0.6592240993870995 Training: 0.6419707531698695 Seed: 3237\n",
      "Testing: 0.6484445393620968 Training: 0.6445084823784188 Seed: 3238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6497565327789913 Training: 0.6442045401709198 Seed: 3242\n",
      "Testing: 0.6626106783119625 Training: 0.6411762487819395 Seed: 3243\n",
      "Testing: 0.6541128333348104 Training: 0.6433073684508103 Seed: 3245\n",
      "Testing: 0.6472737476666263 Training: 0.6448143267060247 Seed: 3246\n",
      "Testing: 0.6508757197943078 Training: 0.6436966409834461 Seed: 3249\n",
      "Testing: 0.6555404075899527 Training: 0.6428571884958569 Seed: 3251\n",
      "Testing: 0.6589065804559145 Training: 0.6419065359271083 Seed: 3252\n",
      "Testing: 0.6483987077191717 Training: 0.6444252807222115 Seed: 3258\n",
      "Testing: 0.6497218008213264 Training: 0.6442934348904477 Seed: 3260\n",
      "Testing: 0.6462109187306391 Training: 0.6450645657121994 Seed: 3261\n",
      "Testing: 0.6651931006677247 Training: 0.6402076615952521 Seed: 3264\n",
      "Testing: 0.6674308790067204 Training: 0.6396480979972592 Seed: 3265\n",
      "Testing: 0.6588604685424692 Training: 0.6420607879441041 Seed: 3267\n",
      "Testing: 0.6495416231656884 Training: 0.6443267285118743 Seed: 3269\n",
      "Testing: 0.6685789271145941 Training: 0.6393412374201416 Seed: 3270\n",
      "Testing: 0.6509870166946073 Training: 0.6440438980688837 Seed: 3275\n",
      "Testing: 0.6460263454998731 Training: 0.6452005257883174 Seed: 3276\n",
      "Testing: 0.6511171134583082 Training: 0.6440587854230774 Seed: 3278\n",
      "Testing: 0.6459549828372083 Training: 0.6452009546152245 Seed: 3283\n",
      "Testing: 0.6584914083614887 Training: 0.6419872077792341 Seed: 3287\n",
      "Testing: 0.6533114396981968 Training: 0.6433790649617062 Seed: 3289\n",
      "Testing: 0.6506483422779937 Training: 0.6439964923960471 Seed: 3291\n",
      "Testing: 0.6490155949995691 Training: 0.6443738545682688 Seed: 3293\n",
      "Testing: 0.6472761064936008 Training: 0.6448196355454033 Seed: 3294\n",
      "Testing: 0.6501207264972666 Training: 0.6441902779860216 Seed: 3300\n",
      "Testing: 0.6511677897810503 Training: 0.6440384836278604 Seed: 3301\n",
      "Testing: 0.6524668293538218 Training: 0.6436033932038522 Seed: 3304\n",
      "Testing: 0.6630256544196245 Training: 0.6407227240093942 Seed: 3306\n",
      "Testing: 0.6545655497373388 Training: 0.6429424890484247 Seed: 3308\n",
      "Testing: 0.6455537067973018 Training: 0.6453610968410195 Seed: 3310\n",
      "Testing: 0.6557233339678603 Training: 0.6427502893766212 Seed: 3311\n",
      "Testing: 0.6563122595194555 Training: 0.6425558723952554 Seed: 3312\n",
      "Testing: 0.6503435046639108 Training: 0.6440656239258138 Seed: 3313\n",
      "Testing: 0.6534906186432786 Training: 0.6433869863945827 Seed: 3314\n",
      "Testing: 0.6611540714417699 Training: 0.6414974332798603 Seed: 3315\n",
      "Testing: 0.6513221495731196 Training: 0.6438209035281418 Seed: 3316\n",
      "Testing: 0.6687317656253619 Training: 0.6393617367015919 Seed: 3320\n",
      "Testing: 0.6504289124523497 Training: 0.6441699607516967 Seed: 3325\n",
      "Testing: 0.651271337871057 Training: 0.6434685740747565 Seed: 3327\n",
      "Testing: 0.64557591155989 Training: 0.6452766861284507 Seed: 3329\n",
      "Testing: 0.6572446300999706 Training: 0.6423888980669591 Seed: 3331\n",
      "Testing: 0.6453941761967958 Training: 0.6453626048181229 Seed: 3335\n",
      "Testing: 0.6498318247141119 Training: 0.6442363451998618 Seed: 3339\n",
      "Testing: 0.6746822240623083 Training: 0.6378578031380665 Seed: 3341\n",
      "Testing: 0.6488792530308001 Training: 0.6444687290862287 Seed: 3342\n",
      "Testing: 0.6559421761517217 Training: 0.6427743674033352 Seed: 3345\n",
      "Testing: 0.6532016407526791 Training: 0.6432236371302158 Seed: 3346\n",
      "Testing: 0.6664239949758607 Training: 0.639556034050221 Seed: 3347\n",
      "Testing: 0.6567011567396412 Training: 0.6426204988961559 Seed: 3348\n",
      "Testing: 0.6557114641772135 Training: 0.6427433638062628 Seed: 3349\n",
      "Testing: 0.656761789225037 Training: 0.642449185040721 Seed: 3354\n",
      "Testing: 0.6575637313879135 Training: 0.6424085749562238 Seed: 3355\n",
      "Testing: 0.6622796189098226 Training: 0.6409401867901581 Seed: 3356\n",
      "Testing: 0.6550313820425279 Training: 0.6430251113452342 Seed: 3358\n",
      "Testing: 0.6572510074329582 Training: 0.6425093136416965 Seed: 3360\n",
      "Testing: 0.6512845031933681 Training: 0.6437206987680884 Seed: 3361\n",
      "Testing: 0.654110639815586 Training: 0.6432158984205586 Seed: 3362\n",
      "Testing: 0.6558989029339221 Training: 0.642709550589043 Seed: 3364\n",
      "Testing: 0.6655295277771824 Training: 0.640301093862294 Seed: 3365\n",
      "Testing: 0.6483329464046574 Training: 0.6441952449671065 Seed: 3366\n",
      "Testing: 0.6510422403347363 Training: 0.6440689946572029 Seed: 3369\n",
      "Testing: 0.6497746137959143 Training: 0.6442912514235175 Seed: 3370\n",
      "Testing: 0.6573083086086634 Training: 0.6422041650941428 Seed: 3371\n",
      "Testing: 0.6693955950137973 Training: 0.6390632377158478 Seed: 3373\n",
      "Testing: 0.6541291136637221 Training: 0.6432442729097723 Seed: 3374\n",
      "Testing: 0.6655621508099344 Training: 0.6402726551171583 Seed: 3378\n",
      "Testing: 0.6454386935591632 Training: 0.645366284424804 Seed: 3382\n",
      "Testing: 0.6523366029710169 Training: 0.6435723377025832 Seed: 3386\n",
      "Testing: 0.6476111426223821 Training: 0.6448827500168868 Seed: 3387\n",
      "Testing: 0.6494593166324514 Training: 0.6443543606999635 Seed: 3389\n",
      "Testing: 0.6469127667018996 Training: 0.6448619410638379 Seed: 3391\n",
      "Testing: 0.6558437755248838 Training: 0.6426990330895448 Seed: 3392\n",
      "Testing: 0.6487137980580158 Training: 0.6445183066822807 Seed: 3395\n",
      "Testing: 0.6559299127221028 Training: 0.642732066874866 Seed: 3397\n",
      "Testing: 0.6460569741258504 Training: 0.645141566557222 Seed: 3399\n",
      "Testing: 0.6466662206907049 Training: 0.6449482172063519 Seed: 3400\n",
      "Testing: 0.6456588356173855 Training: 0.6452727635981472 Seed: 3401\n",
      "Testing: 0.6529073520299487 Training: 0.643475898237171 Seed: 3405\n",
      "Testing: 0.65896312019946 Training: 0.6419791235551194 Seed: 3406\n",
      "Testing: 0.6462736341517654 Training: 0.6451485477595973 Seed: 3408\n",
      "Testing: 0.6577717447162637 Training: 0.6421682383605529 Seed: 3412\n",
      "Testing: 0.6466983579954375 Training: 0.6449200899777805 Seed: 3413\n",
      "Testing: 0.6621107737293778 Training: 0.6412006493822473 Seed: 3414\n",
      "Testing: 0.6580726047014278 Training: 0.6422978000459039 Seed: 3415\n",
      "Testing: 0.6481931884263258 Training: 0.6446483076873288 Seed: 3417\n",
      "Testing: 0.6593936447951398 Training: 0.6417086305193713 Seed: 3418\n",
      "Testing: 0.6482362347533571 Training: 0.6447004906037284 Seed: 3419\n",
      "Testing: 0.6596497072842972 Training: 0.6416129497425535 Seed: 3423\n",
      "Testing: 0.6510783706364304 Training: 0.6439619176991521 Seed: 3424\n",
      "Testing: 0.6575050724753565 Training: 0.6424215301962626 Seed: 3425\n",
      "Testing: 0.6487771605133286 Training: 0.6445497690945736 Seed: 3426\n",
      "Testing: 0.6567758938146634 Training: 0.6426092698065218 Seed: 3427\n",
      "Testing: 0.6498954959231452 Training: 0.6442528839876654 Seed: 3428\n",
      "Testing: 0.646932784343883 Training: 0.6450132431292499 Seed: 3429\n",
      "Testing: 0.6587762699161404 Training: 0.642106018312305 Seed: 3432\n",
      "Testing: 0.6490636048543974 Training: 0.6444690615664708 Seed: 3434\n",
      "Testing: 0.6506242276929706 Training: 0.6440509352792164 Seed: 3435\n",
      "Testing: 0.6569351728643642 Training: 0.6425636716887062 Seed: 3437\n",
      "Testing: 0.6563463415306282 Training: 0.6427013447167135 Seed: 3441\n",
      "Testing: 0.654064130310229 Training: 0.6432733790994299 Seed: 3445\n",
      "Testing: 0.6561599512458781 Training: 0.6425821002314916 Seed: 3446\n",
      "Testing: 0.6517171468286834 Training: 0.643736779050448 Seed: 3448\n",
      "Testing: 0.6543266530467409 Training: 0.6431427818816025 Seed: 3450\n",
      "Testing: 0.663066103217578 Training: 0.6409700791250726 Seed: 3454\n",
      "Testing: 0.6519935897931508 Training: 0.6436886122637004 Seed: 3455\n",
      "Testing: 0.6460978406449466 Training: 0.6451964446061689 Seed: 3456\n",
      "Testing: 0.6522124213177315 Training: 0.6433836447213532 Seed: 3457\n",
      "Testing: 0.6587995010476636 Training: 0.6418356396423472 Seed: 3459\n",
      "Testing: 0.6552921604597134 Training: 0.6429534142993177 Seed: 3462\n",
      "Testing: 0.6469788264430056 Training: 0.6449535693221995 Seed: 3465\n",
      "Testing: 0.6602712250382727 Training: 0.6416767590217887 Seed: 3466\n",
      "Testing: 0.6615902188307355 Training: 0.6412644850432545 Seed: 3467\n",
      "Testing: 0.6473434916154003 Training: 0.6448882785774335 Seed: 3469\n",
      "Testing: 0.6474184174188616 Training: 0.6448346530690694 Seed: 3470\n",
      "Testing: 0.6486106920767869 Training: 0.6445146666936938 Seed: 3471\n",
      "Testing: 0.6539137696379659 Training: 0.6430265958962961 Seed: 3472\n",
      "Testing: 0.650768953704431 Training: 0.6440464819199021 Seed: 3473\n",
      "Testing: 0.6474077769901605 Training: 0.6448455487346516 Seed: 3481\n",
      "Testing: 0.663152202842277 Training: 0.6407082314925252 Seed: 3483\n",
      "Testing: 0.6512163192080511 Training: 0.6437270404343405 Seed: 3485\n",
      "Testing: 0.6589829971160834 Training: 0.6420477067840474 Seed: 3486\n",
      "Testing: 0.6520507247653357 Training: 0.6437495400929045 Seed: 3487\n",
      "Testing: 0.6459893566279434 Training: 0.6447496404173977 Seed: 3488\n",
      "Testing: 0.6531131295803349 Training: 0.643366871672533 Seed: 3489\n",
      "Testing: 0.6662970536487289 Training: 0.6398164974172653 Seed: 3491\n",
      "Testing: 0.6490455080689876 Training: 0.6444428218857818 Seed: 3494\n",
      "Testing: 0.6475997884350856 Training: 0.6444453008478682 Seed: 3495\n",
      "Testing: 0.6465583302483635 Training: 0.645142305909395 Seed: 3500\n",
      "Testing: 0.6600147861909779 Training: 0.6416531690647168 Seed: 3502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6540712744195536 Training: 0.6432211783039709 Seed: 3503\n",
      "Testing: 0.6455291748736373 Training: 0.6446984516743239 Seed: 3508\n",
      "Testing: 0.6470029300299641 Training: 0.6449879049065159 Seed: 3509\n",
      "Testing: 0.6526395159970358 Training: 0.6435607124947804 Seed: 3511\n",
      "Testing: 0.6509145582383129 Training: 0.6440132792936923 Seed: 3512\n",
      "Testing: 0.6469942115380081 Training: 0.6450417805846865 Seed: 3515\n",
      "Testing: 0.6457966349744767 Training: 0.6453331308988447 Seed: 3516\n",
      "Testing: 0.66227345151908 Training: 0.6408344792546179 Seed: 3518\n",
      "Testing: 0.6634467419713499 Training: 0.6406654680177329 Seed: 3520\n",
      "Testing: 0.6606548064044246 Training: 0.6417230930815384 Seed: 3523\n",
      "Testing: 0.6551206976582027 Training: 0.6428381969811517 Seed: 3526\n",
      "Testing: 0.6461465057399273 Training: 0.6451405927630298 Seed: 3531\n",
      "Testing: 0.6469396414929592 Training: 0.6449693443687632 Seed: 3536\n",
      "Testing: 0.6617460824023643 Training: 0.6412254266498294 Seed: 3538\n",
      "Testing: 0.6617983946959406 Training: 0.6413862021593544 Seed: 3539\n",
      "Testing: 0.6512803875293831 Training: 0.6439226656916603 Seed: 3543\n",
      "Testing: 0.6522159235481151 Training: 0.6436387828751582 Seed: 3545\n",
      "Testing: 0.645599688515093 Training: 0.6453506587272789 Seed: 3546\n",
      "Testing: 0.6461967666913586 Training: 0.6451769807179838 Seed: 3547\n",
      "Testing: 0.6471016829760726 Training: 0.6449217636254989 Seed: 3548\n",
      "Testing: 0.6503414083437284 Training: 0.6440940975514899 Seed: 3551\n",
      "Testing: 0.6471380313422503 Training: 0.6449466269148656 Seed: 3553\n",
      "Testing: 0.6588791736863442 Training: 0.6419839535978703 Seed: 3554\n",
      "Testing: 0.6571915777454177 Training: 0.6423471204497431 Seed: 3555\n",
      "Testing: 0.6556111841533073 Training: 0.6426930937845903 Seed: 3559\n",
      "Testing: 0.6521217036119564 Training: 0.6437523651340058 Seed: 3560\n",
      "Testing: 0.6655259938276484 Training: 0.6403485168572725 Seed: 3562\n",
      "Testing: 0.657892528537565 Training: 0.6421319636490549 Seed: 3564\n",
      "Testing: 0.6530671032814193 Training: 0.6435380790293176 Seed: 3565\n",
      "Testing: 0.6498417823671508 Training: 0.6441957621257657 Seed: 3574\n",
      "Testing: 0.6604441328045038 Training: 0.6417233462877752 Seed: 3575\n",
      "Testing: 0.6501540658527076 Training: 0.6440572884163241 Seed: 3577\n",
      "Testing: 0.6460313236751252 Training: 0.6452290394387163 Seed: 3578\n",
      "Testing: 0.6481831304835722 Training: 0.6447304592428476 Seed: 3584\n",
      "Testing: 0.6586116074239776 Training: 0.6420843834310952 Seed: 3585\n",
      "Testing: 0.6548107812695497 Training: 0.6429817080312317 Seed: 3586\n",
      "Testing: 0.6506944930074263 Training: 0.6439068928622607 Seed: 3590\n",
      "Testing: 0.6697354876148038 Training: 0.6392393120571731 Seed: 3595\n",
      "Testing: 0.6522772348480226 Training: 0.6436271674443648 Seed: 3603\n",
      "Testing: 0.6577600931456237 Training: 0.6420611107240396 Seed: 3606\n",
      "Testing: 0.6597991547428377 Training: 0.6417982226536256 Seed: 3608\n",
      "Testing: 0.6589418443113285 Training: 0.6419920971411451 Seed: 3610\n",
      "Testing: 0.651034745151006 Training: 0.643832371961469 Seed: 3612\n",
      "Testing: 0.6638565996521675 Training: 0.6407966513927909 Seed: 3614\n",
      "Testing: 0.6588320722241476 Training: 0.6420136639534154 Seed: 3616\n",
      "Testing: 0.6531836872589201 Training: 0.6430867355736538 Seed: 3618\n",
      "Testing: 0.6499014637558301 Training: 0.6442615771741464 Seed: 3621\n",
      "Testing: 0.6703124211389397 Training: 0.6388363347534975 Seed: 3622\n",
      "Testing: 0.6467930715080784 Training: 0.644983272635439 Seed: 3623\n",
      "Testing: 0.6515861907949773 Training: 0.6436519599396109 Seed: 3625\n",
      "Testing: 0.6460271050165127 Training: 0.6451723630100874 Seed: 3626\n",
      "Testing: 0.6542645761349769 Training: 0.6429979246009468 Seed: 3627\n",
      "Testing: 0.6482727097563867 Training: 0.6443011865819832 Seed: 3628\n",
      "Testing: 0.651487198999501 Training: 0.6439033387878867 Seed: 3629\n",
      "Testing: 0.6569510837315001 Training: 0.6423496294987435 Seed: 3631\n",
      "Testing: 0.646427377240814 Training: 0.6451032992305326 Seed: 3633\n",
      "Testing: 0.653116258308203 Training: 0.6434379306774133 Seed: 3636\n",
      "Testing: 0.6645092303945981 Training: 0.6406395048214999 Seed: 3637\n",
      "Testing: 0.6513121249790282 Training: 0.6439147921606043 Seed: 3639\n",
      "Testing: 0.6529124544442104 Training: 0.6434844232188499 Seed: 3640\n",
      "Testing: 0.657622317873445 Training: 0.6424104902998671 Seed: 3642\n",
      "Testing: 0.6473656274770423 Training: 0.6449418739499798 Seed: 3645\n",
      "Testing: 0.6531149260713254 Training: 0.6433968704778038 Seed: 3646\n",
      "Testing: 0.6696172880988458 Training: 0.6392499419000662 Seed: 3647\n",
      "Testing: 0.6490170257524951 Training: 0.644466781564315 Seed: 3648\n",
      "Testing: 0.651620894071077 Training: 0.6437653493252358 Seed: 3649\n",
      "Testing: 0.6587967933390931 Training: 0.6419959855359467 Seed: 3650\n",
      "Testing: 0.6519546024075611 Training: 0.6435810671595421 Seed: 3655\n",
      "Testing: 0.6563748332379566 Training: 0.6425267225260315 Seed: 3656\n",
      "Testing: 0.6588901922211208 Training: 0.6420016833898681 Seed: 3657\n",
      "Testing: 0.647667865598937 Training: 0.6448274838600372 Seed: 3658\n",
      "Testing: 0.6577993892391771 Training: 0.6421944798499812 Seed: 3659\n",
      "Testing: 0.6453750132109811 Training: 0.6452592577176922 Seed: 3662\n",
      "Testing: 0.6551593627735195 Training: 0.642759842080643 Seed: 3663\n",
      "Testing: 0.6489636503968245 Training: 0.6443614386643742 Seed: 3664\n",
      "Testing: 0.6505062011336012 Training: 0.6440845424307113 Seed: 3670\n",
      "Testing: 0.663083214834384 Training: 0.6408539515042189 Seed: 3671\n",
      "Testing: 0.6500636075978792 Training: 0.6441638497070813 Seed: 3673\n",
      "Testing: 0.6455331933686915 Training: 0.6453577787328963 Seed: 3674\n",
      "Testing: 0.6479222570940154 Training: 0.6447311382174005 Seed: 3675\n",
      "Testing: 0.6634285825679989 Training: 0.6406240116755775 Seed: 3676\n",
      "Testing: 0.6603594430904988 Training: 0.6415806322853742 Seed: 3677\n",
      "Testing: 0.6558768092688658 Training: 0.642744173207171 Seed: 3678\n",
      "Testing: 0.6610735513808332 Training: 0.6413993946584267 Seed: 3682\n",
      "Testing: 0.6455319303615645 Training: 0.645296585557025 Seed: 3685\n",
      "Testing: 0.649037677447708 Training: 0.6444553343024754 Seed: 3688\n",
      "Testing: 0.6496690834157934 Training: 0.6443493992393504 Seed: 3692\n",
      "Testing: 0.6483045024850768 Training: 0.6447294807558458 Seed: 3693\n",
      "Testing: 0.6546846730678888 Training: 0.6430482728159492 Seed: 3694\n",
      "Testing: 0.6573128213154318 Training: 0.6425341627589741 Seed: 3695\n",
      "Testing: 0.6510284474665926 Training: 0.6438578087733361 Seed: 3696\n",
      "Testing: 0.6565705906860859 Training: 0.6425488616502157 Seed: 3697\n",
      "Testing: 0.6462197382426691 Training: 0.6451478399755726 Seed: 3700\n",
      "Testing: 0.6549754520906067 Training: 0.6429306374840771 Seed: 3701\n",
      "Testing: 0.6543577583956932 Training: 0.6430649930298373 Seed: 3705\n",
      "Testing: 0.6475387518132836 Training: 0.6447969499660469 Seed: 3708\n",
      "Testing: 0.6606236600118074 Training: 0.6414438361731466 Seed: 3710\n",
      "Testing: 0.6643817251610946 Training: 0.6405555349900602 Seed: 3718\n",
      "Testing: 0.6486916675948935 Training: 0.6445440319258727 Seed: 3720\n",
      "Testing: 0.6471630145584176 Training: 0.6449401005489142 Seed: 3722\n",
      "Testing: 0.6581609329641723 Training: 0.6422462040437307 Seed: 3725\n",
      "Testing: 0.6730089442718805 Training: 0.6386419362340033 Seed: 3728\n",
      "Testing: 0.6587468850213868 Training: 0.6421091218440078 Seed: 3729\n",
      "Testing: 0.6576457203050503 Training: 0.6423459978319068 Seed: 3732\n",
      "Testing: 0.6621359800127098 Training: 0.6411953232454253 Seed: 3734\n",
      "Testing: 0.6514413344422235 Training: 0.6439167343810589 Seed: 3736\n",
      "Testing: 0.6476706471100799 Training: 0.6448448306996919 Seed: 3737\n",
      "Testing: 0.6611233123937593 Training: 0.641417381469056 Seed: 3739\n",
      "Testing: 0.6702395248241861 Training: 0.6391413031259566 Seed: 3742\n",
      "Testing: 0.6576299390297787 Training: 0.6422601065987943 Seed: 3748\n",
      "Testing: 0.6525846198731513 Training: 0.6434441643103739 Seed: 3749\n",
      "Testing: 0.6552636710307528 Training: 0.6429361244187298 Seed: 3753\n",
      "Testing: 0.6502309894022977 Training: 0.6441827394201377 Seed: 3755\n",
      "Testing: 0.6488287590957043 Training: 0.6445304728031211 Seed: 3756\n",
      "Testing: 0.6454617178884264 Training: 0.6454112006447317 Seed: 3758\n",
      "Testing: 0.6596581053596411 Training: 0.6417485862067371 Seed: 3759\n",
      "Testing: 0.6544112091535309 Training: 0.6431475319414512 Seed: 3768\n",
      "Testing: 0.6593460339115269 Training: 0.6416002734287181 Seed: 3773\n",
      "Testing: 0.6493106024517361 Training: 0.6443889852330308 Seed: 3774\n",
      "Testing: 0.6522442142766328 Training: 0.6435880183033129 Seed: 3776\n",
      "Testing: 0.6451287805472039 Training: 0.6450329147159771 Seed: 3778\n",
      "Testing: 0.6556194434620306 Training: 0.6425540286260238 Seed: 3780\n",
      "Testing: 0.6637210582365162 Training: 0.6407953401193467 Seed: 3781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6517031020087303 Training: 0.643851729684358 Seed: 3784\n",
      "Testing: 0.6479427864696331 Training: 0.6447121839284179 Seed: 3791\n",
      "Testing: 0.6479620636056455 Training: 0.6446131851133774 Seed: 3794\n",
      "Testing: 0.6508477535629065 Training: 0.644002002236366 Seed: 3797\n",
      "Testing: 0.6593209875963401 Training: 0.6420185205010199 Seed: 3798\n",
      "Testing: 0.6697246181503265 Training: 0.639066820243045 Seed: 3802\n",
      "Testing: 0.6591417612374242 Training: 0.6419729296621096 Seed: 3803\n",
      "Testing: 0.6480861482463762 Training: 0.6446437861925939 Seed: 3805\n",
      "Testing: 0.6546546014632499 Training: 0.6429494473023472 Seed: 3806\n",
      "Testing: 0.6486711689261605 Training: 0.6445174569866469 Seed: 3809\n",
      "Testing: 0.657491309121819 Training: 0.6423608384570619 Seed: 3811\n",
      "Testing: 0.6521603600344552 Training: 0.6436338616895754 Seed: 3813\n",
      "Testing: 0.6509044892976422 Training: 0.643934233728539 Seed: 3814\n",
      "Testing: 0.6556011573648113 Training: 0.642818171539628 Seed: 3818\n",
      "Testing: 0.6615290165521538 Training: 0.641455896685959 Seed: 3819\n",
      "Testing: 0.64861793949786 Training: 0.6445099149884629 Seed: 3820\n",
      "Testing: 0.6465412655840129 Training: 0.6450649117887838 Seed: 3821\n",
      "Testing: 0.659464046535052 Training: 0.6418699809516955 Seed: 3822\n",
      "Testing: 0.6586773501323376 Training: 0.6420528858653617 Seed: 3825\n",
      "Testing: 0.6550657583951702 Training: 0.6428031170830079 Seed: 3826\n",
      "Testing: 0.6456577452594272 Training: 0.6453178630439239 Seed: 3827\n",
      "Testing: 0.6508898473959914 Training: 0.6438960145138153 Seed: 3829\n",
      "Testing: 0.6483572829318106 Training: 0.6446680598044068 Seed: 3830\n",
      "Testing: 0.6635682284632 Training: 0.6407645161262471 Seed: 3832\n",
      "Testing: 0.6641643101173831 Training: 0.6406867888093357 Seed: 3834\n",
      "Testing: 0.6733915147388813 Training: 0.6383631637942844 Seed: 3836\n",
      "Testing: 0.6567677443653934 Training: 0.6424629648845049 Seed: 3838\n",
      "Testing: 0.6551080316988519 Training: 0.6428356434399006 Seed: 3839\n",
      "Testing: 0.6583051829835803 Training: 0.6419579990576981 Seed: 3841\n",
      "Testing: 0.6584462171931968 Training: 0.6421482461909347 Seed: 3844\n",
      "Testing: 0.6558561881533521 Training: 0.6426710069195065 Seed: 3845\n",
      "Testing: 0.6597072602553814 Training: 0.6418687418738401 Seed: 3846\n",
      "Testing: 0.6520612040935889 Training: 0.6437300201579372 Seed: 3849\n",
      "Testing: 0.6459344743711437 Training: 0.6452003504142023 Seed: 3850\n",
      "Testing: 0.6487551253456939 Training: 0.6445605342406509 Seed: 3851\n",
      "Testing: 0.6555385809079757 Training: 0.6427007918886494 Seed: 3854\n",
      "Testing: 0.6494892726636158 Training: 0.6442095725896791 Seed: 3855\n",
      "Testing: 0.6487790070550835 Training: 0.6444476049250647 Seed: 3857\n",
      "Testing: 0.6531269264969829 Training: 0.6434183239072777 Seed: 3858\n",
      "Testing: 0.6663510966888192 Training: 0.6400722054690631 Seed: 3859\n",
      "Testing: 0.6459915831881777 Training: 0.6451979314075764 Seed: 3864\n",
      "Testing: 0.648269131092459 Training: 0.6446745381484529 Seed: 3867\n",
      "Testing: 0.6630801900410326 Training: 0.640958434514419 Seed: 3870\n",
      "Testing: 0.6553158566501809 Training: 0.6429624480933543 Seed: 3871\n",
      "Testing: 0.6461991823587803 Training: 0.6450874648496641 Seed: 3872\n",
      "Testing: 0.6536537708906068 Training: 0.6433323239428982 Seed: 3873\n",
      "Testing: 0.6573710708988734 Training: 0.6423850097720731 Seed: 3874\n",
      "Testing: 0.6587961581960592 Training: 0.6418508388030439 Seed: 3875\n",
      "Testing: 0.6577019450321021 Training: 0.6421774321716464 Seed: 3881\n",
      "Testing: 0.6463840559610184 Training: 0.6449777168305844 Seed: 3884\n",
      "Testing: 0.6493420028564811 Training: 0.6443157269293827 Seed: 3886\n",
      "Testing: 0.653419858201529 Training: 0.6432078567202403 Seed: 3889\n",
      "Testing: 0.6558555060172041 Training: 0.6427105277743499 Seed: 3898\n",
      "Testing: 0.6606945673472039 Training: 0.6413300883688529 Seed: 3899\n",
      "Testing: 0.6541016075795575 Training: 0.6430895493322553 Seed: 3900\n",
      "Testing: 0.6499211447731518 Training: 0.6440778283355482 Seed: 3903\n",
      "Testing: 0.6516791978501022 Training: 0.6436871025421776 Seed: 3905\n",
      "Testing: 0.645516469801003 Training: 0.6453754924531476 Seed: 3907\n",
      "Testing: 0.6469440821457257 Training: 0.6450174800802118 Seed: 3908\n",
      "Testing: 0.647593107324669 Training: 0.6448501934929992 Seed: 3911\n",
      "Testing: 0.6499672243363668 Training: 0.6437630072230461 Seed: 3912\n",
      "Testing: 0.650504005465693 Training: 0.6441346093173196 Seed: 3913\n",
      "Testing: 0.6571781341864404 Training: 0.6424348134803572 Seed: 3914\n",
      "Testing: 0.6517598428664558 Training: 0.6438296719208323 Seed: 3915\n",
      "Testing: 0.6526607149877873 Training: 0.64353952174482 Seed: 3916\n",
      "Testing: 0.6578447074746486 Training: 0.6422961076557191 Seed: 3918\n",
      "Testing: 0.6492378961504616 Training: 0.6443459857315115 Seed: 3919\n",
      "Testing: 0.6527025672564927 Training: 0.643639736588274 Seed: 3920\n",
      "Testing: 0.6592340116003397 Training: 0.641856851607856 Seed: 3922\n",
      "Testing: 0.6527553603218266 Training: 0.6434910875414518 Seed: 3923\n",
      "Testing: 0.651475856880481 Training: 0.6438039875654089 Seed: 3926\n",
      "Testing: 0.6536674180560299 Training: 0.6433814376272813 Seed: 3927\n",
      "Testing: 0.6506554482806324 Training: 0.6439763467867764 Seed: 3930\n",
      "Testing: 0.6598626749893172 Training: 0.6417274283764717 Seed: 3931\n",
      "Testing: 0.6622809887571328 Training: 0.641020764595341 Seed: 3932\n",
      "Testing: 0.6621748512649059 Training: 0.6409442909257989 Seed: 3934\n",
      "Testing: 0.6727000785024303 Training: 0.6385407914388845 Seed: 3940\n",
      "Testing: 0.6472315924914429 Training: 0.6449351054745444 Seed: 3947\n",
      "Testing: 0.6604678136386467 Training: 0.641531875022594 Seed: 3948\n",
      "Testing: 0.6523150010850784 Training: 0.6436965803260843 Seed: 3949\n",
      "Testing: 0.6690013247350627 Training: 0.6389661614212248 Seed: 3950\n",
      "Testing: 0.6453860206748241 Training: 0.6453241576734993 Seed: 3951\n",
      "Testing: 0.651870229621774 Training: 0.643757946767536 Seed: 3954\n",
      "Testing: 0.6491918718845445 Training: 0.6444290590427676 Seed: 3955\n",
      "Testing: 0.6483364417021288 Training: 0.6445496498699805 Seed: 3956\n",
      "Testing: 0.6475374915230634 Training: 0.6448098982537154 Seed: 3957\n",
      "Testing: 0.6476991008955607 Training: 0.6448139688151737 Seed: 3958\n",
      "Testing: 0.6576422884477576 Training: 0.642146181476855 Seed: 3959\n",
      "Testing: 0.6625900657399937 Training: 0.6410529281184678 Seed: 3961\n",
      "Testing: 0.6520966146066438 Training: 0.6436027674209177 Seed: 3962\n",
      "Testing: 0.6538404328804835 Training: 0.643229808465049 Seed: 3963\n",
      "Testing: 0.6670698620280638 Training: 0.6398853497442774 Seed: 3964\n",
      "Testing: 0.6584451543900922 Training: 0.6420531544206307 Seed: 3966\n",
      "Testing: 0.6571957533243658 Training: 0.6422401578267174 Seed: 3967\n",
      "Testing: 0.6556830906654523 Training: 0.6427957684709263 Seed: 3969\n",
      "Testing: 0.653448132233288 Training: 0.6433505764485955 Seed: 3972\n",
      "Testing: 0.6518645147445382 Training: 0.643770277205064 Seed: 3973\n",
      "Testing: 0.6574276045077294 Training: 0.6424246309586978 Seed: 3974\n",
      "Testing: 0.6565407794109261 Training: 0.6425636030700663 Seed: 3975\n",
      "Testing: 0.6686836188280153 Training: 0.6393948572853002 Seed: 3976\n",
      "Testing: 0.6522121565942613 Training: 0.6435883213492206 Seed: 3980\n",
      "Testing: 0.6532516864418002 Training: 0.6431876405057562 Seed: 3981\n",
      "Testing: 0.6556416482507884 Training: 0.6427896126763606 Seed: 3982\n",
      "Testing: 0.6524791433187042 Training: 0.6436146757062349 Seed: 3985\n",
      "Testing: 0.6484808127639253 Training: 0.6439441719766762 Seed: 3987\n",
      "Testing: 0.6556874740411185 Training: 0.6427430770803739 Seed: 3988\n",
      "Testing: 0.6502881982178592 Training: 0.6441388888643507 Seed: 3990\n",
      "Testing: 0.6477216381588918 Training: 0.644831527840988 Seed: 3992\n",
      "Testing: 0.6498675030218846 Training: 0.6441839986122362 Seed: 3993\n",
      "Testing: 0.6477768380283937 Training: 0.6447996211929363 Seed: 3995\n",
      "Testing: 0.6550240492574608 Training: 0.6430886889937297 Seed: 3996\n",
      "Testing: 0.671532861195378 Training: 0.6389775078068728 Seed: 3998\n",
      "Testing: 0.6529573389674102 Training: 0.6434542726141687 Seed: 3999\n",
      "Testing: 0.6680169829384441 Training: 0.6396653604127778 Seed: 4000\n",
      "Testing: 0.661158996048154 Training: 0.6413627629249512 Seed: 4001\n",
      "Testing: 0.6659585750842417 Training: 0.6402274532138905 Seed: 4002\n",
      "Testing: 0.6506147518325018 Training: 0.6440445341049361 Seed: 4005\n",
      "Testing: 0.6501170069427373 Training: 0.6441169059894554 Seed: 4006\n",
      "Testing: 0.6498343814251523 Training: 0.6441439046228064 Seed: 4007\n",
      "Testing: 0.6543623451709898 Training: 0.643052747443795 Seed: 4009\n",
      "Testing: 0.65636835397428 Training: 0.6426330490748273 Seed: 4010\n",
      "Testing: 0.6609001302209013 Training: 0.6415554287290146 Seed: 4011\n",
      "Testing: 0.6539824699331354 Training: 0.6432333147371241 Seed: 4012\n",
      "Testing: 0.6455501657802614 Training: 0.6450847278184555 Seed: 4013\n",
      "Testing: 0.646260470612407 Training: 0.6449612352241011 Seed: 4016\n",
      "Testing: 0.6514524982706649 Training: 0.6439052721272046 Seed: 4017\n",
      "Testing: 0.6464096251839535 Training: 0.645165253010419 Seed: 4021\n",
      "Testing: 0.6463775116750581 Training: 0.6449817937989519 Seed: 4022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6610366331594969 Training: 0.6414876254173167 Seed: 4023\n",
      "Testing: 0.6608510127053429 Training: 0.6416599573618305 Seed: 4024\n",
      "Testing: 0.6539575840000629 Training: 0.6431526405143029 Seed: 4026\n",
      "Testing: 0.6467582531301854 Training: 0.6450598405318086 Seed: 4028\n",
      "Testing: 0.6633130918487656 Training: 0.640670262714407 Seed: 4032\n",
      "Testing: 0.6528823054897839 Training: 0.6435391281487644 Seed: 4033\n",
      "Testing: 0.6495214521710674 Training: 0.6437481264885353 Seed: 4039\n",
      "Testing: 0.6522485526528798 Training: 0.643619209309592 Seed: 4042\n",
      "Testing: 0.6559988235253854 Training: 0.642628305613538 Seed: 4046\n",
      "Testing: 0.6468069455824808 Training: 0.6450830644046595 Seed: 4050\n",
      "Testing: 0.6468981672207152 Training: 0.6448082032336079 Seed: 4054\n",
      "Testing: 0.6527549570489387 Training: 0.6432580711821765 Seed: 4055\n",
      "Testing: 0.6514506102654127 Training: 0.6439007822223484 Seed: 4056\n",
      "Testing: 0.6494688185190263 Training: 0.644331285815949 Seed: 4059\n",
      "Testing: 0.6481982547204481 Training: 0.6445098116401714 Seed: 4060\n",
      "Testing: 0.6602069444677433 Training: 0.641633865557933 Seed: 4062\n",
      "Testing: 0.6606070554183119 Training: 0.6413454237380399 Seed: 4063\n",
      "Testing: 0.6571380979352502 Training: 0.6424970384008579 Seed: 4064\n",
      "Testing: 0.6542174188410824 Training: 0.6431876584889238 Seed: 4067\n",
      "Testing: 0.6477136578947837 Training: 0.644829869210987 Seed: 4068\n",
      "Testing: 0.6473531208970308 Training: 0.6447853366956859 Seed: 4072\n",
      "Testing: 0.6526695709713499 Training: 0.6436284132884466 Seed: 4076\n",
      "Testing: 0.6553881321035069 Training: 0.6428861432319544 Seed: 4078\n",
      "Testing: 0.6486916339222124 Training: 0.6440915802184624 Seed: 4079\n",
      "Testing: 0.6510338805601719 Training: 0.6438876044719174 Seed: 4080\n",
      "Testing: 0.6484198056266003 Training: 0.644577462314166 Seed: 4082\n",
      "Testing: 0.6537531284939597 Training: 0.6433261228477434 Seed: 4083\n",
      "Testing: 0.6544634484927768 Training: 0.6430742234025062 Seed: 4086\n",
      "Testing: 0.6504927596455683 Training: 0.64414707896939 Seed: 4087\n",
      "Testing: 0.6563640350657798 Training: 0.6425600975753561 Seed: 4090\n",
      "Testing: 0.6503496201412909 Training: 0.6440870235153702 Seed: 4095\n",
      "Testing: 0.657159563235651 Training: 0.6424434114784776 Seed: 4096\n",
      "Testing: 0.6575458364463187 Training: 0.6425174583798088 Seed: 4097\n",
      "Testing: 0.6509933383381452 Training: 0.6440399227240236 Seed: 4100\n",
      "Testing: 0.6609635192150383 Training: 0.6414937474085065 Seed: 4101\n",
      "Testing: 0.667900385444669 Training: 0.6395386302502737 Seed: 4103\n",
      "Testing: 0.6476506532115993 Training: 0.6448169851807485 Seed: 4108\n",
      "Testing: 0.6571710945514381 Training: 0.6423078729577381 Seed: 4113\n",
      "Testing: 0.6538402040868764 Training: 0.6429648754066946 Seed: 4115\n",
      "Testing: 0.6626535964225451 Training: 0.6410257575780028 Seed: 4117\n",
      "Testing: 0.6518224344473236 Training: 0.643868977336354 Seed: 4118\n",
      "Testing: 0.6505648367103589 Training: 0.6439090819329738 Seed: 4125\n",
      "Testing: 0.6563808300539081 Training: 0.6427011327914757 Seed: 4126\n",
      "Testing: 0.6642097834638897 Training: 0.6404596161900831 Seed: 4127\n",
      "Testing: 0.6497854704741947 Training: 0.6442526936607391 Seed: 4128\n",
      "Testing: 0.6494021269304088 Training: 0.6442787124003934 Seed: 4129\n",
      "Testing: 0.6498492553848574 Training: 0.6442881956129092 Seed: 4131\n",
      "Testing: 0.6554924827755939 Training: 0.642751433901411 Seed: 4133\n",
      "Testing: 0.6561720466934229 Training: 0.6425963421930247 Seed: 4134\n",
      "Testing: 0.6486417305071077 Training: 0.644485412467611 Seed: 4136\n",
      "Testing: 0.6586323439168932 Training: 0.6420127401734825 Seed: 4138\n",
      "Testing: 0.660739872366402 Training: 0.6414915262540305 Seed: 4139\n",
      "Testing: 0.6536305016788146 Training: 0.6432065532453781 Seed: 4143\n",
      "Testing: 0.6614218842250592 Training: 0.6410804455425796 Seed: 4144\n",
      "Testing: 0.6518312438967372 Training: 0.6436178036631824 Seed: 4146\n",
      "Testing: 0.6528937305849617 Training: 0.6434550114528146 Seed: 4149\n",
      "Testing: 0.6491288867302384 Training: 0.6443925616343978 Seed: 4150\n",
      "Testing: 0.6530215015518096 Training: 0.643557206409959 Seed: 4151\n",
      "Testing: 0.6511629263115071 Training: 0.6438387665244921 Seed: 4153\n",
      "Testing: 0.648366027467979 Training: 0.6442006962379992 Seed: 4154\n",
      "Testing: 0.6572609134278289 Training: 0.6423939411506395 Seed: 4157\n",
      "Testing: 0.6523263870309507 Training: 0.6433989081520946 Seed: 4158\n",
      "Testing: 0.6478749421029598 Training: 0.6447772017446857 Seed: 4159\n",
      "Testing: 0.6571395395450073 Training: 0.642175457942211 Seed: 4161\n",
      "Testing: 0.6653557939993798 Training: 0.6402180125022887 Seed: 4163\n",
      "Testing: 0.6530544709681628 Training: 0.6435403109626299 Seed: 4164\n",
      "Testing: 0.6532594332223818 Training: 0.6433537082597726 Seed: 4167\n",
      "Testing: 0.661087785286427 Training: 0.641254564022836 Seed: 4169\n",
      "Testing: 0.6548488856150023 Training: 0.6430314332166771 Seed: 4170\n",
      "Testing: 0.6472348177992058 Training: 0.6448504218510972 Seed: 4172\n",
      "Testing: 0.6478207249976686 Training: 0.6446670911955844 Seed: 4174\n",
      "Testing: 0.6580905369901068 Training: 0.6420594800951176 Seed: 4175\n",
      "Testing: 0.652596951219998 Training: 0.6436117782905549 Seed: 4178\n",
      "Testing: 0.6499275838588121 Training: 0.6441475412505734 Seed: 4181\n",
      "Testing: 0.6498167347035512 Training: 0.6442456749332726 Seed: 4185\n",
      "Testing: 0.6485829232905542 Training: 0.6444889103889802 Seed: 4187\n",
      "Testing: 0.6493123813220709 Training: 0.6443388091876083 Seed: 4189\n",
      "Testing: 0.6496760868891288 Training: 0.6442124067542113 Seed: 4192\n",
      "Testing: 0.6492344240698592 Training: 0.6440406635768756 Seed: 4195\n",
      "Testing: 0.6715016213156676 Training: 0.6390791185075004 Seed: 4197\n",
      "Testing: 0.6586095901317195 Training: 0.6419668113888283 Seed: 4198\n",
      "Testing: 0.6480839510139189 Training: 0.6446741212453949 Seed: 4199\n",
      "Testing: 0.6658049997566065 Training: 0.6402503819665698 Seed: 4201\n",
      "Testing: 0.6528280354920484 Training: 0.6433908502383552 Seed: 4202\n",
      "Testing: 0.6486289154766891 Training: 0.6446472656384203 Seed: 4203\n",
      "Testing: 0.6586758864983593 Training: 0.6420160998926147 Seed: 4204\n",
      "Testing: 0.6575320249546878 Training: 0.6423602647828281 Seed: 4207\n",
      "Testing: 0.6471497526786592 Training: 0.644957487588596 Seed: 4208\n",
      "Testing: 0.6631811964767765 Training: 0.6408634841944494 Seed: 4210\n",
      "Testing: 0.6545366594002592 Training: 0.643002680532519 Seed: 4211\n",
      "Testing: 0.6535645794342244 Training: 0.6433360700535025 Seed: 4212\n",
      "Testing: 0.6502037085545112 Training: 0.6439872712383665 Seed: 4213\n",
      "Testing: 0.6600712407715381 Training: 0.6415680182791282 Seed: 4214\n",
      "Testing: 0.6642318156679435 Training: 0.6404653802594623 Seed: 4215\n",
      "Testing: 0.6482594233695285 Training: 0.6446951141270059 Seed: 4216\n",
      "Testing: 0.6484938170698084 Training: 0.644484899798072 Seed: 4219\n",
      "Testing: 0.6485584588444055 Training: 0.6445449925788964 Seed: 4222\n",
      "Testing: 0.6507509468944273 Training: 0.6440396354074377 Seed: 4223\n",
      "Testing: 0.6535716725769893 Training: 0.643377735896566 Seed: 4227\n",
      "Testing: 0.6535771494344331 Training: 0.6433439578218504 Seed: 4228\n",
      "Testing: 0.6509838007880473 Training: 0.6440109059936151 Seed: 4229\n",
      "Testing: 0.6495718543157739 Training: 0.6442782995664982 Seed: 4231\n",
      "Testing: 0.6533683808863123 Training: 0.6434225048714175 Seed: 4233\n",
      "Testing: 0.6669742883431478 Training: 0.6400874760459636 Seed: 4234\n",
      "Testing: 0.6586490647488046 Training: 0.6418708544769572 Seed: 4237\n",
      "Testing: 0.6681684568737163 Training: 0.6394916485203741 Seed: 4238\n",
      "Testing: 0.6697393290541253 Training: 0.6393485798454213 Seed: 4239\n",
      "Testing: 0.6524331587945401 Training: 0.6436225419997486 Seed: 4242\n",
      "Testing: 0.6557292930034744 Training: 0.6427465267393802 Seed: 4244\n",
      "Testing: 0.6680799645005723 Training: 0.6395717937716998 Seed: 4251\n",
      "Testing: 0.6466972237948729 Training: 0.6443807386592104 Seed: 4258\n",
      "Testing: 0.6537978929625987 Training: 0.6432172189211858 Seed: 4259\n",
      "Testing: 0.6491168245905754 Training: 0.6442208469068481 Seed: 4262\n",
      "Testing: 0.6624713659520726 Training: 0.6409145319048724 Seed: 4263\n",
      "Testing: 0.6473239480938334 Training: 0.6448424521175793 Seed: 4264\n",
      "Testing: 0.6577443438217752 Training: 0.642309836996 Seed: 4266\n",
      "Testing: 0.6553612330471146 Training: 0.6425343921334219 Seed: 4267\n",
      "Testing: 0.6502004552300931 Training: 0.6442100866740277 Seed: 4268\n",
      "Testing: 0.6540797260527415 Training: 0.6432569145179705 Seed: 4270\n",
      "Testing: 0.6509813864943026 Training: 0.6440233862312603 Seed: 4271\n",
      "Testing: 0.6651170790727594 Training: 0.6402979637670134 Seed: 4274\n",
      "Testing: 0.6547157184797621 Training: 0.6430476581157085 Seed: 4278\n",
      "Testing: 0.6490780107403051 Training: 0.6442952993998775 Seed: 4280\n",
      "Testing: 0.647737601725291 Training: 0.644725549156379 Seed: 4281\n",
      "Testing: 0.6476963454356641 Training: 0.6447981448058113 Seed: 4282\n",
      "Testing: 0.6523954406426651 Training: 0.6433799245059979 Seed: 4286\n",
      "Testing: 0.6616711644715991 Training: 0.6410772949443451 Seed: 4290\n",
      "Testing: 0.661444721263321 Training: 0.6413228008808523 Seed: 4292\n",
      "Testing: 0.652229476023221 Training: 0.6437997509318816 Seed: 4293\n",
      "Testing: 0.6630901076344385 Training: 0.6409295639049288 Seed: 4295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6576383285242633 Training: 0.6422567443495757 Seed: 4296\n",
      "Testing: 0.6496484505770302 Training: 0.6438808824592968 Seed: 4298\n",
      "Testing: 0.6490532957586984 Training: 0.6443449378976359 Seed: 4299\n",
      "Testing: 0.646018553814463 Training: 0.6451414801983265 Seed: 4300\n",
      "Testing: 0.6525398994548947 Training: 0.6436573906015887 Seed: 4303\n",
      "Testing: 0.6615292700234192 Training: 0.6414996672305813 Seed: 4305\n",
      "Testing: 0.6503114204382058 Training: 0.6442333111072046 Seed: 4309\n",
      "Testing: 0.6532018980725727 Training: 0.6434026415279834 Seed: 4313\n",
      "Testing: 0.6466833203015978 Training: 0.6450349829233494 Seed: 4314\n",
      "Testing: 0.6608453025278644 Training: 0.6414220386146307 Seed: 4316\n",
      "Testing: 0.6539321023937276 Training: 0.6428762043893496 Seed: 4320\n",
      "Testing: 0.6468982648596906 Training: 0.6450296578560593 Seed: 4321\n",
      "Testing: 0.6623494384945794 Training: 0.641191799376464 Seed: 4328\n",
      "Testing: 0.6534404613915286 Training: 0.6433404888924469 Seed: 4331\n",
      "Testing: 0.647850225070691 Training: 0.6447494721377729 Seed: 4336\n",
      "Testing: 0.6478890697969357 Training: 0.6447553140124409 Seed: 4338\n",
      "Testing: 0.6468702597306636 Training: 0.6449615663222449 Seed: 4340\n",
      "Testing: 0.6544152771808048 Training: 0.6430915501612358 Seed: 4341\n",
      "Testing: 0.646974187183403 Training: 0.6449682530238565 Seed: 4344\n",
      "Testing: 0.6469653035877996 Training: 0.6447793089305591 Seed: 4345\n",
      "Testing: 0.6478404669650004 Training: 0.6448018000889806 Seed: 4346\n",
      "Testing: 0.6494777222606299 Training: 0.6443672354536376 Seed: 4347\n",
      "Testing: 0.6493679652527916 Training: 0.6443703715161779 Seed: 4351\n",
      "Testing: 0.6600006625230797 Training: 0.641642521375643 Seed: 4352\n",
      "Testing: 0.6498740120771785 Training: 0.6442184478680286 Seed: 4355\n",
      "Testing: 0.6465522818692323 Training: 0.6450522010179343 Seed: 4356\n",
      "Testing: 0.6666273774365415 Training: 0.6401252016882597 Seed: 4357\n",
      "Testing: 0.6537147956648153 Training: 0.6433398136405171 Seed: 4358\n",
      "Testing: 0.6558303166855708 Training: 0.6426708107485956 Seed: 4362\n",
      "Testing: 0.6487336317538421 Training: 0.6443896793477379 Seed: 4363\n",
      "Testing: 0.6454625028547889 Training: 0.6453009058317276 Seed: 4365\n",
      "Testing: 0.6514694000054224 Training: 0.643873860648448 Seed: 4366\n",
      "Testing: 0.6531080257682036 Training: 0.6434861403316989 Seed: 4367\n",
      "Testing: 0.6521991955523817 Training: 0.6436417996215225 Seed: 4368\n",
      "Testing: 0.6561347519430463 Training: 0.6427315494324017 Seed: 4370\n",
      "Testing: 0.6596760501246212 Training: 0.6417522743137304 Seed: 4373\n",
      "Testing: 0.6479391091721707 Training: 0.644659438528632 Seed: 4376\n",
      "Testing: 0.6516089307341093 Training: 0.6438451156173562 Seed: 4378\n",
      "Testing: 0.6474568176309263 Training: 0.6448641077077097 Seed: 4382\n",
      "Testing: 0.6465075779791684 Training: 0.6449898453686214 Seed: 4387\n",
      "Testing: 0.6580292338436458 Training: 0.6423930825647453 Seed: 4388\n",
      "Testing: 0.6468191777597988 Training: 0.6448836326964924 Seed: 4389\n",
      "Testing: 0.6484472035059409 Training: 0.6445872082545301 Seed: 4390\n",
      "Testing: 0.6455258496893754 Training: 0.6453224602941781 Seed: 4391\n",
      "Testing: 0.6472943400190209 Training: 0.6449023654416071 Seed: 4395\n",
      "Testing: 0.6513013737533867 Training: 0.6437603417915752 Seed: 4397\n",
      "Testing: 0.6624585058464008 Training: 0.6411823573308286 Seed: 4398\n",
      "Testing: 0.6479548587272288 Training: 0.6446508223180775 Seed: 4399\n",
      "Testing: 0.6691024100273317 Training: 0.6394117855803991 Seed: 4400\n",
      "Testing: 0.648697204108926 Training: 0.6441050078707284 Seed: 4404\n",
      "Testing: 0.6508555843247856 Training: 0.644056542397725 Seed: 4409\n",
      "Testing: 0.6566069761730572 Training: 0.6426129518764905 Seed: 4410\n",
      "Testing: 0.6514487351741293 Training: 0.6439469130999704 Seed: 4412\n",
      "Testing: 0.6639292340050591 Training: 0.6407677133747888 Seed: 4415\n",
      "Testing: 0.668332502765901 Training: 0.6395221435937497 Seed: 4417\n",
      "Testing: 0.6498602003338552 Training: 0.6442323300361015 Seed: 4418\n",
      "Testing: 0.6512707977888976 Training: 0.643875464620775 Seed: 4419\n",
      "Testing: 0.6605684070594265 Training: 0.6414982128183456 Seed: 4420\n",
      "Testing: 0.6514324274070086 Training: 0.6439348244654381 Seed: 4421\n",
      "Testing: 0.6629578073822673 Training: 0.6408857216174075 Seed: 4423\n",
      "Testing: 0.6478769350296211 Training: 0.6448164025653057 Seed: 4425\n",
      "Testing: 0.6477764970068952 Training: 0.6445796731879003 Seed: 4426\n",
      "Testing: 0.6667512856793653 Training: 0.6402418146129342 Seed: 4427\n",
      "Testing: 0.6501884469009622 Training: 0.6441846581951132 Seed: 4429\n",
      "Testing: 0.653856567585161 Training: 0.6432531286992398 Seed: 4431\n",
      "Testing: 0.6473178460770873 Training: 0.6448227370914101 Seed: 4433\n",
      "Testing: 0.6694777840830011 Training: 0.6390954530953556 Seed: 4435\n",
      "Testing: 0.6503363493299849 Training: 0.6441385018662125 Seed: 4436\n",
      "Testing: 0.6677979194376319 Training: 0.6398808566763919 Seed: 4437\n",
      "Testing: 0.6496066898810539 Training: 0.6443649676102575 Seed: 4441\n",
      "Testing: 0.658856952050576 Training: 0.6420753792879978 Seed: 4443\n",
      "Testing: 0.6467412704319454 Training: 0.6450775321426334 Seed: 4445\n",
      "Testing: 0.6524214896864629 Training: 0.6435113040009555 Seed: 4446\n",
      "Testing: 0.6513459068371815 Training: 0.6438381506345552 Seed: 4447\n",
      "Testing: 0.6615727857289145 Training: 0.6411777444499287 Seed: 4450\n",
      "Testing: 0.6478334609172488 Training: 0.6447123252081945 Seed: 4456\n",
      "Testing: 0.665837537114721 Training: 0.6401739146978412 Seed: 4457\n",
      "Testing: 0.6633502752283649 Training: 0.6409343561114744 Seed: 4461\n",
      "Testing: 0.6509714847877219 Training: 0.6439643482139035 Seed: 4462\n",
      "Testing: 0.646552239648833 Training: 0.6451071581109449 Seed: 4463\n",
      "Testing: 0.6695366605829246 Training: 0.6392267632585624 Seed: 4466\n",
      "Testing: 0.6551764382872965 Training: 0.6429574241313585 Seed: 4469\n",
      "Testing: 0.6551397250797486 Training: 0.6429645015248505 Seed: 4470\n",
      "Testing: 0.6652928944167704 Training: 0.6400795626305623 Seed: 4471\n",
      "Testing: 0.6535185375594327 Training: 0.6433296486993352 Seed: 4472\n",
      "Testing: 0.6488706583460823 Training: 0.6445545891696888 Seed: 4476\n",
      "Testing: 0.6522307661890838 Training: 0.643768010468502 Seed: 4477\n",
      "Testing: 0.6605832249082795 Training: 0.6413896243798454 Seed: 4481\n",
      "Testing: 0.6673786116484955 Training: 0.6397706731524955 Seed: 4482\n",
      "Testing: 0.6467126613926233 Training: 0.645001093248921 Seed: 4484\n",
      "Testing: 0.650850843695355 Training: 0.6440952667506794 Seed: 4485\n",
      "Testing: 0.648668972889455 Training: 0.6445871057383675 Seed: 4486\n",
      "Testing: 0.6582900334816438 Training: 0.6421290654105243 Seed: 4490\n",
      "Testing: 0.6469928350680726 Training: 0.6449535936116737 Seed: 4495\n",
      "Testing: 0.6512796967071115 Training: 0.6438662931324894 Seed: 4496\n",
      "Testing: 0.6582395062036249 Training: 0.6419465577024155 Seed: 4497\n",
      "Testing: 0.6508800908911299 Training: 0.643924683241858 Seed: 4498\n",
      "Testing: 0.6508629729767677 Training: 0.6439623668901497 Seed: 4500\n",
      "Testing: 0.6497398871230392 Training: 0.6442494041509084 Seed: 4503\n",
      "Testing: 0.6564965852775041 Training: 0.6424249834702541 Seed: 4504\n",
      "Testing: 0.6642034054283452 Training: 0.640481566878361 Seed: 4507\n",
      "Testing: 0.6483277107966786 Training: 0.6445844449825957 Seed: 4508\n",
      "Testing: 0.6587751816699845 Training: 0.6418732581287495 Seed: 4509\n",
      "Testing: 0.6456990433120684 Training: 0.6452943572476848 Seed: 4512\n",
      "Testing: 0.6560373769924559 Training: 0.642513824036846 Seed: 4515\n",
      "Testing: 0.6587215400152491 Training: 0.6420788083306782 Seed: 4516\n",
      "Testing: 0.6533557219025434 Training: 0.6433210660388802 Seed: 4519\n",
      "Testing: 0.6485450517691678 Training: 0.6444451743603211 Seed: 4521\n",
      "Testing: 0.6546253659074888 Training: 0.6429871055774273 Seed: 4522\n",
      "Testing: 0.6505378172751226 Training: 0.6439910494739074 Seed: 4524\n",
      "Testing: 0.6555392343619004 Training: 0.6427885123035046 Seed: 4525\n",
      "Testing: 0.6492307544382017 Training: 0.6444804244261584 Seed: 4528\n",
      "Testing: 0.6597490343692493 Training: 0.6419590100494577 Seed: 4530\n",
      "Testing: 0.6539777712344457 Training: 0.6432490078547135 Seed: 4531\n",
      "Testing: 0.6591560194831827 Training: 0.6419791828746519 Seed: 4534\n",
      "Testing: 0.6507147272742775 Training: 0.6439381096650559 Seed: 4535\n",
      "Testing: 0.6532831533611526 Training: 0.6433472545389713 Seed: 4538\n",
      "Testing: 0.6485979449329552 Training: 0.644494998702607 Seed: 4540\n",
      "Testing: 0.6471847251289623 Training: 0.6449728947669446 Seed: 4541\n",
      "Testing: 0.6559855880717262 Training: 0.6428691483980818 Seed: 4542\n",
      "Testing: 0.6481623428205499 Training: 0.6446239334487645 Seed: 4543\n",
      "Testing: 0.6531053926866234 Training: 0.6434990088368415 Seed: 4544\n",
      "Testing: 0.6504808229365688 Training: 0.644118022150454 Seed: 4545\n",
      "Testing: 0.6493284278075986 Training: 0.6443056123061934 Seed: 4549\n",
      "Testing: 0.6478338254553971 Training: 0.6448221980661121 Seed: 4551\n",
      "Testing: 0.6473276123484528 Training: 0.6448426859693756 Seed: 4552\n",
      "Testing: 0.6476018004487406 Training: 0.6447171622231522 Seed: 4554\n",
      "Testing: 0.6495866138577818 Training: 0.6443044813265285 Seed: 4555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.660933427921133 Training: 0.6414138834638781 Seed: 4556\n",
      "Testing: 0.6620279482678422 Training: 0.6411130697766063 Seed: 4560\n",
      "Testing: 0.6592750471774294 Training: 0.6416188900473052 Seed: 4562\n",
      "Testing: 0.6556145017835247 Training: 0.6428125223274213 Seed: 4563\n",
      "Testing: 0.6560915557087215 Training: 0.6427733013486546 Seed: 4564\n",
      "Testing: 0.6475869584858098 Training: 0.6449005492218487 Seed: 4565\n",
      "Testing: 0.6485961814738055 Training: 0.6443918841974281 Seed: 4566\n",
      "Testing: 0.647904918052303 Training: 0.6446987593454373 Seed: 4567\n",
      "Testing: 0.6504014672199747 Training: 0.6441412909775948 Seed: 4568\n",
      "Testing: 0.6464436211022512 Training: 0.6451761092309991 Seed: 4569\n",
      "Testing: 0.6523705901118365 Training: 0.643574878033473 Seed: 4581\n",
      "Testing: 0.6531951772423632 Training: 0.6435469116562771 Seed: 4583\n",
      "Testing: 0.6556380966413378 Training: 0.6426634525231985 Seed: 4584\n",
      "Testing: 0.6524458339490312 Training: 0.643622630914038 Seed: 4585\n",
      "Testing: 0.6517772907760694 Training: 0.6437522102244098 Seed: 4590\n",
      "Testing: 0.6462949168462051 Training: 0.6451480447987823 Seed: 4593\n",
      "Testing: 0.6553306724674772 Training: 0.6429655245505181 Seed: 4595\n",
      "Testing: 0.6603498715716025 Training: 0.6414963775212295 Seed: 4598\n",
      "Testing: 0.6543589393142116 Training: 0.6430521445265898 Seed: 4600\n",
      "Testing: 0.6460250289895106 Training: 0.6452695851003052 Seed: 4601\n",
      "Testing: 0.6572528871506543 Training: 0.6423473456394586 Seed: 4607\n",
      "Testing: 0.6493812633733238 Training: 0.6443927071444752 Seed: 4609\n",
      "Testing: 0.654997445453136 Training: 0.643078426381479 Seed: 4610\n",
      "Testing: 0.6683378830860384 Training: 0.639625384395643 Seed: 4612\n",
      "Testing: 0.6556364773691945 Training: 0.6428458644954282 Seed: 4613\n",
      "Testing: 0.6545890175354793 Training: 0.6429368001049064 Seed: 4615\n",
      "Testing: 0.6617994555985588 Training: 0.6411018210959012 Seed: 4617\n",
      "Testing: 0.6568659017906149 Training: 0.6423664398497144 Seed: 4622\n",
      "Testing: 0.6483192073647874 Training: 0.6442962881012653 Seed: 4624\n",
      "Testing: 0.6596476319928958 Training: 0.6417884467127126 Seed: 4625\n",
      "Testing: 0.6491805320084425 Training: 0.6444814797796656 Seed: 4626\n",
      "Testing: 0.6479667262568192 Training: 0.6444208684254986 Seed: 4627\n",
      "Testing: 0.6474987639707699 Training: 0.6446983782684702 Seed: 4628\n",
      "Testing: 0.6498180209442768 Training: 0.6442160926294204 Seed: 4629\n",
      "Testing: 0.6557254798350337 Training: 0.6427944137886157 Seed: 4632\n",
      "Testing: 0.6531934573430096 Training: 0.6434332670404719 Seed: 4638\n",
      "Testing: 0.6520706939637688 Training: 0.6436843462722708 Seed: 4640\n",
      "Testing: 0.6477749133676507 Training: 0.6447004742574926 Seed: 4643\n",
      "Testing: 0.6588623687522686 Training: 0.6418429340478016 Seed: 4644\n",
      "Testing: 0.6595296801022212 Training: 0.641680109881493 Seed: 4646\n",
      "Testing: 0.6468210423460984 Training: 0.6450217732043118 Seed: 4649\n",
      "Testing: 0.6630313060575757 Training: 0.6410195894311814 Seed: 4650\n",
      "Testing: 0.6511429478827009 Training: 0.6439508832294698 Seed: 4651\n",
      "Testing: 0.6500363620805008 Training: 0.6442293008584189 Seed: 4652\n",
      "Testing: 0.6483475598752821 Training: 0.6446863069080541 Seed: 4653\n",
      "Testing: 0.6473834174914339 Training: 0.6448810746378284 Seed: 4654\n",
      "Testing: 0.649500905119978 Training: 0.6442515958539597 Seed: 4657\n",
      "Testing: 0.6472340522531177 Training: 0.6449086318034981 Seed: 4658\n",
      "Testing: 0.653305332938404 Training: 0.6434550027810726 Seed: 4659\n",
      "Testing: 0.660709349061294 Training: 0.6415452549435404 Seed: 4663\n",
      "Testing: 0.6556897740539092 Training: 0.642779266757112 Seed: 4664\n",
      "Testing: 0.6470026427083828 Training: 0.6448824834477596 Seed: 4665\n",
      "Testing: 0.6538757351502479 Training: 0.6432671361455476 Seed: 4667\n",
      "Testing: 0.6514398394500084 Training: 0.6437885857896664 Seed: 4668\n",
      "Testing: 0.6596350873625489 Training: 0.6418267950462194 Seed: 4669\n",
      "Testing: 0.6568269233333337 Training: 0.6425204457733193 Seed: 4670\n",
      "Testing: 0.6498449702174072 Training: 0.6443376616193721 Seed: 4672\n",
      "Testing: 0.6468474807752326 Training: 0.6450061653921295 Seed: 4673\n",
      "Testing: 0.6518356268167981 Training: 0.643728029013489 Seed: 4674\n",
      "Testing: 0.6505176579637404 Training: 0.6440194990476634 Seed: 4675\n",
      "Testing: 0.6512485409024871 Training: 0.64384722753236 Seed: 4678\n",
      "Testing: 0.6494912312670843 Training: 0.6442608117647906 Seed: 4680\n",
      "Testing: 0.657393213048362 Training: 0.6423453981937625 Seed: 4681\n",
      "Testing: 0.6538390458033838 Training: 0.6431650195942606 Seed: 4682\n",
      "Testing: 0.6463767371333579 Training: 0.6452137263780482 Seed: 4683\n",
      "Testing: 0.6473650162435348 Training: 0.6449102061662345 Seed: 4686\n",
      "Testing: 0.6615404032112259 Training: 0.6414765026069627 Seed: 4687\n",
      "Testing: 0.6555792761878056 Training: 0.6427821545130419 Seed: 4690\n",
      "Testing: 0.6598202589889798 Training: 0.6417050356740132 Seed: 4695\n",
      "Testing: 0.6504392353904382 Training: 0.6441398464789301 Seed: 4700\n",
      "Testing: 0.6563950655107436 Training: 0.6426087509857632 Seed: 4701\n",
      "Testing: 0.646968881000887 Training: 0.6449291896386068 Seed: 4705\n",
      "Testing: 0.6562578812724496 Training: 0.6426381114114569 Seed: 4707\n",
      "Testing: 0.6659466149895573 Training: 0.6402287013602952 Seed: 4709\n",
      "Testing: 0.6455636134852614 Training: 0.6453310145699969 Seed: 4710\n",
      "Testing: 0.6484127058906607 Training: 0.6444050731882672 Seed: 4711\n",
      "Testing: 0.6558336865972119 Training: 0.6427628601963856 Seed: 4712\n",
      "Testing: 0.660479989497316 Training: 0.6414550452658652 Seed: 4714\n",
      "Testing: 0.6619618916035763 Training: 0.6410862159082316 Seed: 4715\n",
      "Testing: 0.653701300856166 Training: 0.6432943297516933 Seed: 4716\n",
      "Testing: 0.6493150248973303 Training: 0.6444221000546633 Seed: 4717\n",
      "Testing: 0.663041274180065 Training: 0.6408758952127542 Seed: 4723\n",
      "Testing: 0.6468966148834228 Training: 0.6449665278981207 Seed: 4725\n",
      "Testing: 0.6478651228637977 Training: 0.6446938923183332 Seed: 4726\n",
      "Testing: 0.6622937133256875 Training: 0.6412582459342292 Seed: 4727\n",
      "Testing: 0.6598647822978922 Training: 0.6416999112701192 Seed: 4730\n",
      "Testing: 0.6479647560729432 Training: 0.6446096116792172 Seed: 4732\n",
      "Testing: 0.6668229605914635 Training: 0.6398218155843665 Seed: 4733\n",
      "Testing: 0.6455182776004647 Training: 0.6447964753775413 Seed: 4734\n",
      "Testing: 0.6471507717078251 Training: 0.6448044868896446 Seed: 4735\n",
      "Testing: 0.6459349683463418 Training: 0.6452133912598619 Seed: 4738\n",
      "Testing: 0.646327495786304 Training: 0.6451243379630268 Seed: 4741\n",
      "Testing: 0.6467795114879602 Training: 0.645078843511377 Seed: 4743\n",
      "Testing: 0.6575579560167217 Training: 0.6424501529184532 Seed: 4744\n",
      "Testing: 0.6544992609198494 Training: 0.6431412560942068 Seed: 4747\n",
      "Testing: 0.6548272784367964 Training: 0.643041003051938 Seed: 4750\n",
      "Testing: 0.6563726701838949 Training: 0.6425638459275562 Seed: 4751\n",
      "Testing: 0.6676576818835649 Training: 0.6396277511676823 Seed: 4752\n",
      "Testing: 0.6479241801230305 Training: 0.6447626805610922 Seed: 4754\n",
      "Testing: 0.6686215571678784 Training: 0.6394599203318577 Seed: 4755\n",
      "Testing: 0.6478348293296952 Training: 0.6447023920445482 Seed: 4756\n",
      "Testing: 0.6482198441935444 Training: 0.6445994059218627 Seed: 4757\n",
      "Testing: 0.6519461126558864 Training: 0.6437303269413389 Seed: 4760\n",
      "Testing: 0.6489368370670658 Training: 0.6443691445143946 Seed: 4761\n",
      "Testing: 0.649467441925116 Training: 0.6442979206275321 Seed: 4767\n",
      "Testing: 0.6655240139462703 Training: 0.6404702788042997 Seed: 4769\n",
      "Testing: 0.6505673406371812 Training: 0.6439942769789495 Seed: 4773\n",
      "Testing: 0.6588789686637314 Training: 0.6419628071936386 Seed: 4775\n",
      "Testing: 0.6557151056607186 Training: 0.6428281336087567 Seed: 4776\n",
      "Testing: 0.6499058952384447 Training: 0.6442131749844275 Seed: 4779\n",
      "Testing: 0.6559067571699679 Training: 0.6427136998805423 Seed: 4780\n",
      "Testing: 0.6479467197564027 Training: 0.6443628551965523 Seed: 4781\n",
      "Testing: 0.6670190281368429 Training: 0.6400805645208166 Seed: 4782\n",
      "Testing: 0.6530384028177558 Training: 0.6432640547053692 Seed: 4784\n",
      "Testing: 0.6639066271250907 Training: 0.6408005650872686 Seed: 4785\n",
      "Testing: 0.6455387888489434 Training: 0.6453935746307832 Seed: 4786\n",
      "Testing: 0.6469903447909274 Training: 0.6449827924720286 Seed: 4787\n",
      "Testing: 0.649627998828597 Training: 0.6442625521140846 Seed: 4789\n",
      "Testing: 0.654170616281322 Training: 0.6428872006734612 Seed: 4792\n",
      "Testing: 0.6558063312337814 Training: 0.642801879718985 Seed: 4793\n",
      "Testing: 0.6488922057065094 Training: 0.6444588721731848 Seed: 4794\n",
      "Testing: 0.6579365595599439 Training: 0.6419953596259845 Seed: 4795\n",
      "Testing: 0.6584768527407479 Training: 0.6420826862958131 Seed: 4797\n",
      "Testing: 0.645836381988097 Training: 0.6451857833664083 Seed: 4798\n",
      "Testing: 0.6521095081204774 Training: 0.6436299266457045 Seed: 4802\n",
      "Testing: 0.6649623661048301 Training: 0.6403887430328041 Seed: 4804\n",
      "Testing: 0.6662019532236593 Training: 0.639812505847335 Seed: 4806\n",
      "Testing: 0.6476911077888521 Training: 0.6448032422738589 Seed: 4808\n",
      "Testing: 0.648174953153777 Training: 0.6445440777490887 Seed: 4809\n",
      "Testing: 0.6489225211289165 Training: 0.6443597861527598 Seed: 4810\n",
      "Testing: 0.6508110500803383 Training: 0.6440129970047871 Seed: 4811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6662864921067488 Training: 0.6400291321656707 Seed: 4813\n",
      "Testing: 0.654271070530982 Training: 0.6431412374204505 Seed: 4815\n",
      "Testing: 0.6513863274107445 Training: 0.6439009377540471 Seed: 4821\n",
      "Testing: 0.6499899099803633 Training: 0.644254832402174 Seed: 4823\n",
      "Testing: 0.6550444549417053 Training: 0.6429129531283609 Seed: 4825\n",
      "Testing: 0.65243554030538 Training: 0.6435760268879798 Seed: 4830\n",
      "Testing: 0.6539171811132782 Training: 0.6433208772989567 Seed: 4831\n",
      "Testing: 0.6482360359713943 Training: 0.6446921680964712 Seed: 4832\n",
      "Testing: 0.6653634524166859 Training: 0.6406489919434535 Seed: 4835\n",
      "Testing: 0.6564523492349994 Training: 0.6426542600309765 Seed: 4837\n",
      "Testing: 0.6566611647103966 Training: 0.6424951952913538 Seed: 4838\n",
      "Testing: 0.6444304389656753 Training: 0.6443052200240397 Seed: 4840\n",
      "Testing: 0.651032370724377 Training: 0.6439407760878837 Seed: 4841\n",
      "Testing: 0.6501476849999872 Training: 0.6439677086334219 Seed: 4847\n",
      "Testing: 0.6526542529990421 Training: 0.6435322268983473 Seed: 4849\n",
      "Testing: 0.6492649961498368 Training: 0.644200197513457 Seed: 4850\n",
      "Testing: 0.647224988390067 Training: 0.6448728661616071 Seed: 4851\n",
      "Testing: 0.6646298052842066 Training: 0.640416622575551 Seed: 4852\n",
      "Testing: 0.6468237640406765 Training: 0.6448938722899868 Seed: 4853\n",
      "Testing: 0.6598228671131703 Training: 0.6414591476665624 Seed: 4854\n",
      "Testing: 0.662535240583223 Training: 0.6411363546807104 Seed: 4857\n",
      "Testing: 0.6563381542865638 Training: 0.6425944200240554 Seed: 4860\n",
      "Testing: 0.6482532162275988 Training: 0.6446539629522465 Seed: 4861\n",
      "Testing: 0.6517570862985131 Training: 0.6437346817311266 Seed: 4862\n",
      "Testing: 0.6657492254491895 Training: 0.6400535436416639 Seed: 4864\n",
      "Testing: 0.6513175186172746 Training: 0.6438536361999914 Seed: 4867\n",
      "Testing: 0.6501511940573995 Training: 0.6441823422948243 Seed: 4870\n",
      "Testing: 0.6609210045754951 Training: 0.641673678483442 Seed: 4871\n",
      "Testing: 0.6658509786607283 Training: 0.6402831948867642 Seed: 4872\n",
      "Testing: 0.6494010279170643 Training: 0.6443932409030443 Seed: 4873\n",
      "Testing: 0.6590641947093016 Training: 0.6418596999265885 Seed: 4874\n",
      "Testing: 0.6561217921452128 Training: 0.6424612976629582 Seed: 4875\n",
      "Testing: 0.6532139017369183 Training: 0.6432892802377747 Seed: 4877\n",
      "Testing: 0.655364474016276 Training: 0.642837129999051 Seed: 4880\n",
      "Testing: 0.6583173320544659 Training: 0.6419562242683138 Seed: 4882\n",
      "Testing: 0.6469843334361971 Training: 0.6445702695250526 Seed: 4883\n",
      "Testing: 0.6602729711459193 Training: 0.6416093827513168 Seed: 4884\n",
      "Testing: 0.6715723618285645 Training: 0.6389015045176114 Seed: 4887\n",
      "Testing: 0.6472955244387468 Training: 0.6449448359081562 Seed: 4888\n",
      "Testing: 0.6471430087787817 Training: 0.6448880575957154 Seed: 4890\n",
      "Testing: 0.655660570780118 Training: 0.642831156597187 Seed: 4896\n",
      "Testing: 0.6525664908739233 Training: 0.643566307823791 Seed: 4898\n",
      "Testing: 0.6462449321831584 Training: 0.6451488889734579 Seed: 4901\n",
      "Testing: 0.6498241126377944 Training: 0.6443627431475951 Seed: 4904\n",
      "Testing: 0.6457980029891576 Training: 0.6453662669418525 Seed: 4906\n",
      "Testing: 0.6532715129701279 Training: 0.643459830469513 Seed: 4909\n",
      "Testing: 0.6540186810715988 Training: 0.6432789484717285 Seed: 4913\n",
      "Testing: 0.648225669025138 Training: 0.6446278542473194 Seed: 4915\n",
      "Testing: 0.6460239176655233 Training: 0.6451769912460161 Seed: 4917\n",
      "Testing: 0.6503261390951899 Training: 0.6442167538897541 Seed: 4918\n",
      "Testing: 0.6763591825706986 Training: 0.6371677568125022 Seed: 4921\n",
      "Testing: 0.6462827997346061 Training: 0.644592153161275 Seed: 4922\n",
      "Testing: 0.6489170907403521 Training: 0.6445322050473661 Seed: 4923\n",
      "Testing: 0.6466186228222888 Training: 0.6451286880304643 Seed: 4924\n",
      "Testing: 0.6560106124402048 Training: 0.6426581732703135 Seed: 4927\n",
      "Testing: 0.6506241809218604 Training: 0.6440978654144414 Seed: 4929\n",
      "Testing: 0.6483097284514876 Training: 0.6446728249869641 Seed: 4932\n",
      "Testing: 0.6529951216233136 Training: 0.643454844188551 Seed: 4933\n",
      "Testing: 0.6471604678357286 Training: 0.6449250936605575 Seed: 4934\n",
      "Testing: 0.647077278421049 Training: 0.644825099080173 Seed: 4937\n",
      "Testing: 0.6616280858367475 Training: 0.6411162746052279 Seed: 4938\n",
      "Testing: 0.6511076692745309 Training: 0.6439040825236417 Seed: 4942\n",
      "Testing: 0.6522179892041252 Training: 0.6437278337812453 Seed: 4944\n",
      "Testing: 0.6457403398233144 Training: 0.6452057055087417 Seed: 4946\n",
      "Testing: 0.6461143293938819 Training: 0.6452489989652725 Seed: 4950\n",
      "Testing: 0.6458210989474829 Training: 0.6448340306366873 Seed: 4951\n",
      "Testing: 0.6495312859036864 Training: 0.6442980928364075 Seed: 4952\n",
      "Testing: 0.6569199923798341 Training: 0.6424539828474018 Seed: 4955\n",
      "Testing: 0.6471711561469994 Training: 0.6449575580374345 Seed: 4958\n",
      "Testing: 0.6469022958868773 Training: 0.6449840613960637 Seed: 4963\n",
      "Testing: 0.6478530174967695 Training: 0.6448394071765785 Seed: 4966\n",
      "Testing: 0.648857246930689 Training: 0.6444384653985009 Seed: 4967\n",
      "Testing: 0.661628210371924 Training: 0.641306061200867 Seed: 4968\n",
      "Testing: 0.6505889517436334 Training: 0.6441241125548398 Seed: 4969\n",
      "Testing: 0.6640469205329772 Training: 0.6405371806252114 Seed: 4970\n",
      "Testing: 0.6503864846071612 Training: 0.6439814706520006 Seed: 4971\n",
      "Testing: 0.6479565632838382 Training: 0.6448209684869096 Seed: 4973\n",
      "Testing: 0.6499904777962935 Training: 0.6442613139335187 Seed: 4975\n",
      "Testing: 0.6661684149394478 Training: 0.6401677426962213 Seed: 4978\n",
      "Testing: 0.64858860717251 Training: 0.6446122058532056 Seed: 4979\n",
      "Testing: 0.6554109997774817 Training: 0.6427380032157329 Seed: 4980\n",
      "Testing: 0.65206263007141 Training: 0.6437576710443591 Seed: 4981\n",
      "Testing: 0.6501192106574569 Training: 0.6441775032508947 Seed: 4983\n",
      "Testing: 0.6595845248535833 Training: 0.6419255720380285 Seed: 4984\n",
      "Testing: 0.6471588749269485 Training: 0.6449414469562083 Seed: 4985\n",
      "Testing: 0.6597160262560113 Training: 0.6417729125705821 Seed: 4987\n",
      "Testing: 0.6510996681864704 Training: 0.6439914032558234 Seed: 4989\n",
      "Testing: 0.651895362513714 Training: 0.6437473425279825 Seed: 4990\n",
      "Testing: 0.6601494174267357 Training: 0.6414358149669201 Seed: 4993\n",
      "Testing: 0.6596457100237473 Training: 0.6418734871630454 Seed: 4994\n",
      "Testing: 0.6562699047432248 Training: 0.6427705786786247 Seed: 4998\n",
      "Testing: 0.6578758393536863 Training: 0.6423364247363906 Seed: 5000\n",
      "Testing: 0.654575012775716 Training: 0.6432124006294069 Seed: 5002\n",
      "Testing: 0.6497664702400955 Training: 0.6443328572010141 Seed: 5003\n",
      "Testing: 0.6655162561484724 Training: 0.6404377679004079 Seed: 5004\n",
      "Testing: 0.653486781539142 Training: 0.6432606570739321 Seed: 5005\n",
      "Testing: 0.6464167683162301 Training: 0.6450533523119689 Seed: 5006\n",
      "Testing: 0.6578115290284908 Training: 0.6423528148852049 Seed: 5007\n",
      "Testing: 0.6510009124558158 Training: 0.6439506392478437 Seed: 5009\n",
      "Testing: 0.6567476622806004 Training: 0.6424624176060778 Seed: 5010\n",
      "Testing: 0.6542767166443704 Training: 0.6432324368271813 Seed: 5012\n",
      "Testing: 0.6517174287461985 Training: 0.6437779784524557 Seed: 5013\n",
      "Testing: 0.6563387710611615 Training: 0.6426786962277868 Seed: 5015\n",
      "Testing: 0.6473921251340863 Training: 0.6448462966716104 Seed: 5016\n",
      "Testing: 0.6514345687423203 Training: 0.6437406880480007 Seed: 5017\n",
      "Testing: 0.6490288623441559 Training: 0.6445106166859937 Seed: 5019\n",
      "Testing: 0.647457152785484 Training: 0.6448866589599708 Seed: 5024\n",
      "Testing: 0.6508855530651514 Training: 0.6439054170855165 Seed: 5027\n",
      "Testing: 0.6619809377257047 Training: 0.6410058646323331 Seed: 5028\n",
      "Testing: 0.6580691465111783 Training: 0.6422004064632622 Seed: 5032\n",
      "Testing: 0.6482266878108807 Training: 0.6447461110867784 Seed: 5034\n",
      "Testing: 0.6536485719895624 Training: 0.6429869905808729 Seed: 5035\n",
      "Testing: 0.6483095369837022 Training: 0.6444623812231457 Seed: 5036\n",
      "Testing: 0.6490407466213475 Training: 0.6444569459271812 Seed: 5037\n",
      "Testing: 0.659805258067868 Training: 0.6416705705554099 Seed: 5038\n",
      "Testing: 0.6487054373417419 Training: 0.6445132962162198 Seed: 5039\n",
      "Testing: 0.649170788557518 Training: 0.6443046929795729 Seed: 5040\n",
      "Testing: 0.6557819285873747 Training: 0.642849565481457 Seed: 5041\n",
      "Testing: 0.6505253742053161 Training: 0.6439968048232856 Seed: 5042\n",
      "Testing: 0.6496878078500608 Training: 0.6444084595719467 Seed: 5043\n",
      "Testing: 0.6470729223341272 Training: 0.6449548629102677 Seed: 5044\n",
      "Testing: 0.6618967070550986 Training: 0.6411346392856213 Seed: 5046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6577925575480204 Training: 0.6418069306922484 Seed: 5051\n",
      "Testing: 0.6578119376084175 Training: 0.6422634601280095 Seed: 5055\n",
      "Testing: 0.6648885312009857 Training: 0.6404367579603796 Seed: 5056\n",
      "Testing: 0.6488925656065251 Training: 0.644363900628071 Seed: 5058\n",
      "Testing: 0.6688710804726385 Training: 0.6394064617224196 Seed: 5064\n",
      "Testing: 0.6496577024198624 Training: 0.6443430880883808 Seed: 5065\n",
      "Testing: 0.6758975570022662 Training: 0.6374093644626256 Seed: 5066\n",
      "Testing: 0.6516473432069823 Training: 0.6438673777325571 Seed: 5068\n",
      "Testing: 0.6497589316381684 Training: 0.6443171448935847 Seed: 5078\n",
      "Testing: 0.6496992797154624 Training: 0.6443342579293149 Seed: 5079\n",
      "Testing: 0.6524547261808962 Training: 0.6436814987542318 Seed: 5081\n",
      "Testing: 0.6536844593222616 Training: 0.6428989270185155 Seed: 5083\n",
      "Testing: 0.6463976622122535 Training: 0.6451342486417706 Seed: 5084\n",
      "Testing: 0.6537067753296748 Training: 0.6433057282055359 Seed: 5086\n",
      "Testing: 0.6468279487513551 Training: 0.645075160429728 Seed: 5087\n",
      "Testing: 0.6538558063453517 Training: 0.6432130095893143 Seed: 5090\n",
      "Testing: 0.6459143331277185 Training: 0.6452806153713959 Seed: 5094\n",
      "Testing: 0.6490483577068601 Training: 0.6444291832367871 Seed: 5095\n",
      "Testing: 0.6605930932243399 Training: 0.6414577911077763 Seed: 5098\n",
      "Testing: 0.6471614917438182 Training: 0.6449754522414759 Seed: 5100\n",
      "Testing: 0.6538214905081092 Training: 0.6432337256281762 Seed: 5104\n",
      "Testing: 0.6527451909469772 Training: 0.643542439694975 Seed: 5105\n",
      "Testing: 0.6534846695093568 Training: 0.6432734105411817 Seed: 5107\n",
      "Testing: 0.6628087059152699 Training: 0.6411007604331507 Seed: 5110\n",
      "Testing: 0.6591840003655857 Training: 0.6419444382884181 Seed: 5112\n",
      "Testing: 0.6525109572309371 Training: 0.6435543988954209 Seed: 5120\n",
      "Testing: 0.6524754038045619 Training: 0.6435953691464436 Seed: 5122\n",
      "Testing: 0.6473963519316855 Training: 0.6448744307325348 Seed: 5124\n",
      "Testing: 0.6474817971003108 Training: 0.6448212953978272 Seed: 5127\n",
      "Testing: 0.6625707272481233 Training: 0.6406317412507077 Seed: 5128\n",
      "Testing: 0.6464348587778247 Training: 0.6451660792018385 Seed: 5130\n",
      "Testing: 0.6466044753518139 Training: 0.6450592064575634 Seed: 5134\n",
      "Testing: 0.6477262116567255 Training: 0.644777232479714 Seed: 5138\n",
      "Testing: 0.656692504016478 Training: 0.6425208494541244 Seed: 5139\n",
      "Testing: 0.6621210396685941 Training: 0.6410200832766939 Seed: 5140\n",
      "Testing: 0.662078143164923 Training: 0.6410719903019577 Seed: 5142\n",
      "Testing: 0.6537289874560356 Training: 0.6432134413590385 Seed: 5143\n",
      "Testing: 0.6542342513973933 Training: 0.6431736320602548 Seed: 5144\n",
      "Testing: 0.661367893984971 Training: 0.6412090659951533 Seed: 5145\n",
      "Testing: 0.6496151402494192 Training: 0.6441738047446603 Seed: 5146\n",
      "Testing: 0.651921868985093 Training: 0.6437573880525358 Seed: 5147\n",
      "Testing: 0.6715770663307208 Training: 0.6388888724960745 Seed: 5149\n",
      "Testing: 0.6674332809299897 Training: 0.6399979260511331 Seed: 5151\n",
      "Testing: 0.6707586768243474 Training: 0.6391039290464866 Seed: 5154\n",
      "Testing: 0.6457516429933888 Training: 0.6451614622534929 Seed: 5156\n",
      "Testing: 0.6455928599767663 Training: 0.6449974867923155 Seed: 5160\n",
      "Testing: 0.6476897985015608 Training: 0.6445152819280644 Seed: 5166\n",
      "Testing: 0.6463234422644313 Training: 0.6449644068775987 Seed: 5167\n",
      "Testing: 0.6494156206054875 Training: 0.644358017073784 Seed: 5170\n",
      "Testing: 0.6711747407627577 Training: 0.6386657179696676 Seed: 5171\n",
      "Testing: 0.6544576726228422 Training: 0.6431580286880412 Seed: 5172\n",
      "Testing: 0.6468300635262347 Training: 0.6449835673627756 Seed: 5173\n",
      "Testing: 0.6509859790969885 Training: 0.6439298132216171 Seed: 5175\n",
      "Testing: 0.6500145693432536 Training: 0.6440334076303825 Seed: 5178\n",
      "Testing: 0.6536405062818142 Training: 0.6434112248708352 Seed: 5180\n",
      "Testing: 0.6489479853975724 Training: 0.6445134816451956 Seed: 5183\n",
      "Testing: 0.6517007085210287 Training: 0.6438827074606738 Seed: 5185\n",
      "Testing: 0.6492837983331919 Training: 0.6444395798328046 Seed: 5186\n",
      "Testing: 0.646027804818405 Training: 0.6452512028747401 Seed: 5188\n",
      "Testing: 0.6542410471334675 Training: 0.6431409762882985 Seed: 5189\n",
      "Testing: 0.6571022618235345 Training: 0.6424672247153917 Seed: 5190\n",
      "Testing: 0.6535929572360252 Training: 0.6432156835229454 Seed: 5191\n",
      "Testing: 0.6570140018857513 Training: 0.6424301171230834 Seed: 5192\n",
      "Testing: 0.6576802058271323 Training: 0.6423463531460938 Seed: 5193\n",
      "Testing: 0.6455369888265576 Training: 0.6449534875921958 Seed: 5195\n",
      "Testing: 0.6608091544206872 Training: 0.64149875455903 Seed: 5197\n",
      "Testing: 0.6507514337843516 Training: 0.6440724068222318 Seed: 5198\n",
      "Testing: 0.6509983955949777 Training: 0.6438145739250414 Seed: 5202\n",
      "Testing: 0.6668927976061945 Training: 0.6401510861307007 Seed: 5206\n",
      "Testing: 0.6459576417444499 Training: 0.6451924123685912 Seed: 5207\n",
      "Testing: 0.6555344654001583 Training: 0.6427700631020805 Seed: 5208\n",
      "Testing: 0.6595510756285223 Training: 0.6417550718638514 Seed: 5211\n",
      "Testing: 0.6489826626910113 Training: 0.6445115938672326 Seed: 5212\n",
      "Testing: 0.6511108622993913 Training: 0.643950513565029 Seed: 5216\n",
      "Testing: 0.6543481707206583 Training: 0.6431354088270297 Seed: 5217\n",
      "Testing: 0.6508328277490705 Training: 0.6440550518994232 Seed: 5218\n",
      "Testing: 0.6598882299456028 Training: 0.6417592939795732 Seed: 5223\n",
      "Testing: 0.6492057217965659 Training: 0.6445109656759698 Seed: 5224\n",
      "Testing: 0.6552484426989295 Training: 0.6429204911272677 Seed: 5226\n",
      "Testing: 0.6581661719232216 Training: 0.6421421656818527 Seed: 5231\n",
      "Testing: 0.6508743521397905 Training: 0.64389266557122 Seed: 5236\n",
      "Testing: 0.648326995305538 Training: 0.6446475029639156 Seed: 5237\n",
      "Testing: 0.6537025083476105 Training: 0.6432346724693294 Seed: 5238\n",
      "Testing: 0.6642655516677072 Training: 0.6406466511522642 Seed: 5239\n",
      "Testing: 0.6549394038593704 Training: 0.6426678417864721 Seed: 5242\n",
      "Testing: 0.6498723697675417 Training: 0.6442568129640946 Seed: 5244\n",
      "Testing: 0.6564336728660903 Training: 0.6425568314687389 Seed: 5246\n",
      "Testing: 0.6485562835197436 Training: 0.6444558627326207 Seed: 5251\n",
      "Testing: 0.649681866531201 Training: 0.6441795910494102 Seed: 5252\n",
      "Testing: 0.6591212520370757 Training: 0.6416935209513661 Seed: 5254\n",
      "Testing: 0.6718841414884358 Training: 0.6385043672839199 Seed: 5255\n",
      "Testing: 0.6572180341896204 Training: 0.6424459503930944 Seed: 5257\n",
      "Testing: 0.6512872837243326 Training: 0.6438171473467174 Seed: 5258\n",
      "Testing: 0.6455411670684974 Training: 0.6454070048432023 Seed: 5263\n",
      "Testing: 0.6455971806083919 Training: 0.6453402687494232 Seed: 5264\n",
      "Testing: 0.6666109942757816 Training: 0.6399431059853071 Seed: 5267\n",
      "Testing: 0.647855764555249 Training: 0.6447245780838083 Seed: 5270\n",
      "Testing: 0.6504468294699255 Training: 0.6437314788593772 Seed: 5271\n",
      "Testing: 0.6625053513085052 Training: 0.6409501438730574 Seed: 5274\n",
      "Testing: 0.6608038986103139 Training: 0.6414125438580096 Seed: 5277\n",
      "Testing: 0.6514982989268936 Training: 0.6438110235590124 Seed: 5278\n",
      "Testing: 0.6591120806548808 Training: 0.6420169605463042 Seed: 5283\n",
      "Testing: 0.6505559425332372 Training: 0.6439810407355732 Seed: 5287\n",
      "Testing: 0.6525736776006574 Training: 0.6435195414931529 Seed: 5290\n",
      "Testing: 0.6519424836306629 Training: 0.6437412618351785 Seed: 5295\n",
      "Testing: 0.6487447962143403 Training: 0.6444712690899292 Seed: 5296\n",
      "Testing: 0.6478292933240446 Training: 0.6447527889061062 Seed: 5299\n",
      "Testing: 0.6511696170003319 Training: 0.6439778084712268 Seed: 5304\n",
      "Testing: 0.647012054723977 Training: 0.6450004948567947 Seed: 5307\n",
      "Testing: 0.6464876619425874 Training: 0.645037163165101 Seed: 5308\n",
      "Testing: 0.6520128586117768 Training: 0.6436909638760638 Seed: 5312\n",
      "Testing: 0.6542292975102211 Training: 0.6431373923931346 Seed: 5315\n",
      "Testing: 0.6537612333999149 Training: 0.6433196469926387 Seed: 5316\n",
      "Testing: 0.650055035261958 Training: 0.6442258204647915 Seed: 5317\n",
      "Testing: 0.6480043744121062 Training: 0.6446303313901142 Seed: 5318\n",
      "Testing: 0.6672455643832822 Training: 0.6398674183871309 Seed: 5319\n",
      "Testing: 0.6467795934171489 Training: 0.6449908751189554 Seed: 5321\n",
      "Testing: 0.6477517932118787 Training: 0.6447596546904805 Seed: 5324\n",
      "Testing: 0.6636232070657871 Training: 0.6406486934781983 Seed: 5325\n",
      "Testing: 0.661936075829595 Training: 0.6413193653041068 Seed: 5326\n",
      "Testing: 0.6646683459848874 Training: 0.6404676915986739 Seed: 5327\n",
      "Testing: 0.6548903362849454 Training: 0.6429625849190714 Seed: 5328\n",
      "Testing: 0.6647314668315725 Training: 0.6405073076826373 Seed: 5330\n",
      "Testing: 0.6530967421075775 Training: 0.6434381968244041 Seed: 5331\n",
      "Testing: 0.6523530872412594 Training: 0.6436141967944008 Seed: 5332\n",
      "Testing: 0.6711320654507796 Training: 0.6389323152957569 Seed: 5335\n",
      "Testing: 0.661587411084168 Training: 0.6413191506627145 Seed: 5337\n",
      "Testing: 0.6603306736004615 Training: 0.641750897879999 Seed: 5338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.650121403863515 Training: 0.6439932857099665 Seed: 5340\n",
      "Testing: 0.6479378401035965 Training: 0.6445422361427695 Seed: 5342\n",
      "Testing: 0.659948907298939 Training: 0.6418052181259828 Seed: 5345\n",
      "Testing: 0.6466246271417786 Training: 0.6451101007143545 Seed: 5347\n",
      "Testing: 0.648248865763173 Training: 0.6446364449078159 Seed: 5355\n",
      "Testing: 0.6527808575210798 Training: 0.6435829455376548 Seed: 5360\n",
      "Testing: 0.6461140785254793 Training: 0.64516767978871 Seed: 5364\n",
      "Testing: 0.6574546471659463 Training: 0.6422916615427872 Seed: 5366\n",
      "Testing: 0.6713299216440349 Training: 0.6389091177028443 Seed: 5367\n",
      "Testing: 0.6592444518632021 Training: 0.6418385681320471 Seed: 5368\n",
      "Testing: 0.6481113917090181 Training: 0.6447232115871948 Seed: 5372\n",
      "Testing: 0.6479839262158786 Training: 0.644727228048691 Seed: 5374\n",
      "Testing: 0.6482248648249572 Training: 0.6445433889139244 Seed: 5379\n",
      "Testing: 0.6459013209660964 Training: 0.6451610291505907 Seed: 5381\n",
      "Testing: 0.6589666090950224 Training: 0.6419275429802944 Seed: 5382\n",
      "Testing: 0.6623524519779951 Training: 0.641051385000321 Seed: 5384\n",
      "Testing: 0.6495350521695149 Training: 0.644359610666446 Seed: 5385\n",
      "Testing: 0.6593309681236934 Training: 0.6419744618915839 Seed: 5386\n",
      "Testing: 0.6551567524011085 Training: 0.6429436827632149 Seed: 5387\n",
      "Testing: 0.6614363633037341 Training: 0.6412075885209068 Seed: 5390\n",
      "Testing: 0.6539119450699855 Training: 0.643312952055635 Seed: 5394\n",
      "Testing: 0.6502898184337067 Training: 0.6440480984776136 Seed: 5396\n",
      "Testing: 0.6496232569555418 Training: 0.6444304882536694 Seed: 5400\n",
      "Testing: 0.6474132888261538 Training: 0.6449056347025381 Seed: 5402\n",
      "Testing: 0.6562037391957678 Training: 0.6427331683433496 Seed: 5405\n",
      "Testing: 0.6606229694137109 Training: 0.6412200755920585 Seed: 5406\n",
      "Testing: 0.6512946036717266 Training: 0.6434940660464721 Seed: 5407\n",
      "Testing: 0.6496875955451871 Training: 0.6441994464325571 Seed: 5409\n",
      "Testing: 0.6526677747878797 Training: 0.6435707089108991 Seed: 5411\n",
      "Testing: 0.6495651702530011 Training: 0.6443028971121147 Seed: 5415\n",
      "Testing: 0.650062675053393 Training: 0.6441787987746801 Seed: 5418\n",
      "Testing: 0.6468473735782769 Training: 0.6450582115385021 Seed: 5421\n",
      "Testing: 0.6474223492115562 Training: 0.6448014707791138 Seed: 5424\n",
      "Testing: 0.6466032063076738 Training: 0.6450584122906056 Seed: 5425\n",
      "Testing: 0.6455465479275881 Training: 0.6453748550901883 Seed: 5427\n",
      "Testing: 0.652165374207208 Training: 0.643653165669488 Seed: 5433\n",
      "Testing: 0.6454056980550047 Training: 0.6454002565112753 Seed: 5434\n",
      "Testing: 0.652708926195941 Training: 0.6433707982145067 Seed: 5436\n",
      "Testing: 0.656627750727263 Training: 0.6425834832139767 Seed: 5437\n",
      "Testing: 0.6461885075283172 Training: 0.6450998960261802 Seed: 5438\n",
      "Testing: 0.6589577936953385 Training: 0.6420270468674144 Seed: 5439\n",
      "Testing: 0.6487211197123478 Training: 0.6444621061081776 Seed: 5444\n",
      "Testing: 0.6642894447677307 Training: 0.6407866014586032 Seed: 5446\n",
      "Testing: 0.6575452690126605 Training: 0.6424630763128428 Seed: 5447\n",
      "Testing: 0.6646811092361793 Training: 0.6403172754177531 Seed: 5448\n",
      "Testing: 0.6536996615073896 Training: 0.6433693192640526 Seed: 5449\n",
      "Testing: 0.6467906612673407 Training: 0.6449901792697231 Seed: 5452\n",
      "Testing: 0.6468730002248857 Training: 0.6448904484791411 Seed: 5453\n",
      "Testing: 0.6649417324537429 Training: 0.6407032715411811 Seed: 5454\n",
      "Testing: 0.6470444985775646 Training: 0.6449882703756897 Seed: 5456\n",
      "Testing: 0.6536748959243952 Training: 0.6434032620478605 Seed: 5457\n",
      "Testing: 0.6548727407767437 Training: 0.6428333189328547 Seed: 5459\n",
      "Testing: 0.6497983064725898 Training: 0.6442179478899392 Seed: 5465\n",
      "Testing: 0.6473370944196992 Training: 0.6449120131548853 Seed: 5466\n",
      "Testing: 0.6478144568978212 Training: 0.6443480152530001 Seed: 5468\n",
      "Testing: 0.6515658575458223 Training: 0.643873949603829 Seed: 5469\n",
      "Testing: 0.6535881922063179 Training: 0.6432937993474458 Seed: 5470\n",
      "Testing: 0.6512124218806538 Training: 0.6437971529503852 Seed: 5471\n",
      "Testing: 0.6537636458366889 Training: 0.6433592034601305 Seed: 5472\n",
      "Testing: 0.6549503815556802 Training: 0.6430737424315844 Seed: 5477\n",
      "Testing: 0.646092730451656 Training: 0.645178930232223 Seed: 5479\n",
      "Testing: 0.6460215963920304 Training: 0.6452154369296651 Seed: 5482\n",
      "Testing: 0.6488329323847758 Training: 0.6445642906619163 Seed: 5485\n",
      "Testing: 0.6535156662575763 Training: 0.6431737566439578 Seed: 5486\n",
      "Testing: 0.6466322157848762 Training: 0.6450586191639358 Seed: 5488\n",
      "Testing: 0.6470374828944668 Training: 0.644991981917463 Seed: 5489\n",
      "Testing: 0.6471521835377667 Training: 0.6448065664024452 Seed: 5490\n",
      "Testing: 0.6523293569974518 Training: 0.6436352924210142 Seed: 5495\n",
      "Testing: 0.6622649415893394 Training: 0.6413098868825968 Seed: 5498\n",
      "Testing: 0.649504092339703 Training: 0.6441827840477115 Seed: 5501\n",
      "Testing: 0.6538419475140226 Training: 0.6433125802042101 Seed: 5503\n",
      "Testing: 0.6535746196473575 Training: 0.643349900688086 Seed: 5505\n",
      "Testing: 0.6492732740536437 Training: 0.6444156058981878 Seed: 5506\n",
      "Testing: 0.6480992024085392 Training: 0.64470410935065 Seed: 5507\n",
      "Testing: 0.6703812661531824 Training: 0.6390406715713857 Seed: 5515\n",
      "Testing: 0.6514123613201596 Training: 0.6439347468965425 Seed: 5516\n",
      "Testing: 0.6639678026274809 Training: 0.6408994603796379 Seed: 5517\n",
      "Testing: 0.6492078882185607 Training: 0.6444301273560837 Seed: 5518\n",
      "Testing: 0.6535381209257289 Training: 0.6433916974049099 Seed: 5522\n",
      "Testing: 0.654175550974648 Training: 0.6429268924155099 Seed: 5524\n",
      "Testing: 0.6545987951949386 Training: 0.6430519979035171 Seed: 5528\n",
      "Testing: 0.6574262962779271 Training: 0.6423784887942652 Seed: 5530\n",
      "Testing: 0.6458039672544488 Training: 0.6452774575247991 Seed: 5531\n",
      "Testing: 0.6470753782015684 Training: 0.6448973550393873 Seed: 5532\n",
      "Testing: 0.6488710242470763 Training: 0.6443130075466756 Seed: 5535\n",
      "Testing: 0.6483338843397257 Training: 0.6446121926882251 Seed: 5538\n",
      "Testing: 0.6565991692005562 Training: 0.6424435853499646 Seed: 5541\n",
      "Testing: 0.659479662082556 Training: 0.6418797398234557 Seed: 5544\n",
      "Testing: 0.6608145986338348 Training: 0.6411774538938864 Seed: 5547\n",
      "Testing: 0.6535995467310549 Training: 0.6432211073439937 Seed: 5549\n",
      "Testing: 0.6519044428118008 Training: 0.6437807783010024 Seed: 5552\n",
      "Testing: 0.6656391543163693 Training: 0.6403660476819154 Seed: 5553\n",
      "Testing: 0.6559071026651219 Training: 0.6427001970248781 Seed: 5557\n",
      "Testing: 0.6560054593988188 Training: 0.6425680385345086 Seed: 5560\n",
      "Testing: 0.6517684215990762 Training: 0.6437932914488077 Seed: 5565\n",
      "Testing: 0.6478400935810957 Training: 0.6446701677579108 Seed: 5567\n",
      "Testing: 0.6652351534173829 Training: 0.6401846930677866 Seed: 5570\n",
      "Testing: 0.6565772062020045 Training: 0.6426999062415322 Seed: 5574\n",
      "Testing: 0.6587140368414889 Training: 0.6420292685051692 Seed: 5575\n",
      "Testing: 0.6499639924722267 Training: 0.6438393079418956 Seed: 5576\n",
      "Testing: 0.6655703696971842 Training: 0.6404375881981037 Seed: 5579\n",
      "Testing: 0.6488158784513874 Training: 0.6444051266484165 Seed: 5580\n",
      "Testing: 0.6618109738739315 Training: 0.6413902884233159 Seed: 5583\n",
      "Testing: 0.6463517720442467 Training: 0.6451165557883896 Seed: 5584\n",
      "Testing: 0.6472765951695832 Training: 0.6448317599186174 Seed: 5586\n",
      "Testing: 0.647634188604372 Training: 0.6448156292912973 Seed: 5587\n",
      "Testing: 0.6626238188467558 Training: 0.6409643734915907 Seed: 5589\n",
      "Testing: 0.6471238173183909 Training: 0.6449567148892765 Seed: 5590\n",
      "Testing: 0.6661386218377602 Training: 0.6401792122203533 Seed: 5592\n",
      "Testing: 0.6491726599222851 Training: 0.6443886105842908 Seed: 5593\n",
      "Testing: 0.6460075723477853 Training: 0.6451041338732861 Seed: 5596\n",
      "Testing: 0.6478865919895977 Training: 0.6447218119367302 Seed: 5597\n",
      "Testing: 0.657721140221412 Training: 0.6423933770519102 Seed: 5600\n",
      "Testing: 0.6513837253761121 Training: 0.6438724570786262 Seed: 5602\n",
      "Testing: 0.6583279872873233 Training: 0.6421206663721818 Seed: 5604\n",
      "Testing: 0.6498296619738836 Training: 0.644330392532167 Seed: 5607\n",
      "Testing: 0.657153582552281 Training: 0.6425004955116247 Seed: 5610\n",
      "Testing: 0.6600075334016345 Training: 0.6413729851559755 Seed: 5612\n",
      "Testing: 0.6609309601747185 Training: 0.6414152820854757 Seed: 5613\n",
      "Testing: 0.6556258435387379 Training: 0.6428597596553416 Seed: 5615\n",
      "Testing: 0.6461626613231839 Training: 0.6452371314721712 Seed: 5616\n",
      "Testing: 0.6473111510852799 Training: 0.644935906363034 Seed: 5617\n",
      "Testing: 0.6595837922816574 Training: 0.6416030017097238 Seed: 5618\n",
      "Testing: 0.6513460207017483 Training: 0.6437691300848829 Seed: 5622\n",
      "Testing: 0.6570078004030664 Training: 0.6424740840610811 Seed: 5623\n",
      "Testing: 0.6542733606813315 Training: 0.6426473997148721 Seed: 5624\n",
      "Testing: 0.6508521798139882 Training: 0.6438361937462875 Seed: 5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6583539965001508 Training: 0.6420089979030804 Seed: 5626\n",
      "Testing: 0.6610972047235318 Training: 0.6414802049707393 Seed: 5627\n",
      "Testing: 0.6509948347760272 Training: 0.6439833163355964 Seed: 5629\n",
      "Testing: 0.6458333558200919 Training: 0.6448215057170321 Seed: 5630\n",
      "Testing: 0.6677806970760273 Training: 0.639723312369531 Seed: 5633\n",
      "Testing: 0.6468870490919051 Training: 0.6446012014574991 Seed: 5634\n",
      "Testing: 0.657453515598565 Training: 0.6424816713325678 Seed: 5637\n",
      "Testing: 0.65204440321793 Training: 0.6434382239433445 Seed: 5640\n",
      "Testing: 0.6466946415075704 Training: 0.6449516885347679 Seed: 5645\n",
      "Testing: 0.6509360985830578 Training: 0.6438313995335108 Seed: 5646\n",
      "Testing: 0.6555669168605651 Training: 0.6428078536998961 Seed: 5647\n",
      "Testing: 0.6630591754455211 Training: 0.6408800399725716 Seed: 5649\n",
      "Testing: 0.6605642869561075 Training: 0.6418035492735616 Seed: 5652\n",
      "Testing: 0.6587983174371272 Training: 0.6420593420547698 Seed: 5653\n",
      "Testing: 0.6580774743333916 Training: 0.6419490603009801 Seed: 5654\n",
      "Testing: 0.6473560955364726 Training: 0.6448402050815939 Seed: 5658\n",
      "Testing: 0.6569828078198289 Training: 0.6424044508591993 Seed: 5659\n",
      "Testing: 0.6505660220035997 Training: 0.644022490612058 Seed: 5661\n",
      "Testing: 0.6461877014950959 Training: 0.6452343164763524 Seed: 5663\n",
      "Testing: 0.6463837385355037 Training: 0.6451443066481696 Seed: 5664\n",
      "Testing: 0.647306509332284 Training: 0.6449843265051737 Seed: 5665\n",
      "Testing: 0.6528307200947698 Training: 0.6435644717230585 Seed: 5666\n",
      "Testing: 0.6514473232738666 Training: 0.6438466946902754 Seed: 5670\n",
      "Testing: 0.6512473422796059 Training: 0.6439821021351052 Seed: 5671\n",
      "Testing: 0.6515595859427679 Training: 0.6437751149761708 Seed: 5673\n",
      "Testing: 0.6484266948835229 Training: 0.6445340991299452 Seed: 5674\n",
      "Testing: 0.652006868168497 Training: 0.6437703361768328 Seed: 5676\n",
      "Testing: 0.6514132537849944 Training: 0.6437845911130795 Seed: 5678\n",
      "Testing: 0.6590546839112454 Training: 0.6419619010309401 Seed: 5679\n",
      "Testing: 0.6512219220467429 Training: 0.6439004947178413 Seed: 5687\n",
      "Testing: 0.666936551112977 Training: 0.6397794589425907 Seed: 5689\n",
      "Testing: 0.6481820148537134 Training: 0.6446686136890348 Seed: 5691\n",
      "Testing: 0.6517773157505178 Training: 0.643766675376875 Seed: 5693\n",
      "Testing: 0.6519613411302766 Training: 0.6436825405197276 Seed: 5694\n",
      "Testing: 0.6464829737456403 Training: 0.6446336496725016 Seed: 5695\n",
      "Testing: 0.6565698505217443 Training: 0.6426455855681861 Seed: 5696\n",
      "Testing: 0.652331416380374 Training: 0.6436732820989993 Seed: 5698\n",
      "Testing: 0.6537205459964774 Training: 0.6432625755809798 Seed: 5699\n",
      "Testing: 0.6649686082591841 Training: 0.6399420741987737 Seed: 5700\n",
      "Testing: 0.6672519948459465 Training: 0.6397062589352107 Seed: 5701\n",
      "Testing: 0.6544039790794671 Training: 0.6430473189380528 Seed: 5702\n",
      "Testing: 0.6492864397195617 Training: 0.6443266975610618 Seed: 5707\n",
      "Testing: 0.6480368924631467 Training: 0.6447878879061791 Seed: 5708\n",
      "Testing: 0.6487326878315908 Training: 0.644540006145985 Seed: 5709\n",
      "Testing: 0.6530101957904837 Training: 0.6434436406018399 Seed: 5710\n",
      "Testing: 0.6570662925323901 Training: 0.6423536482455066 Seed: 5713\n",
      "Testing: 0.6569227661938142 Training: 0.6425650978844883 Seed: 5717\n",
      "Testing: 0.6501158633613393 Training: 0.6442183140484324 Seed: 5719\n",
      "Testing: 0.6464817823318791 Training: 0.6440928960744804 Seed: 5720\n",
      "Testing: 0.6611356589672351 Training: 0.6413875114161868 Seed: 5725\n",
      "Testing: 0.6674557512727877 Training: 0.6397022164771896 Seed: 5726\n",
      "Testing: 0.6600493230268543 Training: 0.6416871505559321 Seed: 5727\n",
      "Testing: 0.6462135508717772 Training: 0.6450752626115225 Seed: 5731\n",
      "Testing: 0.6485546171847678 Training: 0.644468005142761 Seed: 5733\n",
      "Testing: 0.6685978613642382 Training: 0.6395566921711918 Seed: 5735\n",
      "Testing: 0.6505273548086065 Training: 0.6440751745168489 Seed: 5738\n",
      "Testing: 0.6567439061339774 Training: 0.6425355264905707 Seed: 5739\n",
      "Testing: 0.6608540114861231 Training: 0.6414114626613288 Seed: 5742\n",
      "Testing: 0.658069168417331 Training: 0.6421946859661319 Seed: 5747\n",
      "Testing: 0.6512983298172442 Training: 0.6439061841377793 Seed: 5748\n",
      "Testing: 0.6531002920494361 Training: 0.6434531449473331 Seed: 5750\n",
      "Testing: 0.6463405251894143 Training: 0.6451917644827383 Seed: 5751\n",
      "Testing: 0.6456397019864031 Training: 0.645256505093796 Seed: 5756\n",
      "Testing: 0.6521158918622352 Training: 0.6437394600096934 Seed: 5757\n",
      "Testing: 0.658392966909699 Training: 0.642058858586937 Seed: 5760\n",
      "Testing: 0.6501825316616159 Training: 0.6441954612199285 Seed: 5761\n",
      "Testing: 0.646975520529769 Training: 0.6450535020228844 Seed: 5762\n",
      "Testing: 0.6551270927107524 Training: 0.6429441942137397 Seed: 5763\n",
      "Testing: 0.6534287865125071 Training: 0.6433581485561787 Seed: 5765\n",
      "Testing: 0.6460380860273631 Training: 0.6445278696995822 Seed: 5766\n",
      "Testing: 0.6471940660868664 Training: 0.6449415899485881 Seed: 5767\n",
      "Testing: 0.6460949216689366 Training: 0.6452047539042292 Seed: 5772\n",
      "Testing: 0.6513785339617699 Training: 0.6432032859497524 Seed: 5775\n",
      "Testing: 0.6737842804694675 Training: 0.6380779017779595 Seed: 5776\n",
      "Testing: 0.6549481209301037 Training: 0.6428429725543832 Seed: 5777\n",
      "Testing: 0.6489014790765949 Training: 0.6444520894154049 Seed: 5778\n",
      "Testing: 0.6486295551715975 Training: 0.644498633998141 Seed: 5783\n",
      "Testing: 0.6497626588220837 Training: 0.6442227754795883 Seed: 5785\n",
      "Testing: 0.6570056359933968 Training: 0.6422319660963011 Seed: 5787\n",
      "Testing: 0.6548421415097389 Training: 0.6431046565479592 Seed: 5789\n",
      "Testing: 0.660239890599712 Training: 0.6417649139285865 Seed: 5790\n",
      "Testing: 0.6624704851950455 Training: 0.6410688127061854 Seed: 5793\n",
      "Testing: 0.6572539228301808 Training: 0.6422899468242063 Seed: 5794\n",
      "Testing: 0.658485021771605 Training: 0.6420570929963323 Seed: 5795\n",
      "Testing: 0.6704370198921439 Training: 0.6389579651186135 Seed: 5799\n",
      "Testing: 0.6590288709447383 Training: 0.6419055572307284 Seed: 5800\n",
      "Testing: 0.6515384921103253 Training: 0.643907466079934 Seed: 5801\n",
      "Testing: 0.6527035242051855 Training: 0.6435230266131794 Seed: 5803\n",
      "Testing: 0.6495612523418908 Training: 0.644416759991773 Seed: 5804\n",
      "Testing: 0.6546749014379479 Training: 0.6430515726535231 Seed: 5805\n",
      "Testing: 0.6538507060326705 Training: 0.6433575163938579 Seed: 5808\n",
      "Testing: 0.6570225970457987 Training: 0.6423694467922427 Seed: 5809\n",
      "Testing: 0.6484812107416218 Training: 0.6444736132622885 Seed: 5812\n",
      "Testing: 0.6561704502884225 Training: 0.6416260335499231 Seed: 5813\n",
      "Testing: 0.6543574931177871 Training: 0.6431121805663236 Seed: 5815\n",
      "Testing: 0.65357161993803 Training: 0.6432737011590811 Seed: 5816\n",
      "Testing: 0.647309588805228 Training: 0.6449180435268784 Seed: 5818\n",
      "Testing: 0.6490087064556851 Training: 0.644411035014349 Seed: 5820\n",
      "Testing: 0.6492547631616755 Training: 0.6438709641851534 Seed: 5822\n",
      "Testing: 0.6454109900743935 Training: 0.6452572762507992 Seed: 5823\n",
      "Testing: 0.6529120007147171 Training: 0.6435099890729777 Seed: 5825\n",
      "Testing: 0.6555144641408063 Training: 0.6428220696123456 Seed: 5827\n",
      "Testing: 0.654023039639534 Training: 0.642954972777446 Seed: 5831\n",
      "Testing: 0.6645169716729419 Training: 0.6405861009842453 Seed: 5835\n",
      "Testing: 0.6533218421284461 Training: 0.6434126547146835 Seed: 5836\n",
      "Testing: 0.6464909765786456 Training: 0.6451487434307543 Seed: 5840\n",
      "Testing: 0.6485119258570645 Training: 0.6444492667147395 Seed: 5850\n",
      "Testing: 0.6652541567549649 Training: 0.6401426715787255 Seed: 5855\n",
      "Testing: 0.6471243137128799 Training: 0.6449557685705537 Seed: 5856\n",
      "Testing: 0.6503272944306628 Training: 0.6442139558806272 Seed: 5858\n",
      "Testing: 0.6499492055352485 Training: 0.6441967753652087 Seed: 5861\n",
      "Testing: 0.646650947449877 Training: 0.6449581429222773 Seed: 5862\n",
      "Testing: 0.6454635331050418 Training: 0.6454338270410991 Seed: 5863\n",
      "Testing: 0.6491764581596966 Training: 0.6443738551799637 Seed: 5865\n",
      "Testing: 0.6570400186738266 Training: 0.6424156605227116 Seed: 5866\n",
      "Testing: 0.6531237540530881 Training: 0.6430783676957756 Seed: 5867\n",
      "Testing: 0.6687324741849154 Training: 0.6391803985114178 Seed: 5869\n",
      "Testing: 0.6473847215829808 Training: 0.6448458657798067 Seed: 5871\n",
      "Testing: 0.662088690294564 Training: 0.6414468874281061 Seed: 5873\n",
      "Testing: 0.6551889800803236 Training: 0.6426688757984457 Seed: 5874\n",
      "Testing: 0.6586198503521444 Training: 0.6419471068240172 Seed: 5875\n",
      "Testing: 0.650633774395278 Training: 0.6440192339547535 Seed: 5879\n",
      "Testing: 0.6553311305010837 Training: 0.642857594881367 Seed: 5880\n",
      "Testing: 0.6506528206161758 Training: 0.6440646333529079 Seed: 5881\n",
      "Testing: 0.6621470172315281 Training: 0.6413445761780632 Seed: 5884\n",
      "Testing: 0.6497710950576171 Training: 0.6441668201366689 Seed: 5887\n",
      "Testing: 0.6469948280456409 Training: 0.6449259422340554 Seed: 5888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.648646894194034 Training: 0.6445905628982214 Seed: 5891\n",
      "Testing: 0.6497401833137889 Training: 0.644292771208476 Seed: 5895\n",
      "Testing: 0.6494793874254796 Training: 0.6443317342876459 Seed: 5896\n",
      "Testing: 0.647077195964567 Training: 0.6449265284779488 Seed: 5897\n",
      "Testing: 0.6465396441195339 Training: 0.644913412189929 Seed: 5898\n",
      "Testing: 0.6483135626334169 Training: 0.6442935053827581 Seed: 5899\n",
      "Testing: 0.6593529873229125 Training: 0.6417202420758585 Seed: 5900\n",
      "Testing: 0.6456814164859312 Training: 0.6453383660924819 Seed: 5901\n",
      "Testing: 0.6586472896151712 Training: 0.6418116115394816 Seed: 5903\n",
      "Testing: 0.6611483624554745 Training: 0.6410894664207004 Seed: 5904\n",
      "Testing: 0.648658639151372 Training: 0.6443365706212332 Seed: 5906\n",
      "Testing: 0.6528786645441033 Training: 0.6435098612490894 Seed: 5909\n",
      "Testing: 0.6508570538686383 Training: 0.643963049645517 Seed: 5910\n",
      "Testing: 0.6533043171279223 Training: 0.6434552494193458 Seed: 5911\n",
      "Testing: 0.6500527631441482 Training: 0.6440663015285244 Seed: 5912\n",
      "Testing: 0.6511029642844732 Training: 0.6438565513964085 Seed: 5915\n",
      "Testing: 0.6621391253819109 Training: 0.6411700251292347 Seed: 5917\n",
      "Testing: 0.6514894110505547 Training: 0.6436595684087854 Seed: 5918\n",
      "Testing: 0.6499414793108933 Training: 0.6443336269526163 Seed: 5919\n",
      "Testing: 0.6603044103218807 Training: 0.6414780197146309 Seed: 5924\n",
      "Testing: 0.6610256873289289 Training: 0.6414289493854994 Seed: 5928\n",
      "Testing: 0.6625468696996436 Training: 0.6411965580872256 Seed: 5930\n",
      "Testing: 0.653378662855419 Training: 0.6433461270266003 Seed: 5931\n",
      "Testing: 0.6517624588981941 Training: 0.6437279078504407 Seed: 5935\n",
      "Testing: 0.6490239707809131 Training: 0.6444398513987593 Seed: 5937\n",
      "Testing: 0.648576598037806 Training: 0.6445038783516821 Seed: 5941\n",
      "Testing: 0.649133181649303 Training: 0.6444006430941946 Seed: 5943\n",
      "Testing: 0.6485281242952139 Training: 0.6446459917440555 Seed: 5944\n",
      "Testing: 0.6635820359140466 Training: 0.6404977090008479 Seed: 5945\n",
      "Testing: 0.6478753191886626 Training: 0.6446849947130154 Seed: 5946\n",
      "Testing: 0.66283770157933 Training: 0.6410227566695837 Seed: 5948\n",
      "Testing: 0.6544242235091222 Training: 0.6431718167333418 Seed: 5958\n",
      "Testing: 0.6514467459727081 Training: 0.6439049274565471 Seed: 5959\n",
      "Testing: 0.6619475232999488 Training: 0.6413790221913516 Seed: 5960\n",
      "Testing: 0.6540170105021793 Training: 0.6431489445959817 Seed: 5961\n",
      "Testing: 0.6524362529142359 Training: 0.6437117220206983 Seed: 5963\n",
      "Testing: 0.6494133670271611 Training: 0.6442805333209872 Seed: 5964\n",
      "Testing: 0.6552993026764357 Training: 0.6429626795207772 Seed: 5966\n",
      "Testing: 0.6548159797247493 Training: 0.6430400475248291 Seed: 5970\n",
      "Testing: 0.6551118771208195 Training: 0.6429536521655108 Seed: 5971\n",
      "Testing: 0.6550777647970063 Training: 0.6428024278415321 Seed: 5972\n",
      "Testing: 0.6473502446128948 Training: 0.6446720796454213 Seed: 5974\n",
      "Testing: 0.6472875845455454 Training: 0.6444803604752503 Seed: 5975\n",
      "Testing: 0.6586528397520759 Training: 0.6421347936860884 Seed: 5976\n",
      "Testing: 0.6613255953260965 Training: 0.6413758328289454 Seed: 5977\n",
      "Testing: 0.6465726289226178 Training: 0.6450177726159857 Seed: 5978\n",
      "Testing: 0.6487770202893639 Training: 0.6445527491725717 Seed: 5982\n",
      "Testing: 0.6495488410484297 Training: 0.6443063871240231 Seed: 5986\n",
      "Testing: 0.6465921494058273 Training: 0.6450122283370274 Seed: 5988\n",
      "Testing: 0.6464454810395637 Training: 0.6450297752011618 Seed: 5990\n",
      "Testing: 0.6512262767337177 Training: 0.6438958042802667 Seed: 5991\n",
      "Testing: 0.6498258954874072 Training: 0.6442685913382462 Seed: 5992\n",
      "Testing: 0.6587237247961328 Training: 0.641878238370624 Seed: 5996\n",
      "Testing: 0.6516563207622638 Training: 0.6437578485605884 Seed: 5997\n",
      "Testing: 0.6455438462936964 Training: 0.645143912327982 Seed: 5998\n",
      "Testing: 0.652350340532967 Training: 0.6436031092173391 Seed: 6000\n",
      "Testing: 0.6473740367420958 Training: 0.6448177293633197 Seed: 6003\n",
      "Testing: 0.6619712584776687 Training: 0.6413735686117157 Seed: 6004\n",
      "Testing: 0.6505702999612608 Training: 0.6440643439007401 Seed: 6006\n",
      "Testing: 0.6465111666288864 Training: 0.6450718443659604 Seed: 6007\n",
      "Testing: 0.6544484281345708 Training: 0.6428648882633452 Seed: 6008\n",
      "Testing: 0.6455330389895868 Training: 0.6453668720588843 Seed: 6010\n",
      "Testing: 0.647900130987021 Training: 0.644493399253216 Seed: 6012\n",
      "Testing: 0.6573044138264025 Training: 0.6424000978998836 Seed: 6013\n",
      "Testing: 0.6485161731181147 Training: 0.6445971474844682 Seed: 6014\n",
      "Testing: 0.6541499046783057 Training: 0.6432414423657671 Seed: 6015\n",
      "Testing: 0.6458970758363257 Training: 0.6452802063179586 Seed: 6018\n",
      "Testing: 0.6546149998389491 Training: 0.643104760812955 Seed: 6020\n",
      "Testing: 0.650013532648878 Training: 0.6442695475180436 Seed: 6021\n",
      "Testing: 0.6493117933553757 Training: 0.6443642030762314 Seed: 6022\n",
      "Testing: 0.6537358730312156 Training: 0.643216049511449 Seed: 6023\n",
      "Testing: 0.6474631721535763 Training: 0.6445473564985004 Seed: 6025\n",
      "Testing: 0.6494949115010475 Training: 0.6443572880416051 Seed: 6027\n",
      "Testing: 0.6561699630556334 Training: 0.6426586152789258 Seed: 6028\n",
      "Testing: 0.6542294343031605 Training: 0.6430976589447193 Seed: 6029\n",
      "Testing: 0.6659115745005953 Training: 0.6399976420492521 Seed: 6030\n",
      "Testing: 0.6481948275117887 Training: 0.6447376261828617 Seed: 6032\n",
      "Testing: 0.6484901661126232 Training: 0.6445526724091334 Seed: 6040\n",
      "Testing: 0.659204681816472 Training: 0.6419450187948349 Seed: 6041\n",
      "Testing: 0.6503756971308801 Training: 0.6440132481201088 Seed: 6042\n",
      "Testing: 0.6620932298056859 Training: 0.6409099860945255 Seed: 6044\n",
      "Testing: 0.6461298089839602 Training: 0.645180011663617 Seed: 6047\n",
      "Testing: 0.6460695352385901 Training: 0.6452004086241324 Seed: 6049\n",
      "Testing: 0.6549844467486835 Training: 0.6430212140249556 Seed: 6050\n",
      "Testing: 0.6565657767012972 Training: 0.6425473961525904 Seed: 6056\n",
      "Testing: 0.6522245239760238 Training: 0.6437301728235503 Seed: 6057\n",
      "Testing: 0.6467991932215211 Training: 0.6449555063943508 Seed: 6059\n",
      "Testing: 0.6492401259276525 Training: 0.6444137484826724 Seed: 6060\n",
      "Testing: 0.6467189505024866 Training: 0.6448878296252696 Seed: 6061\n",
      "Testing: 0.6517116572055496 Training: 0.6437988667197363 Seed: 6062\n",
      "Testing: 0.6586586853990404 Training: 0.6419300695986344 Seed: 6067\n",
      "Testing: 0.6536857033960621 Training: 0.6432363438455928 Seed: 6073\n",
      "Testing: 0.6521804527176293 Training: 0.6436202374475605 Seed: 6076\n",
      "Testing: 0.6631509728642058 Training: 0.6408122747458846 Seed: 6077\n",
      "Testing: 0.6583105645590966 Training: 0.6423224471525777 Seed: 6079\n",
      "Testing: 0.6535010279730478 Training: 0.643304443168769 Seed: 6083\n",
      "Testing: 0.6629319711249988 Training: 0.6409265468366492 Seed: 6086\n",
      "Testing: 0.645448303275345 Training: 0.6453551003515694 Seed: 6088\n",
      "Testing: 0.6507432473523906 Training: 0.6438701023883981 Seed: 6089\n",
      "Testing: 0.6518294472103826 Training: 0.643619726189899 Seed: 6096\n",
      "Testing: 0.6778887776430994 Training: 0.6375614138960153 Seed: 6097\n",
      "Testing: 0.651678600855566 Training: 0.6438649969417448 Seed: 6098\n",
      "Testing: 0.6464670885902137 Training: 0.6451523947781327 Seed: 6105\n",
      "Testing: 0.6476637024832556 Training: 0.6448239130844913 Seed: 6106\n",
      "Testing: 0.6490013359481352 Training: 0.6444573151839383 Seed: 6107\n",
      "Testing: 0.6578742480346511 Training: 0.642289684414097 Seed: 6113\n",
      "Testing: 0.6496179084385786 Training: 0.6443217776902573 Seed: 6114\n",
      "Testing: 0.6475025921145237 Training: 0.6448036970706319 Seed: 6115\n",
      "Testing: 0.6590261545202495 Training: 0.6418705184737146 Seed: 6117\n",
      "Testing: 0.6500598130382713 Training: 0.6441420136863987 Seed: 6118\n",
      "Testing: 0.6702749127147158 Training: 0.6391593512863977 Seed: 6120\n",
      "Testing: 0.6559542788253245 Training: 0.6428767636584692 Seed: 6121\n",
      "Testing: 0.6559855057834594 Training: 0.6426712914292636 Seed: 6127\n",
      "Testing: 0.6500797111540264 Training: 0.6441188339071424 Seed: 6128\n",
      "Testing: 0.6558062948430804 Training: 0.6427200358750929 Seed: 6130\n",
      "Testing: 0.6503173444096626 Training: 0.6442241382262164 Seed: 6132\n",
      "Testing: 0.6462717070757061 Training: 0.6451324759758486 Seed: 6133\n",
      "Testing: 0.6505015681120339 Training: 0.6441636663657226 Seed: 6135\n",
      "Testing: 0.6506947680937083 Training: 0.6440663772263636 Seed: 6139\n",
      "Testing: 0.647751270304527 Training: 0.644633201542112 Seed: 6143\n",
      "Testing: 0.648728980696231 Training: 0.6443807041469605 Seed: 6144\n",
      "Testing: 0.6602117872336403 Training: 0.6415741620475804 Seed: 6146\n",
      "Testing: 0.653017031637353 Training: 0.6431401889508718 Seed: 6149\n",
      "Testing: 0.6690117057780499 Training: 0.6395430738719703 Seed: 6151\n",
      "Testing: 0.648714374527288 Training: 0.6445699533949028 Seed: 6152\n",
      "Testing: 0.6595503435047433 Training: 0.6419215127486387 Seed: 6157\n",
      "Testing: 0.6461730704154667 Training: 0.6451754705296939 Seed: 6160\n",
      "Testing: 0.6633739052071204 Training: 0.6407358872339408 Seed: 6161\n",
      "Testing: 0.6580479431056547 Training: 0.6420236080707871 Seed: 6162\n",
      "Testing: 0.6464445697478178 Training: 0.6449965445287085 Seed: 6163\n",
      "Testing: 0.6580398772047035 Training: 0.6421819267128339 Seed: 6164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6558424944793309 Training: 0.6427019784164985 Seed: 6165\n",
      "Testing: 0.6559360013929356 Training: 0.6426872862874429 Seed: 6166\n",
      "Testing: 0.6564348087888174 Training: 0.6427474899930467 Seed: 6169\n",
      "Testing: 0.6597312498516378 Training: 0.6416091210882948 Seed: 6171\n",
      "Testing: 0.6506549080413476 Training: 0.6441258163576816 Seed: 6173\n",
      "Testing: 0.6463308249257377 Training: 0.6451915667282427 Seed: 6175\n",
      "Testing: 0.6539867962068744 Training: 0.643181638591383 Seed: 6176\n",
      "Testing: 0.6539608610811293 Training: 0.6430848507931177 Seed: 6177\n",
      "Testing: 0.658601177157221 Training: 0.6419522405248728 Seed: 6181\n",
      "Testing: 0.6508906770988356 Training: 0.6439424394773652 Seed: 6183\n",
      "Testing: 0.6486914515814544 Training: 0.6444490638417892 Seed: 6184\n",
      "Testing: 0.6570153337706995 Training: 0.6425868519249005 Seed: 6188\n",
      "Testing: 0.6599161312783267 Training: 0.6417755267085693 Seed: 6189\n",
      "Testing: 0.663578701633455 Training: 0.6406059451300055 Seed: 6191\n",
      "Testing: 0.6538026244013788 Training: 0.6432516680770501 Seed: 6194\n",
      "Testing: 0.6580970288019576 Training: 0.6422528452636643 Seed: 6198\n",
      "Testing: 0.6457908796596585 Training: 0.6452437352030325 Seed: 6199\n",
      "Testing: 0.6507102387817242 Training: 0.6440353369187686 Seed: 6201\n",
      "Testing: 0.6572551147698198 Training: 0.6423689849516363 Seed: 6204\n",
      "Testing: 0.6487178169264673 Training: 0.6444963858352097 Seed: 6205\n",
      "Testing: 0.650087827646477 Training: 0.6441157849399022 Seed: 6211\n",
      "Testing: 0.6460512215224744 Training: 0.6452040265710901 Seed: 6212\n",
      "Testing: 0.6549238797470118 Training: 0.6430477290297917 Seed: 6213\n",
      "Testing: 0.6531233175191816 Training: 0.6434792861208712 Seed: 6214\n",
      "Testing: 0.6527871080126808 Training: 0.6436112246197846 Seed: 6217\n",
      "Testing: 0.6486133312367516 Training: 0.6444654573178434 Seed: 6219\n",
      "Testing: 0.6479461355718199 Training: 0.644654475107003 Seed: 6224\n",
      "Testing: 0.6568815700431043 Training: 0.6424964930975219 Seed: 6225\n",
      "Testing: 0.6492688278025976 Training: 0.644262671938897 Seed: 6227\n",
      "Testing: 0.6584631661806584 Training: 0.6419075808539252 Seed: 6228\n",
      "Testing: 0.6548719450763225 Training: 0.642969831074597 Seed: 6229\n",
      "Testing: 0.6523469432776844 Training: 0.6436747267701639 Seed: 6230\n",
      "Testing: 0.6544752841576534 Training: 0.643291537448506 Seed: 6231\n",
      "Testing: 0.6513418774918154 Training: 0.6438935522859665 Seed: 6237\n",
      "Testing: 0.6544574967904127 Training: 0.6431789039684707 Seed: 6238\n",
      "Testing: 0.6454764754650053 Training: 0.6453846729788116 Seed: 6239\n",
      "Testing: 0.6547568361376928 Training: 0.6430225322229031 Seed: 6240\n",
      "Testing: 0.6650223792339571 Training: 0.6400145639986992 Seed: 6243\n",
      "Testing: 0.6618314931692444 Training: 0.6413291333009673 Seed: 6245\n",
      "Testing: 0.6490477480586966 Training: 0.6444508319101444 Seed: 6248\n",
      "Testing: 0.6611368300540421 Training: 0.6412652766528897 Seed: 6249\n",
      "Testing: 0.6531583259471878 Training: 0.6435076257986151 Seed: 6254\n",
      "Testing: 0.6582164989532795 Training: 0.6421340836175188 Seed: 6255\n",
      "Testing: 0.6590449837095307 Training: 0.6419851814137668 Seed: 6262\n",
      "Testing: 0.6569476724613593 Training: 0.6425313642993471 Seed: 6263\n",
      "Testing: 0.6482488883229145 Training: 0.6447122839082462 Seed: 6265\n",
      "Testing: 0.6637174657777581 Training: 0.6407688651664808 Seed: 6266\n",
      "Testing: 0.6555343536232348 Training: 0.6429068600467435 Seed: 6267\n",
      "Testing: 0.6502868406580413 Training: 0.6441389180399737 Seed: 6268\n",
      "Testing: 0.6573075982691141 Training: 0.6425512330699129 Seed: 6270\n",
      "Testing: 0.6581957470020727 Training: 0.6419643290806043 Seed: 6271\n",
      "Testing: 0.6524461193413168 Training: 0.6436419024762988 Seed: 6272\n",
      "Testing: 0.6501719299497071 Training: 0.644122301843673 Seed: 6276\n",
      "Testing: 0.6512192576952025 Training: 0.6439364188442542 Seed: 6278\n",
      "Testing: 0.666521961221353 Training: 0.6398934000730625 Seed: 6281\n",
      "Testing: 0.6485020122300628 Training: 0.6445216392256614 Seed: 6282\n",
      "Testing: 0.6515871478598103 Training: 0.6438270412424323 Seed: 6283\n",
      "Testing: 0.6517204560834557 Training: 0.6438347862481227 Seed: 6285\n",
      "Testing: 0.6501708946384334 Training: 0.6440418890470354 Seed: 6291\n",
      "Testing: 0.6484808148102251 Training: 0.644475618238675 Seed: 6294\n",
      "Testing: 0.6601615640880437 Training: 0.6416246891189519 Seed: 6295\n",
      "Testing: 0.6519961892551918 Training: 0.6434932929867323 Seed: 6297\n",
      "Testing: 0.6526347344018929 Training: 0.6435271785094568 Seed: 6298\n",
      "Testing: 0.6504606171507036 Training: 0.6440314926306209 Seed: 6309\n",
      "Testing: 0.6508819796715387 Training: 0.6439486831507666 Seed: 6310\n",
      "Testing: 0.6587116163604796 Training: 0.6419824139506962 Seed: 6311\n",
      "Testing: 0.6584168720703101 Training: 0.64207313279163 Seed: 6317\n",
      "Testing: 0.6522889464025409 Training: 0.6435508994885314 Seed: 6320\n",
      "Testing: 0.6521768887389192 Training: 0.6436039768343387 Seed: 6322\n",
      "Testing: 0.6514515028815466 Training: 0.6438689362134307 Seed: 6324\n",
      "Testing: 0.6462617127207557 Training: 0.6451384161030397 Seed: 6329\n",
      "Testing: 0.6517965341705165 Training: 0.6437677346519257 Seed: 6330\n",
      "Testing: 0.6559143917621032 Training: 0.6427244619360191 Seed: 6331\n",
      "Testing: 0.6600185513057397 Training: 0.6417540883707369 Seed: 6332\n",
      "Testing: 0.6469140820132171 Training: 0.6448791574200645 Seed: 6333\n",
      "Testing: 0.6635165594734096 Training: 0.6409136614024549 Seed: 6336\n",
      "Testing: 0.6516267484946244 Training: 0.6438061067078303 Seed: 6339\n",
      "Testing: 0.6534422750017451 Training: 0.643396574815771 Seed: 6341\n",
      "Testing: 0.6579514141396592 Training: 0.6419648713264391 Seed: 6342\n",
      "Testing: 0.6548162679497248 Training: 0.6430374794764866 Seed: 6346\n",
      "Testing: 0.6488813118282786 Training: 0.6443582360099624 Seed: 6348\n",
      "Testing: 0.6470399683611033 Training: 0.644916784408224 Seed: 6349\n",
      "Testing: 0.6474010074664145 Training: 0.6448077141143531 Seed: 6350\n",
      "Testing: 0.6521526599910366 Training: 0.643681976223205 Seed: 6351\n",
      "Testing: 0.6471737013887786 Training: 0.6448478100367236 Seed: 6355\n",
      "Testing: 0.651250803119366 Training: 0.6438695561933689 Seed: 6356\n",
      "Testing: 0.6707294727006343 Training: 0.6390212106759283 Seed: 6358\n",
      "Testing: 0.6454446517130954 Training: 0.6453904823913064 Seed: 6359\n",
      "Testing: 0.6476994437739314 Training: 0.6445837327259865 Seed: 6360\n",
      "Testing: 0.6485979876605029 Training: 0.6444565760945099 Seed: 6361\n",
      "Testing: 0.6515049879692763 Training: 0.6439150592627821 Seed: 6362\n",
      "Testing: 0.6575948118148469 Training: 0.642108749528816 Seed: 6364\n",
      "Testing: 0.6625893244542173 Training: 0.6410997765212609 Seed: 6365\n",
      "Testing: 0.645721988473012 Training: 0.6453036738240827 Seed: 6366\n",
      "Testing: 0.6605802324247065 Training: 0.6413734160480685 Seed: 6372\n",
      "Testing: 0.6503111387218415 Training: 0.6440238999703591 Seed: 6373\n",
      "Testing: 0.6573478029603937 Training: 0.642268006663557 Seed: 6375\n",
      "Testing: 0.6558672373992405 Training: 0.64229372957896 Seed: 6379\n",
      "Testing: 0.6597510000360762 Training: 0.6419000284366938 Seed: 6384\n",
      "Testing: 0.6459200192367417 Training: 0.6452762181662028 Seed: 6385\n",
      "Testing: 0.6634929416498009 Training: 0.6407156318320952 Seed: 6386\n",
      "Testing: 0.6453917350330475 Training: 0.645370025022955 Seed: 6388\n",
      "Testing: 0.6465365620960019 Training: 0.6451010968869965 Seed: 6393\n",
      "Testing: 0.6506673484314411 Training: 0.643975225494959 Seed: 6394\n",
      "Testing: 0.648395591374834 Training: 0.6441603107904578 Seed: 6395\n",
      "Testing: 0.6494158618486048 Training: 0.6443473714185833 Seed: 6397\n",
      "Testing: 0.6616319255861896 Training: 0.6411948062474755 Seed: 6406\n",
      "Testing: 0.6538444860518561 Training: 0.6432951314278085 Seed: 6410\n",
      "Testing: 0.6568432085006535 Training: 0.6426241269126213 Seed: 6413\n",
      "Testing: 0.6575165109787151 Training: 0.6422482301832007 Seed: 6418\n",
      "Testing: 0.6500917670915214 Training: 0.6441465043085625 Seed: 6419\n",
      "Testing: 0.6620524391795998 Training: 0.6410406060934641 Seed: 6420\n",
      "Testing: 0.6537678362942972 Training: 0.643275856466474 Seed: 6422\n",
      "Testing: 0.6541089051113336 Training: 0.6430531979887475 Seed: 6426\n",
      "Testing: 0.6505530687736412 Training: 0.6440878872467501 Seed: 6427\n",
      "Testing: 0.6457866880727844 Training: 0.6452313259899248 Seed: 6428\n",
      "Testing: 0.6660229752401194 Training: 0.6401798539388549 Seed: 6430\n",
      "Testing: 0.6493651249101775 Training: 0.6440252310567196 Seed: 6432\n",
      "Testing: 0.6541656296732192 Training: 0.6432034470289082 Seed: 6433\n",
      "Testing: 0.658725893517506 Training: 0.6419209372841637 Seed: 6434\n",
      "Testing: 0.650996126297949 Training: 0.6439025789767888 Seed: 6435\n",
      "Testing: 0.6478028342548096 Training: 0.6448165423991643 Seed: 6437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6562916632406448 Training: 0.642652133511124 Seed: 6439\n",
      "Testing: 0.658286733488807 Training: 0.6422539183152816 Seed: 6444\n",
      "Testing: 0.6518894285541429 Training: 0.6438197900753206 Seed: 6445\n",
      "Testing: 0.6485438182792107 Training: 0.6446112005398427 Seed: 6446\n",
      "Testing: 0.6508218996685251 Training: 0.6438833474206039 Seed: 6448\n",
      "Testing: 0.6543016009588567 Training: 0.6431617493818474 Seed: 6451\n",
      "Testing: 0.6535593400230025 Training: 0.6433318184723896 Seed: 6453\n",
      "Testing: 0.6603454650200098 Training: 0.6414304711089125 Seed: 6454\n",
      "Testing: 0.6469862531767456 Training: 0.6448613291838555 Seed: 6459\n",
      "Testing: 0.6517403787413689 Training: 0.6431630481431831 Seed: 6461\n",
      "Testing: 0.6549546182036478 Training: 0.6427879379708266 Seed: 6462\n",
      "Testing: 0.6492191009379248 Training: 0.6444524006385579 Seed: 6466\n",
      "Testing: 0.6692529718063158 Training: 0.6392952312174662 Seed: 6467\n",
      "Testing: 0.649550786032734 Training: 0.6443240485838051 Seed: 6468\n",
      "Testing: 0.6650378357487727 Training: 0.6403541586203838 Seed: 6476\n",
      "Testing: 0.6660795969917132 Training: 0.6400712929708192 Seed: 6479\n",
      "Testing: 0.6704980807218515 Training: 0.639188157118966 Seed: 6480\n",
      "Testing: 0.6479410352695772 Training: 0.6446344509854243 Seed: 6482\n",
      "Testing: 0.6547197914374854 Training: 0.6431225301061567 Seed: 6484\n",
      "Testing: 0.6541521009171478 Training: 0.6431922415621526 Seed: 6485\n",
      "Testing: 0.652679853361101 Training: 0.643534929951729 Seed: 6486\n",
      "Testing: 0.6514735804054979 Training: 0.6437973891959788 Seed: 6487\n",
      "Testing: 0.6588600454170577 Training: 0.642062061959295 Seed: 6490\n",
      "Testing: 0.6629300031395366 Training: 0.6407079663734848 Seed: 6492\n",
      "Testing: 0.665167675066963 Training: 0.6404194365454268 Seed: 6495\n",
      "Testing: 0.65042317693584 Training: 0.6440849460594242 Seed: 6496\n",
      "Testing: 0.6548837434131752 Training: 0.642938907457019 Seed: 6499\n",
      "Testing: 0.6554677734247016 Training: 0.6428583787080985 Seed: 6500\n",
      "Testing: 0.6464990827362341 Training: 0.6449683606711549 Seed: 6501\n",
      "Testing: 0.6578267049155432 Training: 0.6423780748967413 Seed: 6503\n",
      "Testing: 0.6496150504700196 Training: 0.6443163772649338 Seed: 6505\n",
      "Testing: 0.6484014879863913 Training: 0.6446180448769214 Seed: 6506\n",
      "Testing: 0.6569847363162311 Training: 0.6424319224595398 Seed: 6508\n",
      "Testing: 0.6645572472106726 Training: 0.6402714470487711 Seed: 6509\n",
      "Testing: 0.6559353926852153 Training: 0.6426203000497995 Seed: 6511\n",
      "Testing: 0.6558256479058241 Training: 0.6427225535018805 Seed: 6516\n",
      "Testing: 0.6457379781636831 Training: 0.6452952309095659 Seed: 6517\n",
      "Testing: 0.649248527573045 Training: 0.6443733427766363 Seed: 6518\n",
      "Testing: 0.6554811961566047 Training: 0.6428207657956327 Seed: 6522\n",
      "Testing: 0.6642524844699825 Training: 0.6407258739452215 Seed: 6523\n",
      "Testing: 0.6504268382570181 Training: 0.6440649279478916 Seed: 6526\n",
      "Testing: 0.6605731322024405 Training: 0.641475261195926 Seed: 6527\n",
      "Testing: 0.6639072896985454 Training: 0.6406732863929107 Seed: 6529\n",
      "Testing: 0.6514298655310817 Training: 0.6438463686873234 Seed: 6534\n",
      "Testing: 0.6480841510929389 Training: 0.6447198690075187 Seed: 6535\n",
      "Testing: 0.6461100187593289 Training: 0.6451421024868342 Seed: 6536\n",
      "Testing: 0.6486057962598228 Training: 0.6440138797739358 Seed: 6540\n",
      "Testing: 0.6591543961817702 Training: 0.6419720460134146 Seed: 6543\n",
      "Testing: 0.6575669452095816 Training: 0.6424025043595722 Seed: 6547\n",
      "Testing: 0.6519194572042133 Training: 0.643555788590384 Seed: 6548\n",
      "Testing: 0.645525707194387 Training: 0.6449979819363219 Seed: 6551\n",
      "Testing: 0.6518369032830373 Training: 0.6436823143994337 Seed: 6553\n",
      "Testing: 0.6486955173671475 Training: 0.6445803900819969 Seed: 6554\n",
      "Testing: 0.651474608589424 Training: 0.6437936444481642 Seed: 6555\n",
      "Testing: 0.653371636600077 Training: 0.6433995306389295 Seed: 6556\n",
      "Testing: 0.6578075083075996 Training: 0.6422439073307062 Seed: 6560\n",
      "Testing: 0.6536296028610256 Training: 0.6433925363788484 Seed: 6564\n",
      "Testing: 0.648415009139024 Training: 0.6445135057256007 Seed: 6568\n",
      "Testing: 0.6459886450685572 Training: 0.6451292580531225 Seed: 6573\n",
      "Testing: 0.6469922877455218 Training: 0.644201994471484 Seed: 6587\n",
      "Testing: 0.6604885924619278 Training: 0.6415939224284768 Seed: 6588\n",
      "Testing: 0.649026158433148 Training: 0.6445540918624963 Seed: 6589\n",
      "Testing: 0.6542384569648397 Training: 0.6431906794518223 Seed: 6593\n",
      "Testing: 0.6555702668999803 Training: 0.6427988501892512 Seed: 6594\n",
      "Testing: 0.6622564495032545 Training: 0.6411516818864471 Seed: 6595\n",
      "Testing: 0.657559684443159 Training: 0.6423606753193032 Seed: 6599\n",
      "Testing: 0.6510909701845791 Training: 0.6440807979069373 Seed: 6601\n",
      "Testing: 0.6488764866834493 Training: 0.6445519230829034 Seed: 6603\n",
      "Testing: 0.6636462292709749 Training: 0.6407584371453332 Seed: 6604\n",
      "Testing: 0.6550507616394852 Training: 0.6430495742921344 Seed: 6605\n",
      "Testing: 0.6584675825817149 Training: 0.6421782739750951 Seed: 6606\n",
      "Testing: 0.6505652387538716 Training: 0.6441122074461471 Seed: 6608\n",
      "Testing: 0.6455719675136768 Training: 0.645375627600903 Seed: 6610\n",
      "Testing: 0.6682504686539323 Training: 0.6398600304396898 Seed: 6611\n",
      "Testing: 0.6583111050419332 Training: 0.6421539870999482 Seed: 6612\n",
      "Testing: 0.6536126572965458 Training: 0.6432419629438039 Seed: 6614\n",
      "Testing: 0.6474294355687193 Training: 0.644789269111934 Seed: 6615\n",
      "Testing: 0.6544496925394523 Training: 0.6430773647857893 Seed: 6619\n",
      "Testing: 0.6530076115669653 Training: 0.6432510887085786 Seed: 6621\n",
      "Testing: 0.6537738444546322 Training: 0.643275298420911 Seed: 6622\n",
      "Testing: 0.6659451251032158 Training: 0.6400986139350169 Seed: 6626\n",
      "Testing: 0.6505399549435895 Training: 0.6441374265280089 Seed: 6629\n",
      "Testing: 0.6547764939267243 Training: 0.6428577164484501 Seed: 6633\n",
      "Testing: 0.6515990064635746 Training: 0.6437996252303707 Seed: 6635\n",
      "Testing: 0.6579431750225291 Training: 0.642206649635576 Seed: 6638\n",
      "Testing: 0.6495288743752068 Training: 0.6442459774221765 Seed: 6642\n",
      "Testing: 0.6495949051196479 Training: 0.6443286341589016 Seed: 6643\n",
      "Testing: 0.6462210373229988 Training: 0.6451947282039993 Seed: 6644\n",
      "Testing: 0.6471352171412283 Training: 0.6449184245619842 Seed: 6647\n",
      "Testing: 0.6471054324111187 Training: 0.6449998769673315 Seed: 6652\n",
      "Testing: 0.6609106353690932 Training: 0.641296023865689 Seed: 6653\n",
      "Testing: 0.6547056316413072 Training: 0.6427251119056527 Seed: 6655\n",
      "Testing: 0.660421404240654 Training: 0.6417061223172316 Seed: 6657\n",
      "Testing: 0.647345785314773 Training: 0.6448585413321979 Seed: 6658\n",
      "Testing: 0.6455328715083998 Training: 0.645322415979838 Seed: 6661\n",
      "Testing: 0.6521787259115412 Training: 0.6435357204740921 Seed: 6664\n",
      "Testing: 0.6587807989813976 Training: 0.6421431545776393 Seed: 6665\n",
      "Testing: 0.6490020574672295 Training: 0.6445137742261884 Seed: 6667\n",
      "Testing: 0.6513418262398287 Training: 0.6438998478598267 Seed: 6668\n",
      "Testing: 0.6482885829831875 Training: 0.6444826492601454 Seed: 6671\n",
      "Testing: 0.6584139343913572 Training: 0.6419555181423493 Seed: 6675\n",
      "Testing: 0.6492125187706222 Training: 0.644418034273129 Seed: 6681\n",
      "Testing: 0.6600341627919463 Training: 0.6417082934238709 Seed: 6684\n",
      "Testing: 0.655813531864889 Training: 0.6427634128204496 Seed: 6685\n",
      "Testing: 0.6469874280953535 Training: 0.6449183500585385 Seed: 6687\n",
      "Testing: 0.6459291544481435 Training: 0.6451923366470171 Seed: 6690\n",
      "Testing: 0.6539981952690239 Training: 0.6432273709605034 Seed: 6691\n",
      "Testing: 0.6556422005754988 Training: 0.6428601944300263 Seed: 6692\n",
      "Testing: 0.6479643687374905 Training: 0.6447633660319485 Seed: 6693\n",
      "Testing: 0.6506927484519356 Training: 0.6441145491813306 Seed: 6694\n",
      "Testing: 0.6457657518330152 Training: 0.6446790985426629 Seed: 6696\n",
      "Testing: 0.6628387096804766 Training: 0.6408817435715732 Seed: 6698\n",
      "Testing: 0.6489312402110408 Training: 0.6444414084171857 Seed: 6699\n",
      "Testing: 0.6470537222676317 Training: 0.6449433915060803 Seed: 6701\n",
      "Testing: 0.6489556965529687 Training: 0.6444036360830844 Seed: 6702\n",
      "Testing: 0.6550917406891626 Training: 0.6429684499976076 Seed: 6703\n",
      "Testing: 0.6630496699946979 Training: 0.6410060408010837 Seed: 6707\n",
      "Testing: 0.6493947331201129 Training: 0.6443927589210375 Seed: 6710\n",
      "Testing: 0.6491239859787942 Training: 0.6444909247097892 Seed: 6711\n",
      "Testing: 0.6474896351948253 Training: 0.6447524769200919 Seed: 6715\n",
      "Testing: 0.6502694191639143 Training: 0.6440851689748905 Seed: 6716\n",
      "Testing: 0.6559211840657013 Training: 0.6426555165685134 Seed: 6717\n",
      "Testing: 0.6599216269488718 Training: 0.6417490112101133 Seed: 6718\n",
      "Testing: 0.6457614670200619 Training: 0.64529079130263 Seed: 6723\n",
      "Testing: 0.6477493294612744 Training: 0.6448443506255306 Seed: 6724\n",
      "Testing: 0.6450180821605679 Training: 0.6449492122505205 Seed: 6725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6454145340075977 Training: 0.6452764151510066 Seed: 6729\n",
      "Testing: 0.6603680629590347 Training: 0.6418709445829109 Seed: 6730\n",
      "Testing: 0.6468887131100778 Training: 0.6450286148912495 Seed: 6731\n",
      "Testing: 0.646655556763323 Training: 0.6449086334692337 Seed: 6733\n",
      "Testing: 0.6640803497985807 Training: 0.6403752927829777 Seed: 6740\n",
      "Testing: 0.6466219251366366 Training: 0.6450536337787012 Seed: 6742\n",
      "Testing: 0.6467185470203383 Training: 0.6447809699145267 Seed: 6745\n",
      "Testing: 0.6600999725585619 Training: 0.6417243497722674 Seed: 6747\n",
      "Testing: 0.6505681064421822 Training: 0.6441238174612948 Seed: 6748\n",
      "Testing: 0.6528181293684598 Training: 0.6435173146529856 Seed: 6749\n",
      "Testing: 0.6642144358595297 Training: 0.640679752145284 Seed: 6750\n",
      "Testing: 0.6565938901824243 Training: 0.6424073180546082 Seed: 6751\n",
      "Testing: 0.6700446974283463 Training: 0.6392537094556372 Seed: 6752\n",
      "Testing: 0.6517053073921215 Training: 0.6437964000231786 Seed: 6753\n",
      "Testing: 0.6488854736537264 Training: 0.6444933610930624 Seed: 6756\n",
      "Testing: 0.657941555672408 Training: 0.6422608953636209 Seed: 6757\n",
      "Testing: 0.6529639857187103 Training: 0.6435281441421884 Seed: 6760\n",
      "Testing: 0.6494173077757349 Training: 0.644342336586945 Seed: 6761\n",
      "Testing: 0.6553824334235727 Training: 0.6428592877167769 Seed: 6764\n",
      "Testing: 0.6607531922000449 Training: 0.6415674785575449 Seed: 6767\n",
      "Testing: 0.6502262754089901 Training: 0.6442153328969406 Seed: 6769\n",
      "Testing: 0.661849032006764 Training: 0.6412380441828996 Seed: 6773\n",
      "Testing: 0.6477504840059516 Training: 0.6448024380191933 Seed: 6774\n",
      "Testing: 0.6500758638496383 Training: 0.6441622656617968 Seed: 6780\n",
      "Testing: 0.6542606323568796 Training: 0.6431924570123648 Seed: 6781\n",
      "Testing: 0.6517232144080067 Training: 0.6438180889831621 Seed: 6784\n",
      "Testing: 0.6463281234283258 Training: 0.6451225409895467 Seed: 6785\n",
      "Testing: 0.657276180328212 Training: 0.6425058067255504 Seed: 6789\n",
      "Testing: 0.6480112279870489 Training: 0.644758974784772 Seed: 6790\n",
      "Testing: 0.6464533343853381 Training: 0.6448318961244294 Seed: 6791\n",
      "Testing: 0.6534703311166138 Training: 0.6433240558555386 Seed: 6792\n",
      "Testing: 0.646953630617457 Training: 0.6449984277000422 Seed: 6793\n",
      "Testing: 0.6500168041756451 Training: 0.6442411501742542 Seed: 6794\n",
      "Testing: 0.6557106912333187 Training: 0.6428023962010271 Seed: 6795\n",
      "Testing: 0.6494282794957293 Training: 0.6439952884992928 Seed: 6797\n",
      "Testing: 0.6558966183377525 Training: 0.6427605410059112 Seed: 6798\n",
      "Testing: 0.6472885190474995 Training: 0.644959069057526 Seed: 6799\n",
      "Testing: 0.6613099790701177 Training: 0.6413275851268909 Seed: 6800\n",
      "Testing: 0.647786256944042 Training: 0.6447018452516367 Seed: 6801\n",
      "Testing: 0.6508369655022934 Training: 0.6440165759989465 Seed: 6803\n",
      "Testing: 0.6679232592068585 Training: 0.6394741481253982 Seed: 6804\n",
      "Testing: 0.653878045710345 Training: 0.6431260641342966 Seed: 6805\n",
      "Testing: 0.6526851704481853 Training: 0.6436175621637967 Seed: 6807\n",
      "Testing: 0.6573525905431337 Training: 0.6424766365760981 Seed: 6808\n",
      "Testing: 0.6508499459838268 Training: 0.6440135969699724 Seed: 6809\n",
      "Testing: 0.6460788864130464 Training: 0.6452164502041974 Seed: 6810\n",
      "Testing: 0.6653791139083538 Training: 0.6404478164969287 Seed: 6814\n",
      "Testing: 0.649817407772598 Training: 0.6443366943912686 Seed: 6815\n",
      "Testing: 0.6465495368443474 Training: 0.6450655451446803 Seed: 6817\n",
      "Testing: 0.6488505112867423 Training: 0.6444421030302739 Seed: 6819\n",
      "Testing: 0.6453768239563744 Training: 0.6452748506195941 Seed: 6820\n",
      "Testing: 0.6480721668684296 Training: 0.6447489698060789 Seed: 6821\n",
      "Testing: 0.6578432137828818 Training: 0.6423083596949972 Seed: 6823\n",
      "Testing: 0.6508030259931309 Training: 0.6441047382821854 Seed: 6824\n",
      "Testing: 0.6527133198525047 Training: 0.6436011961423019 Seed: 6825\n",
      "Testing: 0.6633795574836165 Training: 0.6407575338100899 Seed: 6826\n",
      "Testing: 0.6497630783032622 Training: 0.6443498485919128 Seed: 6828\n",
      "Testing: 0.6585837775454692 Training: 0.6420762038584198 Seed: 6830\n",
      "Testing: 0.649868480707892 Training: 0.6442623667546199 Seed: 6831\n",
      "Testing: 0.6600744070480669 Training: 0.6415546682237385 Seed: 6833\n",
      "Testing: 0.6646084888989504 Training: 0.6404076496075084 Seed: 6835\n",
      "Testing: 0.6475313197490384 Training: 0.6448846100839609 Seed: 6836\n",
      "Testing: 0.6547935464220832 Training: 0.643091512704188 Seed: 6837\n",
      "Testing: 0.6664677193146125 Training: 0.6397348890324359 Seed: 6839\n",
      "Testing: 0.6672664342974238 Training: 0.6398048208650606 Seed: 6842\n",
      "Testing: 0.6586641740228497 Training: 0.6419992639533214 Seed: 6843\n",
      "Testing: 0.6461288904702456 Training: 0.6451537196862311 Seed: 6848\n",
      "Testing: 0.6595126446195441 Training: 0.6417044896293476 Seed: 6849\n",
      "Testing: 0.6534631643363675 Training: 0.6431956920915514 Seed: 6851\n",
      "Testing: 0.6549235692617125 Training: 0.6429856233747567 Seed: 6852\n",
      "Testing: 0.6514995410964117 Training: 0.6438074324936187 Seed: 6854\n",
      "Testing: 0.6476572789851249 Training: 0.6448454620275456 Seed: 6856\n",
      "Testing: 0.6466396823977414 Training: 0.6449407583894252 Seed: 6858\n",
      "Testing: 0.6473213441608336 Training: 0.6449165966284864 Seed: 6860\n",
      "Testing: 0.645742249418539 Training: 0.6452290192162999 Seed: 6862\n",
      "Testing: 0.6552394094383285 Training: 0.6430197237331732 Seed: 6864\n",
      "Testing: 0.6467783278595189 Training: 0.6450838344786844 Seed: 6865\n",
      "Testing: 0.659723803619182 Training: 0.6414993856875458 Seed: 6866\n",
      "Testing: 0.6496099584569087 Training: 0.6442307975652252 Seed: 6868\n",
      "Testing: 0.6634452943696065 Training: 0.6408913290553838 Seed: 6869\n",
      "Testing: 0.6507323932702085 Training: 0.643909680984813 Seed: 6874\n",
      "Testing: 0.6583159724600927 Training: 0.6421253899447852 Seed: 6875\n",
      "Testing: 0.6458564446421564 Training: 0.6450935864796432 Seed: 6877\n",
      "Testing: 0.6464757952836837 Training: 0.6449061855687555 Seed: 6880\n",
      "Testing: 0.6496904819362548 Training: 0.6442748406488501 Seed: 6886\n",
      "Testing: 0.6492990572999783 Training: 0.6440788169334959 Seed: 6889\n",
      "Testing: 0.6525614237956792 Training: 0.6436123611646689 Seed: 6890\n",
      "Testing: 0.6477838589445082 Training: 0.6447970220544724 Seed: 6891\n",
      "Testing: 0.657664771370283 Training: 0.6422026224469056 Seed: 6892\n",
      "Testing: 0.6540674780936201 Training: 0.6431801053314103 Seed: 6894\n",
      "Testing: 0.6521052139651108 Training: 0.6436952564393714 Seed: 6895\n",
      "Testing: 0.6565059845987522 Training: 0.6426377829874133 Seed: 6896\n",
      "Testing: 0.6475242638148716 Training: 0.6448059587167894 Seed: 6897\n",
      "Testing: 0.6624841937535147 Training: 0.6412716163079115 Seed: 6898\n",
      "Testing: 0.6587737033992014 Training: 0.6420403302772235 Seed: 6901\n",
      "Testing: 0.6545386770752113 Training: 0.6431669893474126 Seed: 6907\n",
      "Testing: 0.6642940371321265 Training: 0.6407002787277222 Seed: 6911\n",
      "Testing: 0.6457647691358035 Training: 0.6453141042667121 Seed: 6913\n",
      "Testing: 0.6574796644132803 Training: 0.6423375951149768 Seed: 6917\n",
      "Testing: 0.6496872841385077 Training: 0.6443325762821422 Seed: 6925\n",
      "Testing: 0.6475403971350212 Training: 0.644397647097176 Seed: 6926\n",
      "Testing: 0.6524821966386507 Training: 0.6436959359033374 Seed: 6929\n",
      "Testing: 0.6508737185113587 Training: 0.6439670701561684 Seed: 6931\n",
      "Testing: 0.6499122658641605 Training: 0.6442406166043936 Seed: 6938\n",
      "Testing: 0.6463619724024705 Training: 0.6451550869727203 Seed: 6941\n",
      "Testing: 0.647236472691974 Training: 0.6447036288203809 Seed: 6943\n",
      "Testing: 0.6532698296504748 Training: 0.642984477057822 Seed: 6944\n",
      "Testing: 0.6581212040952896 Training: 0.6422603925266177 Seed: 6949\n",
      "Testing: 0.6575674319983641 Training: 0.642426824689631 Seed: 6954\n",
      "Testing: 0.6537757958412099 Training: 0.6433255086815269 Seed: 6955\n",
      "Testing: 0.6509655595254464 Training: 0.6439716767363965 Seed: 6958\n",
      "Testing: 0.6507946206907899 Training: 0.6440277725977173 Seed: 6959\n",
      "Testing: 0.6461734574133522 Training: 0.6450356046583791 Seed: 6960\n",
      "Testing: 0.6595829030952579 Training: 0.6417187304535369 Seed: 6961\n",
      "Testing: 0.645536807901008 Training: 0.645322748934201 Seed: 6962\n",
      "Testing: 0.6524705189621283 Training: 0.643476303283592 Seed: 6965\n",
      "Testing: 0.6460776042794809 Training: 0.6451996104390396 Seed: 6966\n",
      "Testing: 0.6537661789567984 Training: 0.643236381886311 Seed: 6969\n",
      "Testing: 0.6504414556800073 Training: 0.64415680260204 Seed: 6971\n",
      "Testing: 0.64992786884764 Training: 0.6443086609077407 Seed: 6973\n",
      "Testing: 0.6586017565814615 Training: 0.6420631919295565 Seed: 6976\n",
      "Testing: 0.6507305101231946 Training: 0.6440976724307986 Seed: 6977\n",
      "Testing: 0.6529979154144138 Training: 0.6434898983581836 Seed: 6979\n",
      "Testing: 0.6497837525541191 Training: 0.6442966485187654 Seed: 6981\n",
      "Testing: 0.6689176801336607 Training: 0.6395638242353076 Seed: 6987\n",
      "Testing: 0.6623079035954625 Training: 0.6408636331948381 Seed: 6989\n",
      "Testing: 0.6564241316970636 Training: 0.6426878158541622 Seed: 6990\n",
      "Testing: 0.6488185366813639 Training: 0.6444662599479403 Seed: 6991\n",
      "Testing: 0.6608591938312711 Training: 0.6415570779328789 Seed: 6993\n",
      "Testing: 0.6610238287974726 Training: 0.6414345600764646 Seed: 6994\n",
      "Testing: 0.6498473584745785 Training: 0.6442770951540555 Seed: 6996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6518673909268903 Training: 0.6437946456793523 Seed: 6997\n",
      "Testing: 0.6473326008377058 Training: 0.6448664399547558 Seed: 6998\n",
      "Testing: 0.6618690962935667 Training: 0.6410241047093421 Seed: 7000\n",
      "Testing: 0.6461155786474181 Training: 0.645169584191107 Seed: 7002\n",
      "Testing: 0.654067233418232 Training: 0.6431501607744483 Seed: 7004\n",
      "Testing: 0.6616908787431521 Training: 0.6413497770586047 Seed: 7006\n",
      "Testing: 0.6465397078668685 Training: 0.6450731922662778 Seed: 7008\n",
      "Testing: 0.6524111916080249 Training: 0.6436185827963322 Seed: 7012\n",
      "Testing: 0.6473420497713556 Training: 0.6449057256123576 Seed: 7014\n",
      "Testing: 0.6590435762800555 Training: 0.6419515762661422 Seed: 7019\n",
      "Testing: 0.6576055697951025 Training: 0.6423626445523953 Seed: 7024\n",
      "Testing: 0.6476957395623644 Training: 0.6447203432024894 Seed: 7025\n",
      "Testing: 0.6607564477408174 Training: 0.641489539303832 Seed: 7026\n",
      "Testing: 0.652734608502755 Training: 0.6436534506681393 Seed: 7029\n",
      "Testing: 0.646123571123374 Training: 0.6451728212277639 Seed: 7030\n",
      "Testing: 0.6481363353762581 Training: 0.6446460674109434 Seed: 7031\n",
      "Testing: 0.6640161796666715 Training: 0.6403490162153374 Seed: 7036\n",
      "Testing: 0.6555016025108895 Training: 0.6425679999035754 Seed: 7037\n",
      "Testing: 0.6551305163996891 Training: 0.6428900869782146 Seed: 7038\n",
      "Testing: 0.6454478324132054 Training: 0.6454151906434376 Seed: 7039\n",
      "Testing: 0.6527125129992527 Training: 0.6436071592343436 Seed: 7042\n",
      "Testing: 0.6507633464593032 Training: 0.6438666239555688 Seed: 7048\n",
      "Testing: 0.6466003969469529 Training: 0.6449043430153069 Seed: 7049\n",
      "Testing: 0.6484323447257616 Training: 0.64464571737731 Seed: 7050\n",
      "Testing: 0.6501541823506081 Training: 0.644151468508861 Seed: 7051\n",
      "Testing: 0.6694377906335167 Training: 0.6393126861183822 Seed: 7054\n",
      "Testing: 0.6601860882253078 Training: 0.6416119274239594 Seed: 7055\n",
      "Testing: 0.6599650728085021 Training: 0.6417833802189763 Seed: 7056\n",
      "Testing: 0.6759775588297581 Training: 0.6373102926282077 Seed: 7058\n",
      "Testing: 0.6487691490586169 Training: 0.644346354242631 Seed: 7062\n",
      "Testing: 0.6607245602988865 Training: 0.6415257837966335 Seed: 7063\n",
      "Testing: 0.655358549456761 Training: 0.642841324626566 Seed: 7064\n",
      "Testing: 0.6503526927870655 Training: 0.6440884217534463 Seed: 7068\n",
      "Testing: 0.6479563005789457 Training: 0.6441205718070944 Seed: 7072\n",
      "Testing: 0.6510194002953891 Training: 0.6438009592869272 Seed: 7073\n",
      "Testing: 0.6469194201084805 Training: 0.6449594333543205 Seed: 7074\n",
      "Testing: 0.6468637915344109 Training: 0.6449225082093064 Seed: 7075\n",
      "Testing: 0.6474170477993066 Training: 0.6448586008763384 Seed: 7077\n",
      "Testing: 0.6576023162988069 Training: 0.6422070095051369 Seed: 7078\n",
      "Testing: 0.6591333580836454 Training: 0.6420248582722969 Seed: 7079\n",
      "Testing: 0.6502355831687884 Training: 0.6441369311489913 Seed: 7080\n",
      "Testing: 0.6484858021739812 Training: 0.6445998422574353 Seed: 7081\n",
      "Testing: 0.6482808326605147 Training: 0.6446755468802907 Seed: 7082\n",
      "Testing: 0.6545317885386769 Training: 0.643171575476707 Seed: 7083\n",
      "Testing: 0.6477919921914543 Training: 0.6446769842693816 Seed: 7084\n",
      "Testing: 0.6536505016526899 Training: 0.6431721652415826 Seed: 7085\n",
      "Testing: 0.6529572917202253 Training: 0.6434003039808749 Seed: 7095\n",
      "Testing: 0.650880782589167 Training: 0.643705777915232 Seed: 7097\n",
      "Testing: 0.6492434759372911 Training: 0.6443638991326637 Seed: 7102\n",
      "Testing: 0.6532757204867912 Training: 0.6434377133394642 Seed: 7104\n",
      "Testing: 0.6498359186721825 Training: 0.6442372664635958 Seed: 7105\n",
      "Testing: 0.6615755448175187 Training: 0.6414887067122506 Seed: 7106\n",
      "Testing: 0.649257391042331 Training: 0.644168545306393 Seed: 7107\n",
      "Testing: 0.6616396829999884 Training: 0.6412299894005788 Seed: 7108\n",
      "Testing: 0.652697151039935 Training: 0.6435305425676947 Seed: 7109\n",
      "Testing: 0.6565290983221281 Training: 0.6426335180938526 Seed: 7111\n",
      "Testing: 0.656313836673942 Training: 0.6427992467075558 Seed: 7112\n",
      "Testing: 0.6503806571709603 Training: 0.6440699204931473 Seed: 7113\n",
      "Testing: 0.6462395308383465 Training: 0.6448069068491544 Seed: 7116\n",
      "Testing: 0.6565758784125268 Training: 0.6426819305716556 Seed: 7119\n",
      "Testing: 0.6491171037125509 Training: 0.6444761679696451 Seed: 7120\n",
      "Testing: 0.6630557362001662 Training: 0.6409264134922896 Seed: 7121\n",
      "Testing: 0.6511180106176118 Training: 0.6438655883270612 Seed: 7126\n",
      "Testing: 0.6538595175664458 Training: 0.6430162098980872 Seed: 7127\n",
      "Testing: 0.6530191869121909 Training: 0.6434864679296037 Seed: 7129\n",
      "Testing: 0.650804451438802 Training: 0.6439348937683176 Seed: 7131\n",
      "Testing: 0.6640154699720443 Training: 0.640566018161664 Seed: 7134\n",
      "Testing: 0.6461314989503327 Training: 0.6450289988260316 Seed: 7135\n",
      "Testing: 0.6528981025278283 Training: 0.6434974407825591 Seed: 7137\n",
      "Testing: 0.6461018461911059 Training: 0.6451578622180413 Seed: 7139\n",
      "Testing: 0.6470350791287622 Training: 0.6448444527839762 Seed: 7143\n",
      "Testing: 0.6604343952468146 Training: 0.6414339649351821 Seed: 7148\n",
      "Testing: 0.6545527262816513 Training: 0.643073272171159 Seed: 7149\n",
      "Testing: 0.6542947279001077 Training: 0.6431084927391686 Seed: 7150\n",
      "Testing: 0.654673493603589 Training: 0.6430979294231356 Seed: 7151\n",
      "Testing: 0.6508623115195489 Training: 0.6439475493009885 Seed: 7156\n",
      "Testing: 0.6750861973923199 Training: 0.6377858877475063 Seed: 7158\n",
      "Testing: 0.6714409658298279 Training: 0.6389118445024997 Seed: 7162\n",
      "Testing: 0.6633879207557414 Training: 0.6408509276031668 Seed: 7163\n",
      "Testing: 0.6485648160131515 Training: 0.6444101740164917 Seed: 7165\n",
      "Testing: 0.6535286956902757 Training: 0.6432817993213027 Seed: 7169\n",
      "Testing: 0.6567611555591826 Training: 0.6425923776728522 Seed: 7170\n",
      "Testing: 0.6525049279519402 Training: 0.6435628710390989 Seed: 7172\n",
      "Testing: 0.6485098426923742 Training: 0.6445554014881976 Seed: 7175\n",
      "Testing: 0.6680139136435728 Training: 0.6394246632339484 Seed: 7177\n",
      "Testing: 0.6627502593340331 Training: 0.640851451959492 Seed: 7179\n",
      "Testing: 0.6512542084612983 Training: 0.6438998135801601 Seed: 7181\n",
      "Testing: 0.6637415946190712 Training: 0.6408456265666788 Seed: 7182\n",
      "Testing: 0.6517768599383686 Training: 0.6437128917862492 Seed: 7184\n",
      "Testing: 0.6507459524529448 Training: 0.6440714333702056 Seed: 7186\n",
      "Testing: 0.6496531424475279 Training: 0.6442995626575184 Seed: 7187\n",
      "Testing: 0.6699306060831451 Training: 0.6393203855947114 Seed: 7189\n",
      "Testing: 0.6541238591697404 Training: 0.6430551776865261 Seed: 7191\n",
      "Testing: 0.6481173022983492 Training: 0.6444777399112844 Seed: 7192\n",
      "Testing: 0.6555007633553273 Training: 0.6426758895481046 Seed: 7193\n",
      "Testing: 0.6489934433401273 Training: 0.6444193772380908 Seed: 7194\n",
      "Testing: 0.6531446455742659 Training: 0.6434587899628408 Seed: 7202\n",
      "Testing: 0.6470377702714942 Training: 0.6449118052338627 Seed: 7205\n",
      "Testing: 0.6556516592906529 Training: 0.6427528948555025 Seed: 7206\n",
      "Testing: 0.6500838284730602 Training: 0.6440386883017213 Seed: 7207\n",
      "Testing: 0.6473329488780739 Training: 0.6449021043768243 Seed: 7208\n",
      "Testing: 0.6509716931392473 Training: 0.643998109356071 Seed: 7209\n",
      "Testing: 0.6591827409249682 Training: 0.6416899506222296 Seed: 7211\n",
      "Testing: 0.6619378349438737 Training: 0.6413629690498561 Seed: 7212\n",
      "Testing: 0.6529336498472567 Training: 0.6434606034114776 Seed: 7213\n",
      "Testing: 0.6499816806760377 Training: 0.6441129818526463 Seed: 7215\n",
      "Testing: 0.6476512499865537 Training: 0.6448247277946383 Seed: 7216\n",
      "Testing: 0.6507712047479166 Training: 0.6439005954104595 Seed: 7217\n",
      "Testing: 0.6468974303368213 Training: 0.6449929274542898 Seed: 7219\n",
      "Testing: 0.6516809753820002 Training: 0.6437209787263655 Seed: 7220\n",
      "Testing: 0.6562567283556525 Training: 0.6426008281254205 Seed: 7222\n",
      "Testing: 0.6594263564761076 Training: 0.6419679188442869 Seed: 7223\n",
      "Testing: 0.6462957404784198 Training: 0.6450631361951165 Seed: 7226\n",
      "Testing: 0.6520014601661395 Training: 0.6435559722845029 Seed: 7229\n",
      "Testing: 0.6518632019386637 Training: 0.6437297005750233 Seed: 7231\n",
      "Testing: 0.6460070828880993 Training: 0.6452520914085198 Seed: 7232\n",
      "Testing: 0.659711075030259 Training: 0.641774642971921 Seed: 7233\n",
      "Testing: 0.6457591797781737 Training: 0.6452169042886529 Seed: 7234\n",
      "Testing: 0.6558783183720281 Training: 0.6428037869739746 Seed: 7235\n",
      "Testing: 0.6462798556298475 Training: 0.6451524719648059 Seed: 7238\n",
      "Testing: 0.651522092846783 Training: 0.6437999217209232 Seed: 7240\n",
      "Testing: 0.6566466453861155 Training: 0.6423175442725011 Seed: 7242\n",
      "Testing: 0.6505786935643872 Training: 0.6440222525543613 Seed: 7246\n",
      "Testing: 0.6631303329882116 Training: 0.6408055230908689 Seed: 7249\n",
      "Testing: 0.6568013141743567 Training: 0.6426022382607878 Seed: 7250\n",
      "Testing: 0.647888158507127 Training: 0.6446445408130627 Seed: 7251\n",
      "Testing: 0.6489229663642162 Training: 0.6445235845228502 Seed: 7252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.650132127439519 Training: 0.6439413522547537 Seed: 7255\n",
      "Testing: 0.646978423448432 Training: 0.6447493552528374 Seed: 7256\n",
      "Testing: 0.6643096816577803 Training: 0.6403719933580778 Seed: 7260\n",
      "Testing: 0.6457424537707344 Training: 0.6452577329247188 Seed: 7261\n",
      "Testing: 0.6456143394106015 Training: 0.6453279369917926 Seed: 7262\n",
      "Testing: 0.6574363347345888 Training: 0.6425420434710216 Seed: 7264\n",
      "Testing: 0.6496963669423932 Training: 0.6441600174784835 Seed: 7269\n",
      "Testing: 0.6522162110818127 Training: 0.6436501397326926 Seed: 7273\n",
      "Testing: 0.6470912823872461 Training: 0.6444423172817513 Seed: 7274\n",
      "Testing: 0.6515149578809265 Training: 0.6438425977240423 Seed: 7275\n",
      "Testing: 0.6577643456201903 Training: 0.6423766974666737 Seed: 7276\n",
      "Testing: 0.6501983392899109 Training: 0.6442002575426469 Seed: 7277\n",
      "Testing: 0.6486182581696766 Training: 0.6445357601011004 Seed: 7279\n",
      "Testing: 0.6505390757540275 Training: 0.6439772045249051 Seed: 7280\n",
      "Testing: 0.6583377257921079 Training: 0.6418501091683311 Seed: 7281\n",
      "Testing: 0.657133028482079 Training: 0.6426041800620865 Seed: 7282\n",
      "Testing: 0.6570445828139311 Training: 0.6423352660849472 Seed: 7284\n",
      "Testing: 0.6556796419094464 Training: 0.6428471312026239 Seed: 7285\n",
      "Testing: 0.6534505296536532 Training: 0.6433122807715871 Seed: 7286\n",
      "Testing: 0.6580205625302573 Training: 0.6421324251792839 Seed: 7287\n",
      "Testing: 0.6468267008019725 Training: 0.6449463812882663 Seed: 7289\n",
      "Testing: 0.6459152680552973 Training: 0.6450889455764415 Seed: 7290\n",
      "Testing: 0.6581812656924059 Training: 0.6422032540262288 Seed: 7291\n",
      "Testing: 0.6627421585939092 Training: 0.6407849073316356 Seed: 7293\n",
      "Testing: 0.6556609982624222 Training: 0.6428654530817737 Seed: 7295\n",
      "Testing: 0.6456738715000937 Training: 0.6453539423014274 Seed: 7296\n",
      "Testing: 0.6486114444358639 Training: 0.6439814555368094 Seed: 7297\n",
      "Testing: 0.6457323850756727 Training: 0.645193902005888 Seed: 7298\n",
      "Testing: 0.6457202506154192 Training: 0.6452245999702486 Seed: 7299\n",
      "Testing: 0.6628090889911276 Training: 0.6410942292005204 Seed: 7302\n",
      "Testing: 0.6573367822933724 Training: 0.6425284375410174 Seed: 7308\n",
      "Testing: 0.6520933606744801 Training: 0.643347607981249 Seed: 7311\n",
      "Testing: 0.6665731533312641 Training: 0.6399724095513057 Seed: 7312\n",
      "Testing: 0.6594603194686142 Training: 0.6418925136320418 Seed: 7313\n",
      "Testing: 0.651834232469785 Training: 0.643828815974603 Seed: 7314\n",
      "Testing: 0.6691152171391718 Training: 0.6389069522796325 Seed: 7316\n",
      "Testing: 0.6512183697238703 Training: 0.6438955103744718 Seed: 7317\n",
      "Testing: 0.6521160782003332 Training: 0.6436854845348455 Seed: 7319\n",
      "Testing: 0.6616209087584246 Training: 0.641242018540306 Seed: 7320\n",
      "Testing: 0.6475377578697492 Training: 0.6447967175167881 Seed: 7321\n",
      "Testing: 0.6521018618938749 Training: 0.6437050212175898 Seed: 7322\n",
      "Testing: 0.6725849528572018 Training: 0.6384870536835587 Seed: 7324\n",
      "Testing: 0.6592841696053677 Training: 0.6417971784919352 Seed: 7326\n",
      "Testing: 0.6569660322533567 Training: 0.6424123470303437 Seed: 7328\n",
      "Testing: 0.6457595643939833 Training: 0.6453631236393668 Seed: 7332\n",
      "Testing: 0.6635048676429849 Training: 0.6407519914026718 Seed: 7334\n",
      "Testing: 0.6606261031674904 Training: 0.6414314544622963 Seed: 7335\n",
      "Testing: 0.6569557587455171 Training: 0.6424687029314342 Seed: 7336\n",
      "Testing: 0.6487895306158612 Training: 0.6445971251754419 Seed: 7341\n",
      "Testing: 0.6473002292523408 Training: 0.6448291545171583 Seed: 7344\n",
      "Testing: 0.6502162270426847 Training: 0.6441853279147169 Seed: 7345\n",
      "Testing: 0.6588359058870779 Training: 0.6419895538749674 Seed: 7346\n",
      "Testing: 0.6481758703446006 Training: 0.6447098191715173 Seed: 7349\n",
      "Testing: 0.6563242505801734 Training: 0.6424229480583173 Seed: 7350\n",
      "Testing: 0.6474525810602784 Training: 0.6448481707685746 Seed: 7351\n",
      "Testing: 0.6563189347798732 Training: 0.6425935302864922 Seed: 7354\n",
      "Testing: 0.6620014144195046 Training: 0.6412612719598962 Seed: 7357\n",
      "Testing: 0.6504372190499004 Training: 0.6441141167312803 Seed: 7360\n",
      "Testing: 0.6557052375454798 Training: 0.6427965120681205 Seed: 7361\n",
      "Testing: 0.6559821901617541 Training: 0.6427136949195276 Seed: 7363\n",
      "Testing: 0.653604174605037 Training: 0.6434252297855279 Seed: 7365\n",
      "Testing: 0.657636851097672 Training: 0.642431798776512 Seed: 7366\n",
      "Testing: 0.6488737541192795 Training: 0.644403456919358 Seed: 7367\n",
      "Testing: 0.6512219843314286 Training: 0.6437046215333704 Seed: 7373\n",
      "Testing: 0.648499991538986 Training: 0.6444919717278391 Seed: 7375\n",
      "Testing: 0.660481839581293 Training: 0.6414080955433599 Seed: 7384\n",
      "Testing: 0.6467453984741962 Training: 0.6450561402206659 Seed: 7388\n",
      "Testing: 0.6476957710126157 Training: 0.6447927081651683 Seed: 7389\n",
      "Testing: 0.6586219828710294 Training: 0.6418783583934958 Seed: 7390\n",
      "Testing: 0.6602033169315804 Training: 0.6416678798985496 Seed: 7391\n",
      "Testing: 0.6500163328691275 Training: 0.6441966788072999 Seed: 7393\n",
      "Testing: 0.6492968723262471 Training: 0.6443425833741406 Seed: 7395\n",
      "Testing: 0.6544203523698595 Training: 0.6431712234224223 Seed: 7396\n",
      "Testing: 0.6577864470405914 Training: 0.6422506437143649 Seed: 7397\n",
      "Testing: 0.6605851481131333 Training: 0.6416366757689972 Seed: 7400\n",
      "Testing: 0.658499146149049 Training: 0.6420013536162104 Seed: 7401\n",
      "Testing: 0.6502392079142123 Training: 0.6442301811972415 Seed: 7410\n",
      "Testing: 0.6586580216772202 Training: 0.6420045184962115 Seed: 7417\n",
      "Testing: 0.6565087775985106 Training: 0.6425737073428908 Seed: 7421\n",
      "Testing: 0.6554710206604699 Training: 0.6427438644161592 Seed: 7422\n",
      "Testing: 0.6526274867215053 Training: 0.6436033053277461 Seed: 7423\n",
      "Testing: 0.6523447699295996 Training: 0.6434878355673344 Seed: 7424\n",
      "Testing: 0.6522529106541609 Training: 0.6436701525943356 Seed: 7426\n",
      "Testing: 0.6548272380923492 Training: 0.6430723689988176 Seed: 7427\n",
      "Testing: 0.6554372981338602 Training: 0.6426973523239392 Seed: 7429\n",
      "Testing: 0.6546220450262128 Training: 0.6430013155106292 Seed: 7437\n",
      "Testing: 0.6496580118061159 Training: 0.6443073815666047 Seed: 7439\n",
      "Testing: 0.6710941438349315 Training: 0.6388258488537917 Seed: 7441\n",
      "Testing: 0.6496973256702284 Training: 0.6442685261444073 Seed: 7444\n",
      "Testing: 0.6642138177952587 Training: 0.6405604152900761 Seed: 7445\n",
      "Testing: 0.6538665754300345 Training: 0.6434280341663877 Seed: 7448\n",
      "Testing: 0.6559778502218903 Training: 0.642794856274885 Seed: 7455\n",
      "Testing: 0.6577914113745937 Training: 0.6417625928615016 Seed: 7459\n",
      "Testing: 0.6499150655114484 Training: 0.6441780196112541 Seed: 7461\n",
      "Testing: 0.6489311767546273 Training: 0.6444642610756377 Seed: 7465\n",
      "Testing: 0.6476277886818973 Training: 0.64459804248732 Seed: 7466\n",
      "Testing: 0.6460957656441216 Training: 0.6451770079848215 Seed: 7467\n",
      "Testing: 0.6545399948342032 Training: 0.6432042645248308 Seed: 7473\n",
      "Testing: 0.6483051940049116 Training: 0.6446244084600345 Seed: 7474\n",
      "Testing: 0.6506538070956364 Training: 0.6440229470160186 Seed: 7475\n",
      "Testing: 0.6688465880203424 Training: 0.6393237833658172 Seed: 7476\n",
      "Testing: 0.6497267224083176 Training: 0.6443261099419854 Seed: 7478\n",
      "Testing: 0.6530555993195913 Training: 0.6434022566611644 Seed: 7480\n",
      "Testing: 0.6507682505153065 Training: 0.6439314917428686 Seed: 7482\n",
      "Testing: 0.6530035329553614 Training: 0.6435937263665071 Seed: 7485\n",
      "Testing: 0.6455717262006083 Training: 0.6453241023395906 Seed: 7493\n",
      "Testing: 0.6518037342617962 Training: 0.6436422204947336 Seed: 7494\n",
      "Testing: 0.6493860907937751 Training: 0.6443650894211143 Seed: 7495\n",
      "Testing: 0.6564467488912582 Training: 0.6426706783659976 Seed: 7496\n",
      "Testing: 0.6640591962700308 Training: 0.6407191721910821 Seed: 7502\n",
      "Testing: 0.6500548310869548 Training: 0.6440769882494539 Seed: 7503\n",
      "Testing: 0.645347944494825 Training: 0.6452548540855694 Seed: 7504\n",
      "Testing: 0.6560994246781107 Training: 0.6426060587366255 Seed: 7505\n",
      "Testing: 0.6580373337717168 Training: 0.6422134912262378 Seed: 7506\n",
      "Testing: 0.6549072696738582 Training: 0.6430946231736059 Seed: 7507\n",
      "Testing: 0.6457973826446892 Training: 0.645316265521844 Seed: 7509\n",
      "Testing: 0.6550311097872112 Training: 0.6430757006752795 Seed: 7510\n",
      "Testing: 0.6645528726888486 Training: 0.6406412760661828 Seed: 7511\n",
      "Testing: 0.6476650356712161 Training: 0.64465127955899 Seed: 7513\n",
      "Testing: 0.6479158983114778 Training: 0.6447725938835324 Seed: 7514\n",
      "Testing: 0.6552097374963736 Training: 0.6428503984464625 Seed: 7515\n",
      "Testing: 0.6557908767184838 Training: 0.6428774528662644 Seed: 7516\n",
      "Testing: 0.648794296921785 Training: 0.6444318400963354 Seed: 7520\n",
      "Testing: 0.655639090701219 Training: 0.6427005347424946 Seed: 7521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6611879221444352 Training: 0.6413978419671424 Seed: 7523\n",
      "Testing: 0.6506261467892435 Training: 0.644069254776251 Seed: 7526\n",
      "Testing: 0.6574596350295042 Training: 0.6423032271642821 Seed: 7527\n",
      "Testing: 0.6512001679102328 Training: 0.6439084176924179 Seed: 7528\n",
      "Testing: 0.6518406424341295 Training: 0.6436538790976248 Seed: 7529\n",
      "Testing: 0.648446792261907 Training: 0.6444956404359429 Seed: 7530\n",
      "Testing: 0.6483268910986447 Training: 0.6442613553044749 Seed: 7532\n",
      "Testing: 0.6474008428786344 Training: 0.6446336542887761 Seed: 7533\n",
      "Testing: 0.6454598462039978 Training: 0.6453671087463568 Seed: 7534\n",
      "Testing: 0.6524706387819788 Training: 0.643528420800205 Seed: 7535\n",
      "Testing: 0.6611415368931676 Training: 0.6414049241768786 Seed: 7537\n",
      "Testing: 0.6496341504620626 Training: 0.6442803251251266 Seed: 7539\n",
      "Testing: 0.6554800147620807 Training: 0.642663541930067 Seed: 7540\n",
      "Testing: 0.6564429798669562 Training: 0.6425281975290298 Seed: 7542\n",
      "Testing: 0.6613227200613487 Training: 0.6413858726739221 Seed: 7544\n",
      "Testing: 0.6620379592423371 Training: 0.6410898780739638 Seed: 7545\n",
      "Testing: 0.6572969063520941 Training: 0.6422472992342574 Seed: 7549\n",
      "Testing: 0.648408104592754 Training: 0.6446126179675573 Seed: 7550\n",
      "Testing: 0.6574167550991188 Training: 0.6423297218570613 Seed: 7552\n",
      "Testing: 0.6586417196103739 Training: 0.6416542071979325 Seed: 7553\n",
      "Testing: 0.6533149975467927 Training: 0.6433110664810584 Seed: 7554\n",
      "Testing: 0.662260565666082 Training: 0.6410963949963433 Seed: 7558\n",
      "Testing: 0.6484956860195813 Training: 0.6444875646398919 Seed: 7560\n",
      "Testing: 0.6462674154751924 Training: 0.6449730256564953 Seed: 7563\n",
      "Testing: 0.6506072383524769 Training: 0.6440287851913761 Seed: 7565\n",
      "Testing: 0.6550624727065871 Training: 0.6428882591422088 Seed: 7569\n",
      "Testing: 0.6573790407288254 Training: 0.642495463421034 Seed: 7572\n",
      "Testing: 0.6586820219921925 Training: 0.6413142536382286 Seed: 7576\n",
      "Testing: 0.6458891509888098 Training: 0.6452287985508781 Seed: 7578\n",
      "Testing: 0.6501048307556802 Training: 0.6441184119161409 Seed: 7579\n",
      "Testing: 0.648860346605406 Training: 0.6445171088664738 Seed: 7580\n",
      "Testing: 0.6531077247173175 Training: 0.6434343553932673 Seed: 7581\n",
      "Testing: 0.6554209138406589 Training: 0.6428852648787381 Seed: 7582\n",
      "Testing: 0.6542122702771914 Training: 0.643145792614886 Seed: 7583\n",
      "Testing: 0.6534309989201637 Training: 0.6433485341809028 Seed: 7584\n",
      "Testing: 0.6500887707232736 Training: 0.6442325178373778 Seed: 7586\n",
      "Testing: 0.6551803225295884 Training: 0.6428430663521132 Seed: 7590\n",
      "Testing: 0.648674496262114 Training: 0.6445740146884023 Seed: 7591\n",
      "Testing: 0.6514317046000193 Training: 0.6437946789978714 Seed: 7596\n",
      "Testing: 0.6672324165157663 Training: 0.6396598972230377 Seed: 7597\n",
      "Testing: 0.651615849198677 Training: 0.6438464051739239 Seed: 7598\n",
      "Testing: 0.6533699770942094 Training: 0.6433228817975558 Seed: 7599\n",
      "Testing: 0.6792933196589107 Training: 0.6367364570102749 Seed: 7600\n",
      "Testing: 0.6548493378490019 Training: 0.643019197107207 Seed: 7604\n",
      "Testing: 0.6456416526274922 Training: 0.6453024932756464 Seed: 7605\n",
      "Testing: 0.6458471908054882 Training: 0.6452856185118041 Seed: 7610\n",
      "Testing: 0.6651643334266203 Training: 0.6403598640503454 Seed: 7611\n",
      "Testing: 0.6513227572785436 Training: 0.6438067764135526 Seed: 7613\n",
      "Testing: 0.6469804456061319 Training: 0.6447990420144492 Seed: 7615\n",
      "Testing: 0.659564711818429 Training: 0.6416314318886767 Seed: 7616\n",
      "Testing: 0.651658709145124 Training: 0.6438361180781799 Seed: 7620\n",
      "Testing: 0.6516592547259694 Training: 0.6438312262285317 Seed: 7624\n",
      "Testing: 0.6537322354644238 Training: 0.6432268136886576 Seed: 7625\n",
      "Testing: 0.6679096638498558 Training: 0.6396714814635689 Seed: 7626\n",
      "Testing: 0.6495633735597678 Training: 0.6443573623791496 Seed: 7627\n",
      "Testing: 0.6491095388672952 Training: 0.6443029422453613 Seed: 7629\n",
      "Testing: 0.646188656524407 Training: 0.6450553288057764 Seed: 7631\n",
      "Testing: 0.648051524149047 Training: 0.6446695010988076 Seed: 7635\n",
      "Testing: 0.6484261368660899 Training: 0.6446224755142043 Seed: 7638\n",
      "Testing: 0.659286575401825 Training: 0.6418327225288643 Seed: 7639\n",
      "Testing: 0.6515166155141324 Training: 0.643597081429925 Seed: 7643\n",
      "Testing: 0.645561723213866 Training: 0.6452356747562772 Seed: 7644\n",
      "Testing: 0.6504350645638616 Training: 0.6440307536233661 Seed: 7648\n",
      "Testing: 0.6534895852756466 Training: 0.643266819455423 Seed: 7649\n",
      "Testing: 0.6531356851594102 Training: 0.6433251142503957 Seed: 7651\n",
      "Testing: 0.6459661768366385 Training: 0.6452533077395453 Seed: 7654\n",
      "Testing: 0.6576529578301229 Training: 0.6423167404817505 Seed: 7656\n",
      "Testing: 0.6525141419391068 Training: 0.6435097683732967 Seed: 7658\n",
      "Testing: 0.652388442023939 Training: 0.6429106695437021 Seed: 7659\n",
      "Testing: 0.6459561343029354 Training: 0.6451868606448555 Seed: 7660\n",
      "Testing: 0.6490098806042277 Training: 0.6444494874788361 Seed: 7662\n",
      "Testing: 0.6586861104334043 Training: 0.6420959531315813 Seed: 7665\n",
      "Testing: 0.6522277926250513 Training: 0.643697212126585 Seed: 7667\n",
      "Testing: 0.6515832317627829 Training: 0.64369943881313 Seed: 7668\n",
      "Testing: 0.6480781292061224 Training: 0.6446547172462022 Seed: 7671\n",
      "Testing: 0.6698765479782477 Training: 0.6390033915247861 Seed: 7673\n",
      "Testing: 0.6491274465282024 Training: 0.6444491148179258 Seed: 7678\n",
      "Testing: 0.6462930811908607 Training: 0.6452304466260994 Seed: 7679\n",
      "Testing: 0.6511835719681595 Training: 0.643955495611707 Seed: 7681\n",
      "Testing: 0.6569677698141869 Training: 0.6422781620614846 Seed: 7682\n",
      "Testing: 0.6465872212285975 Training: 0.6449329932569986 Seed: 7683\n",
      "Testing: 0.6518697834853321 Training: 0.6436456214335399 Seed: 7685\n",
      "Testing: 0.6570838251320867 Training: 0.6423742486743558 Seed: 7686\n",
      "Testing: 0.6496005253156174 Training: 0.644393114453994 Seed: 7687\n",
      "Testing: 0.6584372041390912 Training: 0.6420971594430438 Seed: 7691\n",
      "Testing: 0.6731564553947467 Training: 0.6381250174229582 Seed: 7693\n",
      "Testing: 0.6580253188653692 Training: 0.6422739332324422 Seed: 7696\n",
      "Testing: 0.6463556976216669 Training: 0.6451548792131798 Seed: 7697\n",
      "Testing: 0.6574687678957754 Training: 0.6423996897667286 Seed: 7699\n",
      "Testing: 0.6643151843491102 Training: 0.6404099031045951 Seed: 7701\n",
      "Testing: 0.661554568348502 Training: 0.6410449201588849 Seed: 7703\n",
      "Testing: 0.6513471073852488 Training: 0.6438533529861364 Seed: 7705\n",
      "Testing: 0.6588763058300033 Training: 0.6418845847014099 Seed: 7706\n",
      "Testing: 0.6618758116994062 Training: 0.6410519005485498 Seed: 7709\n",
      "Testing: 0.6521339520153877 Training: 0.6437679134412284 Seed: 7710\n",
      "Testing: 0.6516931147634937 Training: 0.6438873129936804 Seed: 7711\n",
      "Testing: 0.6563249965849559 Training: 0.6426939890839727 Seed: 7714\n",
      "Testing: 0.6486965487906146 Training: 0.6445734645673121 Seed: 7715\n",
      "Testing: 0.6585732108302914 Training: 0.6419720916029529 Seed: 7717\n",
      "Testing: 0.6530666288720322 Training: 0.6434468615350012 Seed: 7718\n",
      "Testing: 0.65738490957651 Training: 0.6424991807450025 Seed: 7719\n",
      "Testing: 0.6636554589400114 Training: 0.6409134322312507 Seed: 7723\n",
      "Testing: 0.6515844201092135 Training: 0.6437870345394342 Seed: 7725\n",
      "Testing: 0.6466532790665176 Training: 0.6447480768697863 Seed: 7726\n",
      "Testing: 0.6576011136046865 Training: 0.6423347162406152 Seed: 7727\n",
      "Testing: 0.6502914818476722 Training: 0.6440423173266878 Seed: 7728\n",
      "Testing: 0.6606923162607687 Training: 0.6414038034354979 Seed: 7730\n",
      "Testing: 0.6538337223567305 Training: 0.6426420035401825 Seed: 7731\n",
      "Testing: 0.6535589705895574 Training: 0.6433858023638332 Seed: 7732\n",
      "Testing: 0.6618474733344939 Training: 0.641290632385147 Seed: 7735\n",
      "Testing: 0.6459234889905177 Training: 0.6452557967040788 Seed: 7736\n",
      "Testing: 0.6486623026379663 Training: 0.6445860437644524 Seed: 7737\n",
      "Testing: 0.6508197729459658 Training: 0.6440279853581379 Seed: 7738\n",
      "Testing: 0.649501130803578 Training: 0.6442857568092311 Seed: 7742\n",
      "Testing: 0.6453322335162501 Training: 0.6453181185254285 Seed: 7744\n",
      "Testing: 0.6498578260612791 Training: 0.6443218746116046 Seed: 7749\n",
      "Testing: 0.6481990045748871 Training: 0.6446355885130464 Seed: 7751\n",
      "Testing: 0.6553591596298921 Training: 0.6428558465658254 Seed: 7752\n",
      "Testing: 0.6648368393061368 Training: 0.6406700739137992 Seed: 7753\n",
      "Testing: 0.6599134879170453 Training: 0.6413091672854851 Seed: 7754\n",
      "Testing: 0.6577481148301092 Training: 0.642103098550388 Seed: 7755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6645466606307495 Training: 0.6406388706087256 Seed: 7757\n",
      "Testing: 0.6596554513870074 Training: 0.6418402403207675 Seed: 7759\n",
      "Testing: 0.6459291367632324 Training: 0.6451226736169384 Seed: 7761\n",
      "Testing: 0.6518300892999597 Training: 0.6437408963923368 Seed: 7764\n",
      "Testing: 0.6485282640847636 Training: 0.644636684357873 Seed: 7765\n",
      "Testing: 0.6507563486974636 Training: 0.6439343542843384 Seed: 7769\n",
      "Testing: 0.6513128202460899 Training: 0.6439177421787607 Seed: 7770\n",
      "Testing: 0.6554337933239516 Training: 0.6428000079282273 Seed: 7771\n",
      "Testing: 0.6519481761498906 Training: 0.6437090048591891 Seed: 7775\n",
      "Testing: 0.6554466684775818 Training: 0.6427988153568758 Seed: 7777\n",
      "Testing: 0.6471415408387441 Training: 0.6449169951730176 Seed: 7778\n",
      "Testing: 0.6633005681971695 Training: 0.6408770768720764 Seed: 7779\n",
      "Testing: 0.6602153786406337 Training: 0.6415385361645618 Seed: 7780\n",
      "Testing: 0.6608180601894255 Training: 0.6414946420112391 Seed: 7781\n",
      "Testing: 0.649489633680555 Training: 0.6443596005485576 Seed: 7782\n",
      "Testing: 0.6622276206317578 Training: 0.6409421082847907 Seed: 7784\n",
      "Testing: 0.6555886527803068 Training: 0.6426088092331268 Seed: 7785\n",
      "Testing: 0.6534281450689584 Training: 0.6433139365853052 Seed: 7788\n",
      "Testing: 0.6478945192601869 Training: 0.6447027620151243 Seed: 7791\n",
      "Testing: 0.6532331578382156 Training: 0.6434175725139728 Seed: 7792\n",
      "Testing: 0.6516355827644666 Training: 0.6437750018101381 Seed: 7796\n",
      "Testing: 0.6529241304346536 Training: 0.6434323873821627 Seed: 7797\n",
      "Testing: 0.656963812726805 Training: 0.6424945724003066 Seed: 7798\n",
      "Testing: 0.6487702487553463 Training: 0.6442531183208409 Seed: 7801\n",
      "Testing: 0.6621183718247622 Training: 0.641167624602466 Seed: 7802\n",
      "Testing: 0.6608398758401036 Training: 0.6413871581217974 Seed: 7803\n",
      "Testing: 0.6566573383919712 Training: 0.6426106089396865 Seed: 7805\n",
      "Testing: 0.6457235744575517 Training: 0.6452981817252182 Seed: 7806\n",
      "Testing: 0.6460863580503267 Training: 0.6452543434888997 Seed: 7810\n",
      "Testing: 0.6483156865028828 Training: 0.6445904821467319 Seed: 7811\n",
      "Testing: 0.6594402607232522 Training: 0.641774639777434 Seed: 7812\n",
      "Testing: 0.6471795491618542 Training: 0.644977607142412 Seed: 7813\n",
      "Testing: 0.6517100548678612 Training: 0.6439312347308546 Seed: 7816\n",
      "Testing: 0.6512471301192044 Training: 0.6440081560183012 Seed: 7817\n",
      "Testing: 0.645829222479054 Training: 0.6451232087658155 Seed: 7818\n",
      "Testing: 0.6478375914640747 Training: 0.6447844100280375 Seed: 7819\n",
      "Testing: 0.6627669294476616 Training: 0.6408370457735044 Seed: 7820\n",
      "Testing: 0.645822362938641 Training: 0.6453400675681742 Seed: 7825\n",
      "Testing: 0.6571681670412391 Training: 0.6423381940883495 Seed: 7827\n",
      "Testing: 0.6467620998710211 Training: 0.6450800059924806 Seed: 7829\n",
      "Testing: 0.6519402114694227 Training: 0.643733579355763 Seed: 7830\n",
      "Testing: 0.6459631076710864 Training: 0.6452300581226911 Seed: 7831\n",
      "Testing: 0.6500026721638743 Training: 0.6441223381108606 Seed: 7833\n",
      "Testing: 0.6598109432679095 Training: 0.641661965483179 Seed: 7836\n",
      "Testing: 0.6494017121427729 Training: 0.6442550153352795 Seed: 7840\n",
      "Testing: 0.6495358281818656 Training: 0.6444438456034323 Seed: 7842\n",
      "Testing: 0.6581974423379361 Training: 0.6421126641772298 Seed: 7843\n",
      "Testing: 0.6599734388630554 Training: 0.6415729831069464 Seed: 7845\n",
      "Testing: 0.6473683352522601 Training: 0.6449546847080097 Seed: 7849\n",
      "Testing: 0.6459874205633743 Training: 0.6450773248978361 Seed: 7850\n",
      "Testing: 0.6767852224819738 Training: 0.6375551033969535 Seed: 7851\n",
      "Testing: 0.6551382137540012 Training: 0.64292283183852 Seed: 7856\n",
      "Testing: 0.6501341115661029 Training: 0.6442430225714457 Seed: 7858\n",
      "Testing: 0.6576977702306926 Training: 0.6423667142740896 Seed: 7862\n",
      "Testing: 0.657662763066027 Training: 0.6421368336403879 Seed: 7864\n",
      "Testing: 0.6580521864822693 Training: 0.6421269917610111 Seed: 7865\n",
      "Testing: 0.6655744553722329 Training: 0.6405006289431279 Seed: 7868\n",
      "Testing: 0.6538796120035567 Training: 0.6432598851662361 Seed: 7870\n",
      "Testing: 0.6581203895686705 Training: 0.6421226874278776 Seed: 7873\n",
      "Testing: 0.6454463964935991 Training: 0.6454071918559775 Seed: 7879\n",
      "Testing: 0.6645233778255 Training: 0.640629964439619 Seed: 7881\n",
      "Testing: 0.6582807661079252 Training: 0.6422682676448435 Seed: 7883\n",
      "Testing: 0.6543866514580516 Training: 0.6430847796290168 Seed: 7885\n",
      "Testing: 0.6457254383259501 Training: 0.6453417068939364 Seed: 7887\n",
      "Testing: 0.6460786165556966 Training: 0.6450148899299664 Seed: 7890\n",
      "Testing: 0.6685647250054894 Training: 0.639395651616139 Seed: 7891\n",
      "Testing: 0.6584123008267824 Training: 0.6420821564131618 Seed: 7894\n",
      "Testing: 0.6479897487028821 Training: 0.6446708907703815 Seed: 7896\n",
      "Testing: 0.6481670236842157 Training: 0.6446493023563704 Seed: 7897\n",
      "Testing: 0.6596694585492722 Training: 0.6418545350278668 Seed: 7898\n",
      "Testing: 0.6532874219939941 Training: 0.6433454432393646 Seed: 7902\n",
      "Testing: 0.6565746480660877 Training: 0.6426202020986744 Seed: 7904\n",
      "Testing: 0.6547868344325655 Training: 0.6430231803380443 Seed: 7906\n",
      "Testing: 0.6647812838558428 Training: 0.6404661591898617 Seed: 7908\n",
      "Testing: 0.6520734047058052 Training: 0.6437125494640901 Seed: 7912\n",
      "Testing: 0.6525288620865561 Training: 0.6435797639855305 Seed: 7915\n",
      "Testing: 0.6595432217766113 Training: 0.6418995519401469 Seed: 7916\n",
      "Testing: 0.6566697493320018 Training: 0.6424391638419735 Seed: 7921\n",
      "Testing: 0.649605297975249 Training: 0.644294257685077 Seed: 7922\n",
      "Testing: 0.6495637698415522 Training: 0.6443450865492845 Seed: 7924\n",
      "Testing: 0.6525181200134301 Training: 0.6435911603244231 Seed: 7929\n",
      "Testing: 0.6557118843181489 Training: 0.6427634887946876 Seed: 7930\n",
      "Testing: 0.6535461720530966 Training: 0.6431312519985732 Seed: 7931\n",
      "Testing: 0.6481686713498443 Training: 0.644754097847876 Seed: 7933\n",
      "Testing: 0.6619155465611014 Training: 0.6412788113449314 Seed: 7935\n",
      "Testing: 0.647230747019969 Training: 0.6449023382550543 Seed: 7936\n",
      "Testing: 0.6472017730527374 Training: 0.6446472701689242 Seed: 7938\n",
      "Testing: 0.6467394797778382 Training: 0.645004522964135 Seed: 7941\n",
      "Testing: 0.6474252990888496 Training: 0.6448748339633995 Seed: 7943\n",
      "Testing: 0.6509001886641158 Training: 0.6440083014636364 Seed: 7945\n",
      "Testing: 0.649628874013118 Training: 0.6443661045410378 Seed: 7947\n",
      "Testing: 0.6549346284723729 Training: 0.6429553035310518 Seed: 7954\n",
      "Testing: 0.6461763253446204 Training: 0.6450494235545926 Seed: 7956\n",
      "Testing: 0.6456175689599016 Training: 0.6452361744247296 Seed: 7958\n",
      "Testing: 0.6479252767584283 Training: 0.644795860273804 Seed: 7969\n",
      "Testing: 0.65281694869468 Training: 0.6434756684687045 Seed: 7974\n",
      "Testing: 0.6607163682889989 Training: 0.6415626755346722 Seed: 7975\n",
      "Testing: 0.6460288771293037 Training: 0.645142250561058 Seed: 7976\n",
      "Testing: 0.6559631108743015 Training: 0.6428372546400798 Seed: 7977\n",
      "Testing: 0.6595196089199749 Training: 0.6419415924706844 Seed: 7979\n",
      "Testing: 0.6534069510637058 Training: 0.6434298812710114 Seed: 7982\n",
      "Testing: 0.6463964057341007 Training: 0.6450802971527201 Seed: 7983\n",
      "Testing: 0.651496034557695 Training: 0.6439121147103293 Seed: 7984\n",
      "Testing: 0.6624970074372502 Training: 0.6408819627234073 Seed: 7990\n",
      "Testing: 0.6550621287782692 Training: 0.6429487687072396 Seed: 7991\n",
      "Testing: 0.6470180661142246 Training: 0.644893163127435 Seed: 7992\n",
      "Testing: 0.6581807654126617 Training: 0.6422767503460884 Seed: 7993\n",
      "Testing: 0.6683972454216687 Training: 0.6394103025844917 Seed: 7994\n",
      "Testing: 0.6650923317463994 Training: 0.6405266874473846 Seed: 7996\n",
      "Testing: 0.6460301460095252 Training: 0.6450724915907372 Seed: 7999\n",
      "Testing: 0.654419148564147 Training: 0.6431657871840929 Seed: 8000\n",
      "Testing: 0.6537656893819013 Training: 0.6433628026132738 Seed: 8001\n",
      "Testing: 0.6471660571127197 Training: 0.6449550721752348 Seed: 8006\n",
      "Testing: 0.6541634000971088 Training: 0.643027121097711 Seed: 8008\n",
      "Testing: 0.6796840942522611 Training: 0.6360614306587966 Seed: 8010\n",
      "Testing: 0.6495260063195777 Training: 0.6442742560651701 Seed: 8011\n",
      "Testing: 0.651490822355431 Training: 0.6437540310058867 Seed: 8012\n",
      "Testing: 0.6478776448919108 Training: 0.644651793965475 Seed: 8015\n",
      "Testing: 0.6541724401540245 Training: 0.643161442130978 Seed: 8016\n",
      "Testing: 0.6509634111238091 Training: 0.6438503253397543 Seed: 8017\n",
      "Testing: 0.6465539949838479 Training: 0.644613720141157 Seed: 8018\n",
      "Testing: 0.6503974115655766 Training: 0.6439332938477069 Seed: 8021\n",
      "Testing: 0.6487299569020399 Training: 0.6443398117579759 Seed: 8022\n",
      "Testing: 0.6503233376680122 Training: 0.6441019740005122 Seed: 8024\n",
      "Testing: 0.6477572323031674 Training: 0.6447712420378138 Seed: 8025\n",
      "Testing: 0.6499126305552776 Training: 0.6439349046144887 Seed: 8026\n",
      "Testing: 0.6460941461766816 Training: 0.6452366763170247 Seed: 8027\n",
      "Testing: 0.6516810412132028 Training: 0.6437712414994485 Seed: 8028\n",
      "Testing: 0.6452204716367749 Training: 0.644994314710754 Seed: 8030\n",
      "Testing: 0.6491289738162024 Training: 0.6444002581817155 Seed: 8032\n",
      "Testing: 0.658081358365246 Training: 0.6421895734647329 Seed: 8034\n",
      "Testing: 0.6470207804267039 Training: 0.6446713203035187 Seed: 8035\n",
      "Testing: 0.6565165065260808 Training: 0.6426557792276791 Seed: 8042\n",
      "Testing: 0.6486246120667494 Training: 0.6441274618255426 Seed: 8045\n",
      "Testing: 0.6478747606641145 Training: 0.6446920091424218 Seed: 8047\n",
      "Testing: 0.6774701391235599 Training: 0.6370112912729928 Seed: 8048\n",
      "Testing: 0.6501838085828529 Training: 0.6442770911603459 Seed: 8052\n",
      "Testing: 0.6545274272900862 Training: 0.6431083256142455 Seed: 8055\n",
      "Testing: 0.6495316779921317 Training: 0.6443903631745985 Seed: 8057\n",
      "Testing: 0.653654942048187 Training: 0.6431596387333289 Seed: 8061\n",
      "Testing: 0.6652222237851432 Training: 0.6403454619787801 Seed: 8062\n",
      "Testing: 0.6561942179355285 Training: 0.6427284652360908 Seed: 8064\n",
      "Testing: 0.648749863719495 Training: 0.6444368464605829 Seed: 8066\n",
      "Testing: 0.6477961186764254 Training: 0.6447162952519007 Seed: 8067\n",
      "Testing: 0.6477076337590287 Training: 0.6448127066298309 Seed: 8068\n",
      "Testing: 0.6533891477448067 Training: 0.6433358836848698 Seed: 8081\n",
      "Testing: 0.6474794723290602 Training: 0.6448843831166071 Seed: 8082\n",
      "Testing: 0.6527598202685179 Training: 0.643590520990983 Seed: 8084\n",
      "Testing: 0.6540725296564278 Training: 0.6432309404877842 Seed: 8085\n",
      "Testing: 0.6519919578136041 Training: 0.6438415374178832 Seed: 8087\n",
      "Testing: 0.6604519397685288 Training: 0.6414479520282034 Seed: 8088\n",
      "Testing: 0.6481946113027692 Training: 0.6446266489231244 Seed: 8091\n",
      "Testing: 0.6554322967691045 Training: 0.6428656015617569 Seed: 8093\n",
      "Testing: 0.6632910605470258 Training: 0.6407442316337449 Seed: 8095\n",
      "Testing: 0.6593820631656097 Training: 0.6418700898488403 Seed: 8098\n",
      "Testing: 0.652092725328064 Training: 0.6433097924715907 Seed: 8100\n",
      "Testing: 0.6574115803956544 Training: 0.6422901508514912 Seed: 8103\n",
      "Testing: 0.6531081266845112 Training: 0.6434555052964578 Seed: 8105\n",
      "Testing: 0.6498102963917793 Training: 0.6441591266557096 Seed: 8107\n",
      "Testing: 0.6517000951473931 Training: 0.6436064633965121 Seed: 8110\n",
      "Testing: 0.6537687325452897 Training: 0.6432975614818095 Seed: 8111\n",
      "Testing: 0.64584038783227 Training: 0.6452608332995289 Seed: 8112\n",
      "Testing: 0.6655289908370601 Training: 0.6403699178590551 Seed: 8114\n",
      "Testing: 0.6527195991603656 Training: 0.6434410923175823 Seed: 8115\n",
      "Testing: 0.6479558295079296 Training: 0.644720990247971 Seed: 8117\n",
      "Testing: 0.6454271244336871 Training: 0.6454208959371565 Seed: 8118\n",
      "Testing: 0.6473137108163434 Training: 0.644836548048531 Seed: 8119\n",
      "Testing: 0.6460308022112117 Training: 0.6452379151762577 Seed: 8122\n",
      "Testing: 0.6663550160239294 Training: 0.6398362781523438 Seed: 8123\n",
      "Testing: 0.6465741106750689 Training: 0.6451432590146525 Seed: 8127\n",
      "Testing: 0.6532401253604758 Training: 0.6432894092494822 Seed: 8128\n",
      "Testing: 0.655797631092827 Training: 0.6427711168207872 Seed: 8129\n",
      "Testing: 0.6476300024441971 Training: 0.6446731123083826 Seed: 8130\n",
      "Testing: 0.6468112215764218 Training: 0.6450424220792974 Seed: 8131\n",
      "Testing: 0.6467835906705864 Training: 0.6450030234941981 Seed: 8133\n",
      "Testing: 0.6493383677024906 Training: 0.6444192099637415 Seed: 8135\n",
      "Testing: 0.6470530829019105 Training: 0.6449866175888161 Seed: 8136\n",
      "Testing: 0.6603510264512877 Training: 0.6416190736883506 Seed: 8138\n",
      "Testing: 0.664926790903533 Training: 0.6404574487880896 Seed: 8139\n",
      "Testing: 0.649115915281177 Training: 0.6442989492627471 Seed: 8141\n",
      "Testing: 0.6466093743553726 Training: 0.645103175214512 Seed: 8144\n",
      "Testing: 0.6489498775260111 Training: 0.6444572807386272 Seed: 8152\n",
      "Testing: 0.6530717044334499 Training: 0.6434835933767447 Seed: 8153\n",
      "Testing: 0.6509257625781877 Training: 0.6440560728383984 Seed: 8156\n",
      "Testing: 0.6478799663298053 Training: 0.6447287467300805 Seed: 8157\n",
      "Testing: 0.653389736813578 Training: 0.6433943045100153 Seed: 8161\n",
      "Testing: 0.6507899942061572 Training: 0.6440581028483388 Seed: 8162\n",
      "Testing: 0.6496731625548676 Training: 0.6442942365625347 Seed: 8164\n",
      "Testing: 0.6468270866531494 Training: 0.6450815483512509 Seed: 8165\n",
      "Testing: 0.6453719951579117 Training: 0.6453325668056119 Seed: 8166\n",
      "Testing: 0.6597844539496228 Training: 0.6417314446201363 Seed: 8167\n",
      "Testing: 0.6524513791549302 Training: 0.6436866477054342 Seed: 8169\n",
      "Testing: 0.6532651117584863 Training: 0.6432076077515163 Seed: 8172\n",
      "Testing: 0.6613666751875674 Training: 0.6414812739695377 Seed: 8174\n",
      "Testing: 0.6702425383335172 Training: 0.6389063325161162 Seed: 8175\n",
      "Testing: 0.6476653801881057 Training: 0.6447881281028854 Seed: 8176\n",
      "Testing: 0.6519452427541249 Training: 0.6435847775750454 Seed: 8180\n",
      "Testing: 0.6539758546781987 Training: 0.6430915539960862 Seed: 8182\n",
      "Testing: 0.6467065990331601 Training: 0.645054638719616 Seed: 8183\n",
      "Testing: 0.645734144981008 Training: 0.6451943886149161 Seed: 8187\n",
      "Testing: 0.6554971597760882 Training: 0.6427812482581949 Seed: 8191\n",
      "Testing: 0.6594469588884848 Training: 0.641912081166154 Seed: 8196\n",
      "Testing: 0.6560030507437202 Training: 0.64271441655894 Seed: 8197\n",
      "Testing: 0.6597266731013587 Training: 0.6418968092395794 Seed: 8199\n",
      "Testing: 0.6514663100795042 Training: 0.6438381094719504 Seed: 8201\n",
      "Testing: 0.6672938861143607 Training: 0.6395954531848476 Seed: 8203\n",
      "Testing: 0.6612004221134866 Training: 0.6414541563167383 Seed: 8208\n",
      "Testing: 0.64832072394499 Training: 0.6446840755312979 Seed: 8214\n",
      "Testing: 0.6639049509520034 Training: 0.6406121698383467 Seed: 8215\n",
      "Testing: 0.6463743499579184 Training: 0.6451068623413159 Seed: 8217\n",
      "Testing: 0.648345015815007 Training: 0.6441605048436166 Seed: 8219\n",
      "Testing: 0.6460075374989076 Training: 0.6451189670800225 Seed: 8221\n",
      "Testing: 0.6623829157518165 Training: 0.6410836660451392 Seed: 8223\n",
      "Testing: 0.6542365676818803 Training: 0.6432206464514261 Seed: 8226\n",
      "Testing: 0.66142120720045 Training: 0.6414665223731685 Seed: 8228\n",
      "Testing: 0.6499129407302755 Training: 0.6441539805250269 Seed: 8230\n",
      "Testing: 0.6488632391048321 Training: 0.6444562536185516 Seed: 8231\n",
      "Testing: 0.6543320832343006 Training: 0.6433378979271921 Seed: 8232\n",
      "Testing: 0.6537485672582353 Training: 0.6433194374298411 Seed: 8234\n",
      "Testing: 0.6575046984903568 Training: 0.642485543507457 Seed: 8236\n",
      "Testing: 0.6692080585512615 Training: 0.6392496150462534 Seed: 8237\n",
      "Testing: 0.6532251398858774 Training: 0.6434459669029271 Seed: 8238\n",
      "Testing: 0.6533019834579713 Training: 0.6435189885956976 Seed: 8239\n",
      "Testing: 0.6538049098913569 Training: 0.6433240871761999 Seed: 8240\n",
      "Testing: 0.6580244271447164 Training: 0.6422717582554308 Seed: 8244\n",
      "Testing: 0.6526684289763202 Training: 0.6436000822820598 Seed: 8245\n",
      "Testing: 0.6483679391863586 Training: 0.6445832033469067 Seed: 8246\n",
      "Testing: 0.6529705709620415 Training: 0.6433610713499556 Seed: 8247\n",
      "Testing: 0.6505492567831918 Training: 0.6441269642273709 Seed: 8248\n",
      "Testing: 0.6569238676532668 Training: 0.6423070036072707 Seed: 8250\n",
      "Testing: 0.6576095152556788 Training: 0.6422250719324085 Seed: 8251\n",
      "Testing: 0.6570886875571641 Training: 0.6423468803685842 Seed: 8253\n",
      "Testing: 0.6455415212046083 Training: 0.6453123449453755 Seed: 8258\n",
      "Testing: 0.6507352350701043 Training: 0.6439774482529113 Seed: 8259\n",
      "Testing: 0.6475291195970699 Training: 0.6448280701045412 Seed: 8260\n",
      "Testing: 0.6490391063251051 Training: 0.6445016816666657 Seed: 8262\n",
      "Testing: 0.6492989629891819 Training: 0.6444101132744535 Seed: 8264\n",
      "Testing: 0.6460073049575754 Training: 0.6452691179129935 Seed: 8266\n",
      "Testing: 0.6625177417629107 Training: 0.6409396611741561 Seed: 8268\n",
      "Testing: 0.6482036853240963 Training: 0.6444014000926838 Seed: 8270\n",
      "Testing: 0.6465958973517156 Training: 0.6449952544684016 Seed: 8271\n",
      "Testing: 0.6605307176010059 Training: 0.6415369226447153 Seed: 8274\n",
      "Testing: 0.6595700791030489 Training: 0.6415672749674548 Seed: 8275\n",
      "Testing: 0.6533770427959386 Training: 0.643392136823367 Seed: 8276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.65616035008727 Training: 0.6425875886690586 Seed: 8279\n",
      "Testing: 0.6665075488837355 Training: 0.6398941737393729 Seed: 8281\n",
      "Testing: 0.6571349849667045 Training: 0.6420334917644797 Seed: 8282\n",
      "Testing: 0.647691089035975 Training: 0.644795911164207 Seed: 8283\n",
      "Testing: 0.6481580470595721 Training: 0.6447340398976694 Seed: 8287\n",
      "Testing: 0.6463954209555298 Training: 0.6451734954891789 Seed: 8289\n",
      "Testing: 0.6509297185009981 Training: 0.6439887252793374 Seed: 8291\n",
      "Testing: 0.6461987023123839 Training: 0.6452149829771753 Seed: 8292\n",
      "Testing: 0.6488474643724719 Training: 0.6445376456728682 Seed: 8294\n",
      "Testing: 0.664168976987839 Training: 0.6406302541908737 Seed: 8295\n",
      "Testing: 0.663101230626572 Training: 0.6406499411990387 Seed: 8296\n",
      "Testing: 0.6566248125552602 Training: 0.6424358270513568 Seed: 8299\n",
      "Testing: 0.6517842904015563 Training: 0.6438743431764444 Seed: 8301\n",
      "Testing: 0.6457794205536354 Training: 0.6453370950454353 Seed: 8303\n",
      "Testing: 0.6553960986733477 Training: 0.642784345484867 Seed: 8308\n",
      "Testing: 0.6594115237576595 Training: 0.6415364936904571 Seed: 8309\n",
      "Testing: 0.6562640336065875 Training: 0.6426816261343067 Seed: 8311\n",
      "Testing: 0.654611572849358 Training: 0.6431123398034915 Seed: 8312\n",
      "Testing: 0.6536129471438585 Training: 0.6432197950212517 Seed: 8313\n",
      "Testing: 0.6472707514951476 Training: 0.6446804345799895 Seed: 8315\n",
      "Testing: 0.6501874196193215 Training: 0.6441180758888261 Seed: 8317\n",
      "Testing: 0.6544957228168092 Training: 0.643087804144922 Seed: 8318\n",
      "Testing: 0.6553616019998649 Training: 0.6429109014628828 Seed: 8321\n",
      "Testing: 0.6541272240488574 Training: 0.6432202527027271 Seed: 8322\n",
      "Testing: 0.6499619411411741 Training: 0.6442617785659789 Seed: 8323\n",
      "Testing: 0.6569970955703436 Training: 0.6423836624886257 Seed: 8325\n",
      "Testing: 0.6573023507105296 Training: 0.6424653041437367 Seed: 8329\n",
      "Testing: 0.6586264639856607 Training: 0.6419068481248286 Seed: 8335\n",
      "Testing: 0.6616133155793873 Training: 0.6413943770112076 Seed: 8337\n",
      "Testing: 0.6493938147426096 Training: 0.6443849411522965 Seed: 8338\n",
      "Testing: 0.6536861718153651 Training: 0.6432195587235943 Seed: 8339\n",
      "Testing: 0.6633861785657902 Training: 0.6404111794151643 Seed: 8343\n",
      "Testing: 0.6469885918719864 Training: 0.6447932442638052 Seed: 8344\n",
      "Testing: 0.6540277274303347 Training: 0.6432359960562462 Seed: 8352\n",
      "Testing: 0.6510221798074842 Training: 0.6438855860178732 Seed: 8356\n",
      "Testing: 0.6508908514605524 Training: 0.6436941928139108 Seed: 8357\n",
      "Testing: 0.6532157831630412 Training: 0.6434663943497207 Seed: 8360\n",
      "Testing: 0.664448049799486 Training: 0.6405536873625892 Seed: 8364\n",
      "Testing: 0.6590686859059683 Training: 0.6417973150176604 Seed: 8365\n",
      "Testing: 0.650737986740268 Training: 0.6440442434162365 Seed: 8366\n",
      "Testing: 0.6464520580257843 Training: 0.6450238616960758 Seed: 8372\n",
      "Testing: 0.6487135990257097 Training: 0.6444648656958706 Seed: 8373\n",
      "Testing: 0.6491024005937398 Training: 0.6443789128958739 Seed: 8374\n",
      "Testing: 0.6609809041583221 Training: 0.6412627830685503 Seed: 8376\n",
      "Testing: 0.6510473885012261 Training: 0.643934690440444 Seed: 8378\n",
      "Testing: 0.6512113045278405 Training: 0.6439480360901233 Seed: 8380\n",
      "Testing: 0.6575377293797536 Training: 0.642450661910662 Seed: 8382\n",
      "Testing: 0.6496559373585046 Training: 0.644222069271838 Seed: 8383\n",
      "Testing: 0.658347864095058 Training: 0.6422194085699299 Seed: 8386\n",
      "Testing: 0.6475203284955824 Training: 0.6448315723956184 Seed: 8389\n",
      "Testing: 0.6493072510579277 Training: 0.6437123687614086 Seed: 8394\n",
      "Testing: 0.6509660314701982 Training: 0.6438673017813106 Seed: 8395\n",
      "Testing: 0.6625366196435962 Training: 0.6412217142894565 Seed: 8397\n",
      "Testing: 0.6535438326162621 Training: 0.6433710877938661 Seed: 8398\n",
      "Testing: 0.6620851109574346 Training: 0.6412510784686444 Seed: 8399\n",
      "Testing: 0.653816090666832 Training: 0.6431492592869121 Seed: 8402\n",
      "Testing: 0.6548339282558101 Training: 0.6428241395953518 Seed: 8404\n",
      "Testing: 0.658770453143082 Training: 0.641936511517913 Seed: 8410\n",
      "Testing: 0.651239751335876 Training: 0.6438921545650247 Seed: 8411\n",
      "Testing: 0.6620140129306793 Training: 0.6411456970040665 Seed: 8413\n",
      "Testing: 0.6612493440355696 Training: 0.6412624776007222 Seed: 8421\n",
      "Testing: 0.6579655875444639 Training: 0.6420625900160652 Seed: 8423\n",
      "Testing: 0.652709274854239 Training: 0.6433804756911057 Seed: 8424\n",
      "Testing: 0.6667729239261899 Training: 0.6401029115326429 Seed: 8426\n",
      "Testing: 0.6495844184577582 Training: 0.6442740181328261 Seed: 8428\n",
      "Testing: 0.6533670028671595 Training: 0.6433999846974165 Seed: 8429\n",
      "Testing: 0.6467021869690252 Training: 0.6450899063857105 Seed: 8431\n",
      "Testing: 0.6480326229615039 Training: 0.6444177338874859 Seed: 8432\n",
      "Testing: 0.6494225723147959 Training: 0.6442885915248815 Seed: 8433\n",
      "Testing: 0.6579675627703336 Training: 0.642329134402438 Seed: 8435\n",
      "Testing: 0.648400548694685 Training: 0.6446287430183107 Seed: 8437\n",
      "Testing: 0.6601158964219807 Training: 0.6416473019486351 Seed: 8439\n",
      "Testing: 0.6508224632962514 Training: 0.6438852105289345 Seed: 8441\n",
      "Testing: 0.6539491236766192 Training: 0.6431468979540618 Seed: 8442\n",
      "Testing: 0.6465565649825822 Training: 0.6450743746108496 Seed: 8445\n",
      "Testing: 0.6595645948888164 Training: 0.641969100816686 Seed: 8447\n",
      "Testing: 0.6515489120212228 Training: 0.6437038914362676 Seed: 8448\n",
      "Testing: 0.6539967231959187 Training: 0.6431110734120484 Seed: 8449\n",
      "Testing: 0.6558087303858877 Training: 0.6429016487090218 Seed: 8451\n",
      "Testing: 0.6480989638660137 Training: 0.644562497520432 Seed: 8453\n",
      "Testing: 0.653165489226731 Training: 0.6433988211019414 Seed: 8456\n",
      "Testing: 0.6477967875481274 Training: 0.6448213861950124 Seed: 8457\n",
      "Testing: 0.6466306769682024 Training: 0.6449993767182922 Seed: 8459\n",
      "Testing: 0.6557978115510212 Training: 0.6428557186628551 Seed: 8460\n",
      "Testing: 0.6480302662652967 Training: 0.6446561019595791 Seed: 8462\n",
      "Testing: 0.6487164311453503 Training: 0.6445977656782158 Seed: 8464\n",
      "Testing: 0.649533928118647 Training: 0.6443800663091626 Seed: 8465\n",
      "Testing: 0.6489154874852141 Training: 0.6445667008913865 Seed: 8467\n",
      "Testing: 0.6556090877887675 Training: 0.6427326749638604 Seed: 8469\n",
      "Testing: 0.6537948536502973 Training: 0.64330704733798 Seed: 8470\n",
      "Testing: 0.6617994300607726 Training: 0.6412448998609339 Seed: 8472\n",
      "Testing: 0.6482404888790287 Training: 0.6446903367813938 Seed: 8473\n",
      "Testing: 0.6595152265612714 Training: 0.6417393050900475 Seed: 8474\n",
      "Testing: 0.6486550865477332 Training: 0.6445261764765479 Seed: 8477\n",
      "Testing: 0.6548925519463259 Training: 0.6429874705597627 Seed: 8479\n",
      "Testing: 0.6506650504928277 Training: 0.6440377299290366 Seed: 8480\n",
      "Testing: 0.6675482430593284 Training: 0.6396841623353557 Seed: 8482\n",
      "Testing: 0.6491410689799244 Training: 0.6443206306336513 Seed: 8484\n",
      "Testing: 0.6611957503300534 Training: 0.641398425238982 Seed: 8485\n",
      "Testing: 0.6463157556580112 Training: 0.6452115858127839 Seed: 8490\n",
      "Testing: 0.6609991413372212 Training: 0.6413920186167912 Seed: 8491\n",
      "Testing: 0.6680825152868964 Training: 0.6397372757595707 Seed: 8493\n",
      "Testing: 0.6469770426602856 Training: 0.6445997895781663 Seed: 8495\n",
      "Testing: 0.6510062020421162 Training: 0.6440283314711023 Seed: 8496\n",
      "Testing: 0.6581547475282353 Training: 0.64209633387456 Seed: 8500\n",
      "Testing: 0.6469176485377887 Training: 0.6450122095379135 Seed: 8503\n",
      "Testing: 0.6536392031152005 Training: 0.6433047850719917 Seed: 8506\n",
      "Testing: 0.6533644865388019 Training: 0.6434236258986208 Seed: 8513\n",
      "Testing: 0.6544092740291381 Training: 0.6431150263798983 Seed: 8514\n",
      "Testing: 0.6494242724029635 Training: 0.6443780445391133 Seed: 8518\n",
      "Testing: 0.651287847402577 Training: 0.6440052132704288 Seed: 8519\n",
      "Testing: 0.6535937985489728 Training: 0.6433961599382044 Seed: 8521\n",
      "Testing: 0.6569856768759385 Training: 0.6424749205648976 Seed: 8522\n",
      "Testing: 0.6487748447428685 Training: 0.6445360118795425 Seed: 8524\n",
      "Testing: 0.6535238194353282 Training: 0.6432551273894116 Seed: 8526\n",
      "Testing: 0.6476163305174356 Training: 0.6448297112527632 Seed: 8528\n",
      "Testing: 0.6518614265851178 Training: 0.6436860431588947 Seed: 8529\n",
      "Testing: 0.6464590700537978 Training: 0.6451371939527255 Seed: 8532\n",
      "Testing: 0.6461817062338185 Training: 0.6451357944763201 Seed: 8535\n",
      "Testing: 0.6456657018704668 Training: 0.6453490562235796 Seed: 8537\n",
      "Testing: 0.6531492921817891 Training: 0.6434122521545236 Seed: 8538\n",
      "Testing: 0.6556992058754533 Training: 0.642666421052158 Seed: 8542\n",
      "Testing: 0.6506865464379881 Training: 0.644098927063505 Seed: 8543\n",
      "Testing: 0.6562987437115044 Training: 0.6425945165229627 Seed: 8544\n",
      "Testing: 0.6533984590287734 Training: 0.6432364764124174 Seed: 8545\n",
      "Testing: 0.6506095377734622 Training: 0.6440255289787775 Seed: 8547\n",
      "Testing: 0.6503721875767574 Training: 0.644164518510731 Seed: 8548\n",
      "Testing: 0.6508725009675544 Training: 0.6441353001096298 Seed: 8549\n",
      "Testing: 0.6477456835854971 Training: 0.6447420443372098 Seed: 8550\n",
      "Testing: 0.6565179128072455 Training: 0.6424638954726953 Seed: 8553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6614507556815513 Training: 0.6413948159037495 Seed: 8554\n",
      "Testing: 0.6590653614597778 Training: 0.6417890857140369 Seed: 8557\n",
      "Testing: 0.646327518945631 Training: 0.6443073325110287 Seed: 8558\n",
      "Testing: 0.6528910452322048 Training: 0.6435664550605222 Seed: 8562\n",
      "Testing: 0.6509644390758792 Training: 0.6440538574174838 Seed: 8563\n",
      "Testing: 0.6567253826138422 Training: 0.6425839188542724 Seed: 8564\n",
      "Testing: 0.6754332549363894 Training: 0.6378684292260779 Seed: 8565\n",
      "Testing: 0.6704263312846852 Training: 0.6389643157568279 Seed: 8567\n",
      "Testing: 0.6467931043561387 Training: 0.6450410332235533 Seed: 8568\n",
      "Testing: 0.6502532403810137 Training: 0.644144563167667 Seed: 8569\n",
      "Testing: 0.6624364925684219 Training: 0.6408950877033681 Seed: 8570\n",
      "Testing: 0.6521208316592592 Training: 0.6435672537638093 Seed: 8571\n",
      "Testing: 0.6528571019202571 Training: 0.6433479683056857 Seed: 8574\n",
      "Testing: 0.6510451158410248 Training: 0.6439939675919424 Seed: 8576\n",
      "Testing: 0.6688478550211466 Training: 0.6392601101297359 Seed: 8577\n",
      "Testing: 0.6654089648471407 Training: 0.6400049977481996 Seed: 8580\n",
      "Testing: 0.6468254600206145 Training: 0.6450359353213274 Seed: 8584\n",
      "Testing: 0.6611000542245058 Training: 0.641387306063984 Seed: 8586\n",
      "Testing: 0.6540823533956386 Training: 0.642616032259173 Seed: 8588\n",
      "Testing: 0.6488603337978527 Training: 0.6440548130939154 Seed: 8589\n",
      "Testing: 0.6545403814867092 Training: 0.6431202933055629 Seed: 8590\n",
      "Testing: 0.6566720105311207 Training: 0.6425438156640221 Seed: 8594\n",
      "Testing: 0.6485872601552565 Training: 0.6445124131254605 Seed: 8596\n",
      "Testing: 0.6539221289092776 Training: 0.6432491954717177 Seed: 8597\n",
      "Testing: 0.6459672215700776 Training: 0.6452019226104317 Seed: 8598\n",
      "Testing: 0.6473830808140819 Training: 0.6445100891426581 Seed: 8599\n",
      "Testing: 0.6549319573316612 Training: 0.6429626353426108 Seed: 8603\n",
      "Testing: 0.6570840934158629 Training: 0.6425000292879345 Seed: 8604\n",
      "Testing: 0.656768753743776 Training: 0.6424439540045996 Seed: 8606\n",
      "Testing: 0.6619920748269578 Training: 0.6411656047004528 Seed: 8607\n",
      "Testing: 0.6485604036774273 Training: 0.6445916181364209 Seed: 8608\n",
      "Testing: 0.6546776303540222 Training: 0.6430565845466895 Seed: 8612\n",
      "Testing: 0.6467513759774868 Training: 0.6450845171278099 Seed: 8613\n",
      "Testing: 0.6545961429910919 Training: 0.643164296889092 Seed: 8617\n",
      "Testing: 0.6564654063008212 Training: 0.6425625880416828 Seed: 8620\n",
      "Testing: 0.6463093080356669 Training: 0.6450023085084186 Seed: 8622\n",
      "Testing: 0.6501304498066665 Training: 0.6441055362670071 Seed: 8623\n",
      "Testing: 0.6470706257484161 Training: 0.6448638404937257 Seed: 8624\n",
      "Testing: 0.6503483578142818 Training: 0.6441395333224551 Seed: 8627\n",
      "Testing: 0.6492438411756609 Training: 0.6444121849662369 Seed: 8630\n",
      "Testing: 0.6525250643582269 Training: 0.6436352139864858 Seed: 8631\n",
      "Testing: 0.6470588829360003 Training: 0.6449641209185502 Seed: 8632\n",
      "Testing: 0.6471385179739713 Training: 0.6449780472464222 Seed: 8633\n",
      "Testing: 0.6581497416410467 Training: 0.6423047598994848 Seed: 8634\n",
      "Testing: 0.6538769185714456 Training: 0.643063847674488 Seed: 8635\n",
      "Testing: 0.6550378252475055 Training: 0.6429610807342016 Seed: 8643\n",
      "Testing: 0.6616005383081086 Training: 0.6411104640969651 Seed: 8645\n",
      "Testing: 0.6524935180260735 Training: 0.6433863418124008 Seed: 8648\n",
      "Testing: 0.6464919314793227 Training: 0.644959882190451 Seed: 8650\n",
      "Testing: 0.6514508135103009 Training: 0.6438155558462717 Seed: 8652\n",
      "Testing: 0.6482471822092549 Training: 0.6445852919546556 Seed: 8655\n",
      "Testing: 0.6570736123391536 Training: 0.6424035466724525 Seed: 8657\n",
      "Testing: 0.6493340783120842 Training: 0.6442986537004386 Seed: 8659\n",
      "Testing: 0.6556534102557608 Training: 0.6429788423700912 Seed: 8662\n",
      "Testing: 0.6555648720281424 Training: 0.6427846576235039 Seed: 8664\n",
      "Testing: 0.6538791484384634 Training: 0.6433493143996759 Seed: 8667\n",
      "Testing: 0.6466178863723778 Training: 0.6450189741866461 Seed: 8672\n",
      "Testing: 0.6661620210437875 Training: 0.6402390965125828 Seed: 8674\n",
      "Testing: 0.6482660106874325 Training: 0.6446454517949494 Seed: 8675\n",
      "Testing: 0.6532232301923181 Training: 0.6434205298610375 Seed: 8678\n",
      "Testing: 0.6499909753116092 Training: 0.6441632517181966 Seed: 8679\n",
      "Testing: 0.6517284693827355 Training: 0.6437326630859217 Seed: 8680\n",
      "Testing: 0.6601811796795569 Training: 0.6414784424940547 Seed: 8682\n",
      "Testing: 0.654894723010905 Training: 0.643135566612794 Seed: 8683\n",
      "Testing: 0.6608674206614084 Training: 0.6415816832205692 Seed: 8686\n",
      "Testing: 0.6538955740112682 Training: 0.6433120608757235 Seed: 8687\n",
      "Testing: 0.6457366117840005 Training: 0.6452717803949028 Seed: 8688\n",
      "Testing: 0.6458696776744683 Training: 0.6451437954223115 Seed: 8691\n",
      "Testing: 0.6489621653753044 Training: 0.6443585957321296 Seed: 8694\n",
      "Testing: 0.649090741084469 Training: 0.6445172751948862 Seed: 8697\n",
      "Testing: 0.6487412307222507 Training: 0.6445873655994034 Seed: 8699\n",
      "Testing: 0.6529372452511193 Training: 0.6432744850614371 Seed: 8701\n",
      "Testing: 0.6549930445507798 Training: 0.6430164949356245 Seed: 8702\n",
      "Testing: 0.6470477614216097 Training: 0.6448804600592097 Seed: 8703\n",
      "Testing: 0.6598358370491857 Training: 0.6418858363239116 Seed: 8705\n",
      "Testing: 0.6558951337648469 Training: 0.642836521007819 Seed: 8706\n",
      "Testing: 0.6572642982658178 Training: 0.642168539500707 Seed: 8710\n",
      "Testing: 0.6505876186113886 Training: 0.6441071737432882 Seed: 8711\n",
      "Testing: 0.6452950315089199 Training: 0.6452120852071414 Seed: 8712\n",
      "Testing: 0.6531024394791438 Training: 0.6433904092274679 Seed: 8714\n",
      "Testing: 0.6543499430812185 Training: 0.6430568787442432 Seed: 8715\n",
      "Testing: 0.6634981999972462 Training: 0.6408890738589128 Seed: 8716\n",
      "Testing: 0.6461429041553168 Training: 0.6451599554591206 Seed: 8722\n",
      "Testing: 0.6487995808637943 Training: 0.6444656510927171 Seed: 8723\n",
      "Testing: 0.6480987259684208 Training: 0.6447394377823294 Seed: 8725\n",
      "Testing: 0.6578263891083005 Training: 0.6422193535871614 Seed: 8726\n",
      "Testing: 0.6511761580809019 Training: 0.6435271382705774 Seed: 8728\n",
      "Testing: 0.6628251303450543 Training: 0.6411456197663952 Seed: 8730\n",
      "Testing: 0.6568669297125056 Training: 0.6425078435720571 Seed: 8731\n",
      "Testing: 0.6461799241231373 Training: 0.6450242179674344 Seed: 8733\n",
      "Testing: 0.6572014807220599 Training: 0.6423520160752254 Seed: 8736\n",
      "Testing: 0.6704421804963857 Training: 0.6391830440786935 Seed: 8738\n",
      "Testing: 0.6583081134553443 Training: 0.6422361183004122 Seed: 8740\n",
      "Testing: 0.6514373578368374 Training: 0.6439197234427194 Seed: 8741\n",
      "Testing: 0.6519263702899014 Training: 0.6436929027320455 Seed: 8743\n",
      "Testing: 0.6534996089112153 Training: 0.6432751186137824 Seed: 8744\n",
      "Testing: 0.6473396819020022 Training: 0.6448175378769478 Seed: 8748\n",
      "Testing: 0.6514267974617983 Training: 0.6438713525885844 Seed: 8751\n",
      "Testing: 0.6455033951792967 Training: 0.6453638286217394 Seed: 8753\n",
      "Testing: 0.6668650926961402 Training: 0.6400370703500163 Seed: 8754\n",
      "Testing: 0.6671859192910908 Training: 0.639832397398679 Seed: 8758\n",
      "Testing: 0.6519663281588843 Training: 0.643694069719147 Seed: 8762\n",
      "Testing: 0.6484524066911375 Training: 0.6446687048749657 Seed: 8763\n",
      "Testing: 0.6566452711254818 Training: 0.6427341699896266 Seed: 8768\n",
      "Testing: 0.6514148984552064 Training: 0.6436743925952217 Seed: 8769\n",
      "Testing: 0.6454266679484806 Training: 0.6452610993940312 Seed: 8770\n",
      "Testing: 0.6519794849955338 Training: 0.6436877841623515 Seed: 8773\n",
      "Testing: 0.652429508773346 Training: 0.6437012283114538 Seed: 8774\n",
      "Testing: 0.656716779932056 Training: 0.6423946641743801 Seed: 8782\n",
      "Testing: 0.6487716413871434 Training: 0.644504174493078 Seed: 8784\n",
      "Testing: 0.6480024420024648 Training: 0.6447424017427772 Seed: 8786\n",
      "Testing: 0.6531103102283388 Training: 0.6434213733068028 Seed: 8787\n",
      "Testing: 0.6630473851140952 Training: 0.6409373127415261 Seed: 8789\n",
      "Testing: 0.6464424162229179 Training: 0.6450743312292218 Seed: 8794\n",
      "Testing: 0.6454524670568813 Training: 0.6452687215245418 Seed: 8795\n",
      "Testing: 0.6596326446487955 Training: 0.6417379912778725 Seed: 8796\n",
      "Testing: 0.6502479479519646 Training: 0.6441371024839758 Seed: 8798\n",
      "Testing: 0.6460646632619876 Training: 0.6452137732435147 Seed: 8802\n",
      "Testing: 0.6494143610681963 Training: 0.644221841490723 Seed: 8805\n",
      "Testing: 0.654443774793614 Training: 0.6430494340954682 Seed: 8806\n",
      "Testing: 0.6539648506775694 Training: 0.64320189038119 Seed: 8809\n",
      "Testing: 0.6559328011629535 Training: 0.6427463806134022 Seed: 8810\n",
      "Testing: 0.6585397264793962 Training: 0.6421321362708287 Seed: 8811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6595446901156434 Training: 0.6416234432041816 Seed: 8813\n",
      "Testing: 0.6584850281881469 Training: 0.6419268209572027 Seed: 8815\n",
      "Testing: 0.6595823520537016 Training: 0.6419318902238689 Seed: 8816\n",
      "Testing: 0.6515384873727946 Training: 0.6438869558625979 Seed: 8817\n",
      "Testing: 0.6523498944159063 Training: 0.6434394542034503 Seed: 8819\n",
      "Testing: 0.6454831457623205 Training: 0.6453401186937412 Seed: 8820\n",
      "Testing: 0.6526591923897802 Training: 0.643601481809129 Seed: 8821\n",
      "Testing: 0.6468259049431925 Training: 0.6449535902982269 Seed: 8823\n",
      "Testing: 0.6726535360358304 Training: 0.6386626088585171 Seed: 8829\n",
      "Testing: 0.6488917597796064 Training: 0.644569382279231 Seed: 8832\n",
      "Testing: 0.647511425980825 Training: 0.6447891634539573 Seed: 8833\n",
      "Testing: 0.6501365479691744 Training: 0.6442439292395886 Seed: 8838\n",
      "Testing: 0.6480129576814329 Training: 0.6445541882265338 Seed: 8839\n",
      "Testing: 0.6468537629766226 Training: 0.6449675895403748 Seed: 8840\n",
      "Testing: 0.6468080868557335 Training: 0.6444243728840063 Seed: 8841\n",
      "Testing: 0.650510959649718 Training: 0.6440634885218002 Seed: 8842\n",
      "Testing: 0.6539499157570956 Training: 0.6432447268428153 Seed: 8843\n",
      "Testing: 0.6459636009084062 Training: 0.6452707373614133 Seed: 8848\n",
      "Testing: 0.6458659748010571 Training: 0.6451080834814256 Seed: 8850\n",
      "Testing: 0.6462160178810812 Training: 0.6451351836118061 Seed: 8852\n",
      "Testing: 0.649572696475144 Training: 0.6443378853271493 Seed: 8854\n",
      "Testing: 0.6458004665586808 Training: 0.6453321854051042 Seed: 8858\n",
      "Testing: 0.6651922528712216 Training: 0.6403869500673879 Seed: 8859\n",
      "Testing: 0.6498241938202137 Training: 0.6442700271424219 Seed: 8862\n",
      "Testing: 0.6470709990857985 Training: 0.644987515255252 Seed: 8865\n",
      "Testing: 0.647367960295965 Training: 0.644908618190774 Seed: 8866\n",
      "Testing: 0.6517754623339815 Training: 0.6437957960147066 Seed: 8868\n",
      "Testing: 0.6671115207129837 Training: 0.6402023360823743 Seed: 8869\n",
      "Testing: 0.655520057454682 Training: 0.6426940012058404 Seed: 8875\n",
      "Testing: 0.6565674459038497 Training: 0.6425544424761938 Seed: 8877\n",
      "Testing: 0.6539618899033404 Training: 0.6432106944625142 Seed: 8879\n",
      "Testing: 0.6584010775213239 Training: 0.6420502231711628 Seed: 8880\n",
      "Testing: 0.6480455589854682 Training: 0.6447680885634286 Seed: 8882\n",
      "Testing: 0.6573049958547413 Training: 0.6421395515150601 Seed: 8885\n",
      "Testing: 0.6458057455865593 Training: 0.645289679740516 Seed: 8886\n",
      "Testing: 0.6529637525385887 Training: 0.6436129645772405 Seed: 8888\n",
      "Testing: 0.6533512568179237 Training: 0.6432687663568157 Seed: 8890\n",
      "Testing: 0.6508485366473211 Training: 0.6438713158789511 Seed: 8891\n",
      "Testing: 0.6539476218452176 Training: 0.6429469707798676 Seed: 8893\n",
      "Testing: 0.6536501356332478 Training: 0.6433256517984425 Seed: 8895\n",
      "Testing: 0.6606421886594686 Training: 0.6415265882130867 Seed: 8898\n",
      "Testing: 0.6546276863463136 Training: 0.6429994250849098 Seed: 8903\n",
      "Testing: 0.6467531563272559 Training: 0.6450127557498803 Seed: 8905\n",
      "Testing: 0.6660748993891084 Training: 0.640305564364755 Seed: 8907\n",
      "Testing: 0.6616572416947981 Training: 0.6411794789563725 Seed: 8908\n",
      "Testing: 0.6561277125465443 Training: 0.6425307590426298 Seed: 8909\n",
      "Testing: 0.6459908267698997 Training: 0.6452118435687522 Seed: 8911\n",
      "Testing: 0.6457073290302724 Training: 0.6452692872822041 Seed: 8914\n",
      "Testing: 0.6496827118347128 Training: 0.644255270762274 Seed: 8918\n",
      "Testing: 0.6559613326019446 Training: 0.6427228180300695 Seed: 8923\n",
      "Testing: 0.6480856585314301 Training: 0.6439790031235888 Seed: 8924\n",
      "Testing: 0.657223180084973 Training: 0.6422979763976949 Seed: 8925\n",
      "Testing: 0.6643318989511788 Training: 0.6407520848936252 Seed: 8929\n",
      "Testing: 0.6579058238277303 Training: 0.6420757978007461 Seed: 8933\n",
      "Testing: 0.6473674637934933 Training: 0.6448794577081471 Seed: 8935\n",
      "Testing: 0.646767755595383 Training: 0.6448971722402485 Seed: 8938\n",
      "Testing: 0.6613592819087424 Training: 0.6416330071834886 Seed: 8939\n",
      "Testing: 0.6516840591216251 Training: 0.6437622922333548 Seed: 8940\n",
      "Testing: 0.6486455975918607 Training: 0.6445275877623544 Seed: 8941\n",
      "Testing: 0.6489647059338118 Training: 0.6444207949589806 Seed: 8943\n",
      "Testing: 0.6472236766067009 Training: 0.6449110326862847 Seed: 8944\n",
      "Testing: 0.6595165709282345 Training: 0.6420211674002326 Seed: 8945\n",
      "Testing: 0.6533428857190084 Training: 0.6434363124075867 Seed: 8946\n",
      "Testing: 0.6513099810983979 Training: 0.6438312881368811 Seed: 8950\n",
      "Testing: 0.6622943124889411 Training: 0.6412342698587643 Seed: 8951\n",
      "Testing: 0.6550764940944367 Training: 0.6430269507575821 Seed: 8956\n",
      "Testing: 0.6487694992989702 Training: 0.6443800006676041 Seed: 8957\n",
      "Testing: 0.6684131071892386 Training: 0.6395873149834659 Seed: 8959\n",
      "Testing: 0.6529689078054025 Training: 0.6435290188370951 Seed: 8960\n",
      "Testing: 0.6534714112508471 Training: 0.6432400224428055 Seed: 8965\n",
      "Testing: 0.6519585980170982 Training: 0.6436035897898166 Seed: 8970\n",
      "Testing: 0.6468430918132194 Training: 0.6448801274806578 Seed: 8973\n",
      "Testing: 0.6486358120533768 Training: 0.6445620409768633 Seed: 8974\n",
      "Testing: 0.6543495280197194 Training: 0.6431163634900954 Seed: 8976\n",
      "Testing: 0.6539896558620454 Training: 0.6432189835100119 Seed: 8977\n",
      "Testing: 0.6601801721389209 Training: 0.6414258171573624 Seed: 8979\n",
      "Testing: 0.662053944686236 Training: 0.6412716586020093 Seed: 8980\n",
      "Testing: 0.6461192437548728 Training: 0.645059446369606 Seed: 8982\n",
      "Testing: 0.6605687167857157 Training: 0.6414806955948367 Seed: 8983\n",
      "Testing: 0.6570563129510703 Training: 0.6424391490654183 Seed: 8985\n",
      "Testing: 0.6458152941670801 Training: 0.645279330705301 Seed: 8986\n",
      "Testing: 0.6557514429102775 Training: 0.6427119284315558 Seed: 8988\n",
      "Testing: 0.6454171714840699 Training: 0.6453531851765494 Seed: 8990\n",
      "Testing: 0.6566058342115567 Training: 0.6425844884261134 Seed: 8992\n",
      "Testing: 0.6460582778454704 Training: 0.6451590538379638 Seed: 8993\n",
      "Testing: 0.648074572486625 Training: 0.6446286661522875 Seed: 8994\n",
      "Testing: 0.6761419391441498 Training: 0.6374112645104532 Seed: 8996\n",
      "Testing: 0.6570218175461424 Training: 0.6424252271418334 Seed: 8999\n",
      "Testing: 0.6650321081319853 Training: 0.640253351369064 Seed: 9000\n",
      "Testing: 0.6660648092748278 Training: 0.6402404277848845 Seed: 9002\n",
      "Testing: 0.6478470939377973 Training: 0.6447378475758928 Seed: 9008\n",
      "Testing: 0.6560326935509739 Training: 0.6426812060452239 Seed: 9012\n",
      "Testing: 0.6506948779454211 Training: 0.6440016462271286 Seed: 9013\n",
      "Testing: 0.6585654481046265 Training: 0.6419093518252109 Seed: 9015\n",
      "Testing: 0.6488783039211451 Training: 0.644361566810752 Seed: 9017\n",
      "Testing: 0.6561138403814766 Training: 0.64269990595758 Seed: 9018\n",
      "Testing: 0.6468508129090806 Training: 0.6450268777172068 Seed: 9021\n",
      "Testing: 0.6495377149816924 Training: 0.6441613688691084 Seed: 9023\n",
      "Testing: 0.6501423548401537 Training: 0.6442472262759582 Seed: 9024\n",
      "Testing: 0.6603366767406964 Training: 0.6410201886449182 Seed: 9025\n",
      "Testing: 0.6520770011414795 Training: 0.6437599619611478 Seed: 9026\n",
      "Testing: 0.6470378977397844 Training: 0.644976377111289 Seed: 9028\n",
      "Testing: 0.645289135199685 Training: 0.6452584391595382 Seed: 9029\n",
      "Testing: 0.6576580986573616 Training: 0.6421984717310361 Seed: 9030\n",
      "Testing: 0.6529950669554476 Training: 0.6434124039232838 Seed: 9033\n",
      "Testing: 0.6540588728067653 Training: 0.6431600721144815 Seed: 9034\n",
      "Testing: 0.6548330186415793 Training: 0.6430334968084138 Seed: 9035\n",
      "Testing: 0.663719226690688 Training: 0.6407021918111271 Seed: 9037\n",
      "Testing: 0.6545230226646034 Training: 0.6430464418886683 Seed: 9038\n",
      "Testing: 0.6539152743952552 Training: 0.6432450095775548 Seed: 9040\n",
      "Testing: 0.6555061236491668 Training: 0.6429100487666446 Seed: 9041\n",
      "Testing: 0.6503156062393807 Training: 0.6441498902509644 Seed: 9044\n",
      "Testing: 0.6459534995394808 Training: 0.6452660452230197 Seed: 9047\n",
      "Testing: 0.6518554674412543 Training: 0.6437809801371697 Seed: 9052\n",
      "Testing: 0.6537648733372494 Training: 0.6433312330776331 Seed: 9053\n",
      "Testing: 0.6509410447229846 Training: 0.6440403363573342 Seed: 9055\n",
      "Testing: 0.6507455550184508 Training: 0.6440518659267257 Seed: 9057\n",
      "Testing: 0.6572983054929316 Training: 0.6425207386586885 Seed: 9059\n",
      "Testing: 0.6624119872645726 Training: 0.6411755216004306 Seed: 9062\n",
      "Testing: 0.6478484754427554 Training: 0.6448205676231007 Seed: 9065\n",
      "Testing: 0.6479200800129441 Training: 0.6447969745801911 Seed: 9066\n",
      "Testing: 0.6715595035302844 Training: 0.6387693880351639 Seed: 9068\n",
      "Testing: 0.6529136506071124 Training: 0.6435617206179697 Seed: 9069\n",
      "Testing: 0.65061272974635 Training: 0.6440169848324546 Seed: 9071\n",
      "Testing: 0.6589058275497478 Training: 0.6420418792732845 Seed: 9072\n",
      "Testing: 0.6568770102918038 Training: 0.6424244435157302 Seed: 9074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.64610673604928 Training: 0.6451639012437858 Seed: 9075\n",
      "Testing: 0.6461285393537279 Training: 0.6450024817168117 Seed: 9076\n",
      "Testing: 0.6583411320545652 Training: 0.6421681960065906 Seed: 9078\n",
      "Testing: 0.67034523525052 Training: 0.6391118974138617 Seed: 9079\n",
      "Testing: 0.6648407542810683 Training: 0.6397669729543067 Seed: 9081\n",
      "Testing: 0.6558742832486884 Training: 0.6426198886316699 Seed: 9082\n",
      "Testing: 0.6455470837392228 Training: 0.6451159754613363 Seed: 9083\n",
      "Testing: 0.6525541995667603 Training: 0.6435465774122275 Seed: 9084\n",
      "Testing: 0.6525387158634136 Training: 0.6435135443884348 Seed: 9085\n",
      "Testing: 0.6526330295428002 Training: 0.6436140740076193 Seed: 9086\n",
      "Testing: 0.6505255858197154 Training: 0.6441329605142261 Seed: 9087\n",
      "Testing: 0.6503240810428488 Training: 0.6439252678811251 Seed: 9088\n",
      "Testing: 0.6506081952094898 Training: 0.6441368006012392 Seed: 9089\n",
      "Testing: 0.6576528519978981 Training: 0.6420307796480016 Seed: 9090\n",
      "Testing: 0.6605538680954928 Training: 0.6415981180218722 Seed: 9091\n",
      "Testing: 0.6454264561592429 Training: 0.6452011218560286 Seed: 9092\n",
      "Testing: 0.6494470549267782 Training: 0.6442905508967345 Seed: 9096\n",
      "Testing: 0.6463480118785514 Training: 0.6452004242398806 Seed: 9098\n",
      "Testing: 0.6530848163387445 Training: 0.6435345794961582 Seed: 9101\n",
      "Testing: 0.6559278037076541 Training: 0.642868708183312 Seed: 9102\n",
      "Testing: 0.6454339724150397 Training: 0.6454066514767344 Seed: 9104\n",
      "Testing: 0.6482010286547396 Training: 0.6447483654192864 Seed: 9106\n",
      "Testing: 0.6686645158953419 Training: 0.639265052125552 Seed: 9108\n",
      "Testing: 0.6567508958707302 Training: 0.6424603164065544 Seed: 9109\n",
      "Testing: 0.6587347573663268 Training: 0.6416641027463192 Seed: 9112\n",
      "Testing: 0.6473248557984616 Training: 0.644919961087888 Seed: 9114\n",
      "Testing: 0.6538064680958467 Training: 0.6431455366962766 Seed: 9116\n",
      "Testing: 0.6537907891595502 Training: 0.6433693958871513 Seed: 9119\n",
      "Testing: 0.653948835462145 Training: 0.643228627534305 Seed: 9122\n",
      "Testing: 0.6500435465405507 Training: 0.6442534667570667 Seed: 9123\n",
      "Testing: 0.6557843886397459 Training: 0.6428651172066557 Seed: 9126\n",
      "Testing: 0.6681058580067478 Training: 0.6393602508157403 Seed: 9128\n",
      "Testing: 0.6503815313504869 Training: 0.644220570445589 Seed: 9129\n",
      "Testing: 0.6462262145806225 Training: 0.6451885188409783 Seed: 9130\n",
      "Testing: 0.6537747544010355 Training: 0.6431718326472786 Seed: 9133\n",
      "Testing: 0.6517731394611175 Training: 0.643776265190676 Seed: 9136\n",
      "Testing: 0.6480574388891751 Training: 0.6447566728406757 Seed: 9137\n",
      "Testing: 0.6465508628674325 Training: 0.6449957738226657 Seed: 9138\n",
      "Testing: 0.652042967506564 Training: 0.6437158527676614 Seed: 9140\n",
      "Testing: 0.6601426893953589 Training: 0.6416419235230533 Seed: 9142\n",
      "Testing: 0.6532987830011903 Training: 0.6434552018134423 Seed: 9148\n",
      "Testing: 0.6591638892311156 Training: 0.6420284945367153 Seed: 9150\n",
      "Testing: 0.6462315321685251 Training: 0.6451481041974935 Seed: 9153\n",
      "Testing: 0.6674594663565542 Training: 0.6395395568330708 Seed: 9154\n",
      "Testing: 0.64951433339109 Training: 0.644256812723039 Seed: 9155\n",
      "Testing: 0.652123768677798 Training: 0.6437436578482396 Seed: 9157\n",
      "Testing: 0.6506852413306691 Training: 0.6439320394532915 Seed: 9158\n",
      "Testing: 0.6635629422915701 Training: 0.6408027749015957 Seed: 9161\n",
      "Testing: 0.6490271462765124 Training: 0.6444694400621664 Seed: 9164\n",
      "Testing: 0.6576447389071673 Training: 0.642194710772674 Seed: 9165\n",
      "Testing: 0.660929539060169 Training: 0.6413542156374359 Seed: 9167\n",
      "Testing: 0.6518170247906736 Training: 0.6437034247827169 Seed: 9168\n",
      "Testing: 0.6473615705388038 Training: 0.6448369442455723 Seed: 9173\n",
      "Testing: 0.6578368624982854 Training: 0.6422502304771156 Seed: 9175\n",
      "Testing: 0.6515219825453125 Training: 0.6437974612992134 Seed: 9176\n",
      "Testing: 0.6588744788507748 Training: 0.6419332721497184 Seed: 9178\n",
      "Testing: 0.6499397687583905 Training: 0.6440934157776897 Seed: 9179\n",
      "Testing: 0.6471225637659839 Training: 0.6449158425399696 Seed: 9180\n",
      "Testing: 0.6482506405837434 Training: 0.6445436673107843 Seed: 9181\n",
      "Testing: 0.6515881992645861 Training: 0.6437973986558183 Seed: 9184\n",
      "Testing: 0.6557811620969474 Training: 0.642753531278053 Seed: 9185\n",
      "Testing: 0.6618799631107627 Training: 0.6411953897698199 Seed: 9186\n",
      "Testing: 0.6612760183011578 Training: 0.6415332840370551 Seed: 9190\n",
      "Testing: 0.6516275324508658 Training: 0.6437741971335482 Seed: 9191\n",
      "Testing: 0.6602998381771377 Training: 0.6417275586629219 Seed: 9192\n",
      "Testing: 0.6665579275277334 Training: 0.6397769655369436 Seed: 9194\n",
      "Testing: 0.649813206003917 Training: 0.6443024694362528 Seed: 9196\n",
      "Testing: 0.6500120653634195 Training: 0.6442012852312323 Seed: 9199\n",
      "Testing: 0.6510504045384391 Training: 0.6439240631475781 Seed: 9201\n",
      "Testing: 0.6599069736499805 Training: 0.6418105515372662 Seed: 9203\n",
      "Testing: 0.6518267812791619 Training: 0.6437376067816816 Seed: 9204\n",
      "Testing: 0.6469440862751126 Training: 0.645058589164116 Seed: 9205\n",
      "Testing: 0.6482455783192517 Training: 0.6446765373948147 Seed: 9208\n",
      "Testing: 0.647117850479406 Training: 0.6449969251455744 Seed: 9209\n",
      "Testing: 0.6613906802562225 Training: 0.6412753607485926 Seed: 9210\n",
      "Testing: 0.6478168098042049 Training: 0.6447859604792646 Seed: 9211\n",
      "Testing: 0.6556991241758119 Training: 0.6428458114936624 Seed: 9212\n",
      "Testing: 0.6539953351063821 Training: 0.6428875486880927 Seed: 9215\n",
      "Testing: 0.6486538922082672 Training: 0.6445853816055852 Seed: 9218\n",
      "Testing: 0.651520937190913 Training: 0.6437855420400853 Seed: 9223\n",
      "Testing: 0.650554152857969 Training: 0.6441016411965643 Seed: 9226\n",
      "Testing: 0.6566167768734337 Training: 0.642629696614756 Seed: 9229\n",
      "Testing: 0.654096680145273 Training: 0.6431708736284183 Seed: 9231\n",
      "Testing: 0.6464832003082747 Training: 0.6451616875387254 Seed: 9234\n",
      "Testing: 0.6587303206480714 Training: 0.6420203502395597 Seed: 9239\n",
      "Testing: 0.654837011939977 Training: 0.6430211343919543 Seed: 9242\n",
      "Testing: 0.6565511786965709 Training: 0.6424917858793572 Seed: 9244\n",
      "Testing: 0.6504521279639415 Training: 0.6441567313226828 Seed: 9245\n",
      "Testing: 0.6590409456235266 Training: 0.6418538288376765 Seed: 9246\n",
      "Testing: 0.654918772626633 Training: 0.64299423179682 Seed: 9247\n",
      "Testing: 0.6468673247888063 Training: 0.6450349041742507 Seed: 9248\n",
      "Testing: 0.6575362116624963 Training: 0.6422960921073598 Seed: 9252\n",
      "Testing: 0.6507458543586844 Training: 0.6438926785348738 Seed: 9254\n",
      "Testing: 0.6658230583753715 Training: 0.6402019363901168 Seed: 9255\n",
      "Testing: 0.6550432607713863 Training: 0.642929383699165 Seed: 9257\n",
      "Testing: 0.6468051835671217 Training: 0.644933546013261 Seed: 9261\n",
      "Testing: 0.6583349453853231 Training: 0.64163484641663 Seed: 9262\n",
      "Testing: 0.6465232903054486 Training: 0.6449816218967422 Seed: 9264\n",
      "Testing: 0.6468714515281846 Training: 0.6449418521371381 Seed: 9266\n",
      "Testing: 0.6471061721682692 Training: 0.6449444014795015 Seed: 9276\n",
      "Testing: 0.6666241604424276 Training: 0.6402696099363887 Seed: 9277\n",
      "Testing: 0.6525382119696735 Training: 0.6435084435622608 Seed: 9278\n",
      "Testing: 0.6523080948739578 Training: 0.6433685554906586 Seed: 9281\n",
      "Testing: 0.6587491787384044 Training: 0.6419484668553862 Seed: 9283\n",
      "Testing: 0.6477084723046246 Training: 0.6447976356248062 Seed: 9284\n",
      "Testing: 0.6591064545226131 Training: 0.6419022887869634 Seed: 9286\n",
      "Testing: 0.6552383140051447 Training: 0.64286303859601 Seed: 9288\n",
      "Testing: 0.6548307791737569 Training: 0.6430797085338813 Seed: 9289\n",
      "Testing: 0.6559246807495284 Training: 0.642778306909319 Seed: 9292\n",
      "Testing: 0.6503167737238703 Training: 0.6440700240257695 Seed: 9293\n",
      "Testing: 0.6483791039371043 Training: 0.6445511420130068 Seed: 9298\n",
      "Testing: 0.6507229933240551 Training: 0.6440649315117924 Seed: 9300\n",
      "Testing: 0.6465717045841035 Training: 0.6451372408776006 Seed: 9301\n",
      "Testing: 0.6465976742817974 Training: 0.6450072878504175 Seed: 9302\n",
      "Testing: 0.6529412810580072 Training: 0.6434627013759094 Seed: 9303\n",
      "Testing: 0.6589364865861066 Training: 0.6420296920897104 Seed: 9304\n",
      "Testing: 0.6688038903064643 Training: 0.6394433279357649 Seed: 9309\n",
      "Testing: 0.6599721445565826 Training: 0.6418705150075162 Seed: 9311\n",
      "Testing: 0.6550299307741487 Training: 0.642717879999396 Seed: 9312\n",
      "Testing: 0.6485481091313395 Training: 0.644623419301281 Seed: 9314\n",
      "Testing: 0.6596336049687949 Training: 0.6418391589336341 Seed: 9315\n",
      "Testing: 0.651385959433445 Training: 0.6438886838804004 Seed: 9317\n",
      "Testing: 0.6504802718955028 Training: 0.6435982004262953 Seed: 9319\n",
      "Testing: 0.6471078130165417 Training: 0.6449581131262994 Seed: 9320\n",
      "Testing: 0.653262734183979 Training: 0.6434002164822759 Seed: 9323\n",
      "Testing: 0.6653629073648323 Training: 0.6402237194885319 Seed: 9326\n",
      "Testing: 0.6490155777818722 Training: 0.6444767141783059 Seed: 9329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6461063059267134 Training: 0.6452303322500627 Seed: 9331\n",
      "Testing: 0.6529598029247977 Training: 0.6434477538210307 Seed: 9332\n",
      "Testing: 0.6585433485354861 Training: 0.6421071994012744 Seed: 9334\n",
      "Testing: 0.6467293428673111 Training: 0.6450445501103382 Seed: 9340\n",
      "Testing: 0.659087069469549 Training: 0.6418397326731416 Seed: 9341\n",
      "Testing: 0.6454796383732379 Training: 0.6453742582590931 Seed: 9342\n",
      "Testing: 0.6515319686849114 Training: 0.6438333973287749 Seed: 9350\n",
      "Testing: 0.6557424866120041 Training: 0.642627857908756 Seed: 9351\n",
      "Testing: 0.6523465870727198 Training: 0.6437067492545446 Seed: 9353\n",
      "Testing: 0.6599174707802842 Training: 0.6415579542340641 Seed: 9357\n",
      "Testing: 0.6572035092030779 Training: 0.6423787185982217 Seed: 9359\n",
      "Testing: 0.6645549333215814 Training: 0.6405957139926157 Seed: 9360\n",
      "Testing: 0.6457712780418925 Training: 0.6452926195629618 Seed: 9361\n",
      "Testing: 0.6498085835423929 Training: 0.6442610625387958 Seed: 9363\n",
      "Testing: 0.6568082181791758 Training: 0.6424728001609892 Seed: 9365\n",
      "Testing: 0.6510068447280699 Training: 0.6439366454387282 Seed: 9367\n",
      "Testing: 0.6524170648320209 Training: 0.6436653304819613 Seed: 9370\n",
      "Testing: 0.6510269339256264 Training: 0.6439658415451648 Seed: 9371\n",
      "Testing: 0.6619909481088142 Training: 0.6410326907079957 Seed: 9372\n",
      "Testing: 0.662304869041549 Training: 0.6410878508171293 Seed: 9374\n",
      "Testing: 0.6459653266283771 Training: 0.6452468277809588 Seed: 9379\n",
      "Testing: 0.6520741744521545 Training: 0.6436710720802046 Seed: 9382\n",
      "Testing: 0.6515001878713895 Training: 0.6439289965894365 Seed: 9385\n",
      "Testing: 0.6580280459706037 Training: 0.6421880463718904 Seed: 9389\n",
      "Testing: 0.6521999384554692 Training: 0.6436267631235492 Seed: 9391\n",
      "Testing: 0.6483503781209352 Training: 0.6445907053682217 Seed: 9397\n",
      "Testing: 0.6510544577769225 Training: 0.643982567693817 Seed: 9398\n",
      "Testing: 0.6503750718357089 Training: 0.6441272923809498 Seed: 9399\n",
      "Testing: 0.659177926424417 Training: 0.6420663050092329 Seed: 9400\n",
      "Testing: 0.6538984359137803 Training: 0.6430703416295052 Seed: 9401\n",
      "Testing: 0.6503032876526947 Training: 0.644139132292634 Seed: 9403\n",
      "Testing: 0.6502060025002425 Training: 0.643955081227074 Seed: 9404\n",
      "Testing: 0.6618484373776412 Training: 0.6412578656706713 Seed: 9405\n",
      "Testing: 0.6571622897867462 Training: 0.6426235266082302 Seed: 9406\n",
      "Testing: 0.6505519164778268 Training: 0.6440417415842585 Seed: 9408\n",
      "Testing: 0.6512564907682059 Training: 0.643955387441099 Seed: 9413\n",
      "Testing: 0.6569061767065203 Training: 0.6425758894956436 Seed: 9414\n",
      "Testing: 0.6574839442614877 Training: 0.6424440807409648 Seed: 9415\n",
      "Testing: 0.6523787562642785 Training: 0.6434612738273757 Seed: 9418\n",
      "Testing: 0.6473097621000006 Training: 0.6449744188754023 Seed: 9421\n",
      "Testing: 0.6614988711413041 Training: 0.6412109757726586 Seed: 9424\n",
      "Testing: 0.6487102035413232 Training: 0.644636200912558 Seed: 9427\n",
      "Testing: 0.6461011408882676 Training: 0.6450836533852521 Seed: 9428\n",
      "Testing: 0.6487919863655024 Training: 0.6444641057634711 Seed: 9429\n",
      "Testing: 0.6589318468330921 Training: 0.6419399305990121 Seed: 9431\n",
      "Testing: 0.6474551590397376 Training: 0.6448386035977887 Seed: 9432\n",
      "Testing: 0.6504269494491419 Training: 0.6441472460379216 Seed: 9433\n",
      "Testing: 0.6505770714597194 Training: 0.6438855996216998 Seed: 9434\n",
      "Testing: 0.6457547167657991 Training: 0.6452511279115019 Seed: 9436\n",
      "Testing: 0.6474685277445913 Training: 0.6448201667286317 Seed: 9438\n",
      "Testing: 0.6620131837564992 Training: 0.6408822640899452 Seed: 9440\n",
      "Testing: 0.6471318756222317 Training: 0.6449626514736491 Seed: 9441\n",
      "Testing: 0.6484345243642335 Training: 0.6445766795950116 Seed: 9442\n",
      "Testing: 0.6485828918909082 Training: 0.6445889407933436 Seed: 9443\n",
      "Testing: 0.65176671696139 Training: 0.6437738680850814 Seed: 9446\n",
      "Testing: 0.6583275136823471 Training: 0.6421254072387498 Seed: 9448\n",
      "Testing: 0.6615654199721255 Training: 0.6415332810535672 Seed: 9450\n",
      "Testing: 0.6608863783155876 Training: 0.6415910222387177 Seed: 9451\n",
      "Testing: 0.6451777981572895 Training: 0.6451726716598499 Seed: 9452\n",
      "Testing: 0.6505245225457222 Training: 0.6438706550250991 Seed: 9453\n",
      "Testing: 0.6508587185633337 Training: 0.6441028958539687 Seed: 9454\n",
      "Testing: 0.6466206101703305 Training: 0.6450167329366014 Seed: 9455\n",
      "Testing: 0.6486286741542123 Training: 0.644284011352673 Seed: 9457\n",
      "Testing: 0.6462419700904989 Training: 0.6449289867577038 Seed: 9458\n",
      "Testing: 0.6466977156886669 Training: 0.6450892632617922 Seed: 9467\n",
      "Testing: 0.6544093726992601 Training: 0.6430392428000372 Seed: 9469\n",
      "Testing: 0.6491923089693359 Training: 0.6444132956117313 Seed: 9470\n",
      "Testing: 0.6482268919797078 Training: 0.6445110659703499 Seed: 9473\n",
      "Testing: 0.65356360605796 Training: 0.6431479305032026 Seed: 9475\n",
      "Testing: 0.6575393113609 Training: 0.6422988990296272 Seed: 9477\n",
      "Testing: 0.6464927768614963 Training: 0.6451791747551658 Seed: 9479\n",
      "Testing: 0.6517333321249777 Training: 0.6437832561448211 Seed: 9483\n",
      "Testing: 0.654600289476269 Training: 0.6430159327945711 Seed: 9488\n",
      "Testing: 0.6671503714856437 Training: 0.6399086019344415 Seed: 9490\n",
      "Testing: 0.6532353143853764 Training: 0.6434018119953115 Seed: 9493\n",
      "Testing: 0.6507890342690366 Training: 0.6440116528711795 Seed: 9495\n",
      "Testing: 0.6509872955875429 Training: 0.6439032803729146 Seed: 9496\n",
      "Testing: 0.6458055505310798 Training: 0.6452570069459971 Seed: 9497\n",
      "Testing: 0.650720843747589 Training: 0.6439527090482113 Seed: 9498\n",
      "Testing: 0.6522702545959587 Training: 0.6436263709099815 Seed: 9499\n",
      "Testing: 0.6518460793272749 Training: 0.6436204154372953 Seed: 9500\n",
      "Testing: 0.6552297268755068 Training: 0.6427846823024347 Seed: 9505\n",
      "Testing: 0.6595767311386974 Training: 0.6417071803398813 Seed: 9507\n",
      "Testing: 0.6647694090394627 Training: 0.6403748642933116 Seed: 9508\n",
      "Testing: 0.6454293126218652 Training: 0.6453704548333857 Seed: 9512\n",
      "Testing: 0.650561554551134 Training: 0.6439383587416962 Seed: 9515\n",
      "Testing: 0.653797935407742 Training: 0.6432091445771393 Seed: 9516\n",
      "Testing: 0.6591914834103332 Training: 0.6417913705816534 Seed: 9521\n",
      "Testing: 0.663894080993597 Training: 0.6406237788773047 Seed: 9523\n",
      "Testing: 0.651654343006224 Training: 0.6438519411129003 Seed: 9524\n",
      "Testing: 0.6546538288148975 Training: 0.6431009405339778 Seed: 9525\n",
      "Testing: 0.648952677090354 Training: 0.6445650460939164 Seed: 9526\n",
      "Testing: 0.6567778314535443 Training: 0.6424681654538338 Seed: 9529\n",
      "Testing: 0.6563582147037498 Training: 0.6426776002832526 Seed: 9535\n",
      "Testing: 0.6576329094497341 Training: 0.6423234440714982 Seed: 9537\n",
      "Testing: 0.6485732761917589 Training: 0.6445673349443914 Seed: 9538\n",
      "Testing: 0.6588597760365147 Training: 0.6420210294200361 Seed: 9539\n",
      "Testing: 0.6492489296908621 Training: 0.6443314885088638 Seed: 9542\n",
      "Testing: 0.6488555994150065 Training: 0.6444926355579198 Seed: 9543\n",
      "Testing: 0.6555194657597405 Training: 0.6427901534098643 Seed: 9544\n",
      "Testing: 0.6490057775180682 Training: 0.6444158598201991 Seed: 9545\n",
      "Testing: 0.6491503214926111 Training: 0.6444038618477759 Seed: 9546\n",
      "Testing: 0.6607115791555626 Training: 0.6415916607987513 Seed: 9548\n",
      "Testing: 0.6469935997245256 Training: 0.6449739338163454 Seed: 9549\n",
      "Testing: 0.6472456776904261 Training: 0.6449077227143468 Seed: 9553\n",
      "Testing: 0.6534615125721321 Training: 0.6434210917469947 Seed: 9554\n",
      "Testing: 0.6505502390255831 Training: 0.6441018180979694 Seed: 9556\n",
      "Testing: 0.6520232602137923 Training: 0.643721446016491 Seed: 9557\n",
      "Testing: 0.6569125306238225 Training: 0.6424909297661139 Seed: 9559\n",
      "Testing: 0.6491024366355174 Training: 0.6444457542591239 Seed: 9567\n",
      "Testing: 0.6495293317985037 Training: 0.6443420629961031 Seed: 9569\n",
      "Testing: 0.6561399120137758 Training: 0.6427593550815864 Seed: 9571\n",
      "Testing: 0.645656246976225 Training: 0.645274149125569 Seed: 9574\n",
      "Testing: 0.6576337945177203 Training: 0.6421444576643545 Seed: 9575\n",
      "Testing: 0.6612103562412497 Training: 0.6414835238962743 Seed: 9581\n",
      "Testing: 0.6656895725353265 Training: 0.6401103137352482 Seed: 9582\n",
      "Testing: 0.6523536777866522 Training: 0.6436216311570738 Seed: 9583\n",
      "Testing: 0.6571032239209607 Training: 0.6422620944567043 Seed: 9584\n",
      "Testing: 0.6528329458320591 Training: 0.6435732670710345 Seed: 9588\n",
      "Testing: 0.6535395057409479 Training: 0.6433292293197479 Seed: 9591\n",
      "Testing: 0.6553545586955575 Training: 0.642976542801938 Seed: 9596\n",
      "Testing: 0.6617157575281291 Training: 0.6412768891315507 Seed: 9598\n",
      "Testing: 0.6502060794263692 Training: 0.6440219332357406 Seed: 9599\n",
      "Testing: 0.6612566528274603 Training: 0.641313819556325 Seed: 9602\n",
      "Testing: 0.6460474544871849 Training: 0.645147899668645 Seed: 9606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6557296676689548 Training: 0.6429039364867802 Seed: 9609\n",
      "Testing: 0.6558332227569519 Training: 0.6427456964172693 Seed: 9610\n",
      "Testing: 0.6455954732733535 Training: 0.6453706399051018 Seed: 9611\n",
      "Testing: 0.655261900351652 Training: 0.6427251449655229 Seed: 9612\n",
      "Testing: 0.6497230048170282 Training: 0.6442830903896384 Seed: 9618\n",
      "Testing: 0.6601874636112262 Training: 0.6415636278407307 Seed: 9619\n",
      "Testing: 0.6512133865853622 Training: 0.6439063286351969 Seed: 9620\n",
      "Testing: 0.6455427703262544 Training: 0.6453648526461079 Seed: 9621\n",
      "Testing: 0.6510688779969775 Training: 0.6439468039541538 Seed: 9622\n",
      "Testing: 0.6650989889738894 Training: 0.6404600547245782 Seed: 9624\n",
      "Testing: 0.6683187047625421 Training: 0.6394391785715535 Seed: 9625\n",
      "Testing: 0.6568317776031827 Training: 0.6424673470992706 Seed: 9627\n",
      "Testing: 0.6480165779493874 Training: 0.6446964114859597 Seed: 9628\n",
      "Testing: 0.6476017169940551 Training: 0.6448368541691886 Seed: 9630\n",
      "Testing: 0.6524483642802495 Training: 0.6436661783853477 Seed: 9633\n",
      "Testing: 0.6534074605802336 Training: 0.6433895383182731 Seed: 9634\n",
      "Testing: 0.6507944737801926 Training: 0.6440917296472348 Seed: 9635\n",
      "Testing: 0.6510174917228064 Training: 0.6438211196322511 Seed: 9636\n",
      "Testing: 0.6572615761467178 Training: 0.642238538589633 Seed: 9639\n",
      "Testing: 0.6476823972477824 Training: 0.6448080302042473 Seed: 9643\n",
      "Testing: 0.6533353963458843 Training: 0.6435022294901487 Seed: 9644\n",
      "Testing: 0.6518826294481752 Training: 0.6437620774842324 Seed: 9649\n",
      "Testing: 0.6497421476627643 Training: 0.6443647200208128 Seed: 9654\n",
      "Testing: 0.6602589206525848 Training: 0.6414310879717985 Seed: 9661\n",
      "Testing: 0.6468090125194281 Training: 0.6449658466793671 Seed: 9662\n",
      "Testing: 0.6548103077070895 Training: 0.64299795543818 Seed: 9663\n",
      "Testing: 0.6524065774086126 Training: 0.6435426407093731 Seed: 9666\n",
      "Testing: 0.6463717609669887 Training: 0.6451468440592302 Seed: 9671\n",
      "Testing: 0.6573322388467875 Training: 0.642361942695987 Seed: 9673\n",
      "Testing: 0.652260301235929 Training: 0.6435318888886115 Seed: 9678\n",
      "Testing: 0.6515268554081558 Training: 0.6437539014327351 Seed: 9680\n",
      "Testing: 0.6563791506194419 Training: 0.6426911649071017 Seed: 9682\n",
      "Testing: 0.6499893278119926 Training: 0.643429450629744 Seed: 9683\n",
      "Testing: 0.6556283082700916 Training: 0.6427498458327084 Seed: 9684\n",
      "Testing: 0.6603816217334701 Training: 0.6415601486941115 Seed: 9686\n",
      "Testing: 0.6518796188262478 Training: 0.6436339236004837 Seed: 9688\n",
      "Testing: 0.6487223341958505 Training: 0.6444458774043116 Seed: 9690\n",
      "Testing: 0.6472877262318797 Training: 0.6445172795311185 Seed: 9691\n",
      "Testing: 0.6680666362276853 Training: 0.6395659013440631 Seed: 9692\n",
      "Testing: 0.663000382921973 Training: 0.6409445938421172 Seed: 9693\n",
      "Testing: 0.6592800088577597 Training: 0.6419779549030145 Seed: 9695\n",
      "Testing: 0.6463514102145582 Training: 0.6450527822860996 Seed: 9698\n",
      "Testing: 0.6481790317797917 Training: 0.644416462440345 Seed: 9699\n",
      "Testing: 0.6575542563165611 Training: 0.6423333198964595 Seed: 9702\n",
      "Testing: 0.6479769818633268 Training: 0.6447564504179562 Seed: 9703\n",
      "Testing: 0.6478128022155216 Training: 0.6447053919552753 Seed: 9705\n",
      "Testing: 0.6473785091228295 Training: 0.6448547510144349 Seed: 9706\n",
      "Testing: 0.6599781793530326 Training: 0.6417485870678385 Seed: 9709\n",
      "Testing: 0.6487190997280788 Training: 0.6443847234980575 Seed: 9713\n",
      "Testing: 0.657519106534782 Training: 0.6421329270722866 Seed: 9714\n",
      "Testing: 0.6455545836845182 Training: 0.6453657618768553 Seed: 9715\n",
      "Testing: 0.647595304856638 Training: 0.6447665136245986 Seed: 9716\n",
      "Testing: 0.6508503746334994 Training: 0.6438957480401928 Seed: 9718\n",
      "Testing: 0.6612220721615242 Training: 0.6415626393760797 Seed: 9724\n",
      "Testing: 0.6497238038916311 Training: 0.6442678261101764 Seed: 9725\n",
      "Testing: 0.6464470069812995 Training: 0.645130332352154 Seed: 9729\n",
      "Testing: 0.6479982972665576 Training: 0.6447237851329168 Seed: 9731\n",
      "Testing: 0.6583129933382297 Training: 0.6422887421398302 Seed: 9732\n",
      "Testing: 0.6486231629237387 Training: 0.644552476437582 Seed: 9734\n",
      "Testing: 0.6465927021440256 Training: 0.6448251302244912 Seed: 9735\n",
      "Testing: 0.6477298798623126 Training: 0.6448593519229572 Seed: 9736\n",
      "Testing: 0.6541842645406072 Training: 0.6431460768571757 Seed: 9737\n",
      "Testing: 0.651293988645985 Training: 0.6438408893552936 Seed: 9740\n",
      "Testing: 0.6608996592983638 Training: 0.6414422853101756 Seed: 9741\n",
      "Testing: 0.6513806945305235 Training: 0.6439240054935391 Seed: 9743\n",
      "Testing: 0.6467285222435838 Training: 0.6450026428132101 Seed: 9744\n",
      "Testing: 0.6567725035626758 Training: 0.6424417517863326 Seed: 9746\n",
      "Testing: 0.6559814388738862 Training: 0.6426554480457369 Seed: 9748\n",
      "Testing: 0.6465856723056547 Training: 0.6447816629350436 Seed: 9752\n",
      "Testing: 0.6526183088876181 Training: 0.6436130677118687 Seed: 9756\n",
      "Testing: 0.6527066878064446 Training: 0.6434878260071023 Seed: 9757\n",
      "Testing: 0.6650103542906464 Training: 0.6402748055636182 Seed: 9763\n",
      "Testing: 0.6491147370024294 Training: 0.644428965440728 Seed: 9764\n",
      "Testing: 0.647097436892217 Training: 0.6448310328538667 Seed: 9765\n",
      "Testing: 0.6508985135086195 Training: 0.6440572843884989 Seed: 9766\n",
      "Testing: 0.6470427040647028 Training: 0.6450213650947629 Seed: 9769\n",
      "Testing: 0.6481363245209493 Training: 0.6445915960934238 Seed: 9771\n",
      "Testing: 0.6511251778838374 Training: 0.6438117940153558 Seed: 9773\n",
      "Testing: 0.6590318101925458 Training: 0.6420610221280179 Seed: 9775\n",
      "Testing: 0.6478827324983095 Training: 0.6447554688532666 Seed: 9776\n",
      "Testing: 0.6542140526881368 Training: 0.6429207488251185 Seed: 9778\n",
      "Testing: 0.6473784148944074 Training: 0.6447667996177022 Seed: 9781\n",
      "Testing: 0.6507733480712996 Training: 0.6439794630725593 Seed: 9783\n",
      "Testing: 0.6552921768643541 Training: 0.6427986349574761 Seed: 9786\n",
      "Testing: 0.6461496155569674 Training: 0.6451422790046035 Seed: 9787\n",
      "Testing: 0.6616555833958411 Training: 0.6412268555456488 Seed: 9789\n",
      "Testing: 0.657664467869603 Training: 0.6421256078918613 Seed: 9790\n",
      "Testing: 0.6533121706252956 Training: 0.6432577807414841 Seed: 9792\n",
      "Testing: 0.6522528722317427 Training: 0.6430664396350506 Seed: 9793\n",
      "Testing: 0.6483321113400373 Training: 0.644638640179493 Seed: 9795\n",
      "Testing: 0.650708984781755 Training: 0.6440941316013823 Seed: 9797\n",
      "Testing: 0.6541438834392088 Training: 0.6430441797131299 Seed: 9800\n",
      "Testing: 0.6485639967733772 Training: 0.6445454445432147 Seed: 9802\n",
      "Testing: 0.6664493451760708 Training: 0.6399800295818117 Seed: 9808\n",
      "Testing: 0.6473850335919253 Training: 0.6447588670802769 Seed: 9811\n",
      "Testing: 0.6593027492948088 Training: 0.6417806187888123 Seed: 9813\n",
      "Testing: 0.6632338227189694 Training: 0.6406680946147763 Seed: 9814\n",
      "Testing: 0.6489500432060865 Training: 0.6442821319178395 Seed: 9816\n",
      "Testing: 0.657452771754623 Training: 0.642430974160183 Seed: 9820\n",
      "Testing: 0.6606372325077258 Training: 0.6414359199359045 Seed: 9822\n",
      "Testing: 0.6457599004840338 Training: 0.6449881611658468 Seed: 9824\n",
      "Testing: 0.6489161564374922 Training: 0.6445056898550477 Seed: 9829\n",
      "Testing: 0.6651658751416115 Training: 0.6401636856351456 Seed: 9830\n",
      "Testing: 0.646757490808602 Training: 0.6449441843990299 Seed: 9832\n",
      "Testing: 0.6604323846448789 Training: 0.6416717357957659 Seed: 9833\n",
      "Testing: 0.6492327017987449 Training: 0.6444251908185477 Seed: 9835\n",
      "Testing: 0.6567212673356583 Training: 0.6425953210336914 Seed: 9836\n",
      "Testing: 0.6475348271337102 Training: 0.6447588437668844 Seed: 9838\n",
      "Testing: 0.6637898036759977 Training: 0.6408353746081449 Seed: 9842\n",
      "Testing: 0.6572986332567378 Training: 0.6423267004556859 Seed: 9845\n",
      "Testing: 0.6569440691962567 Training: 0.6423550475417256 Seed: 9847\n",
      "Testing: 0.6540583630600894 Training: 0.6431961038487919 Seed: 9848\n",
      "Testing: 0.6462574095522651 Training: 0.6451464688399545 Seed: 9849\n",
      "Testing: 0.6584108745374151 Training: 0.6421942763683144 Seed: 9854\n",
      "Testing: 0.6549975659927699 Training: 0.6428764972306846 Seed: 9856\n",
      "Testing: 0.6492136657016521 Training: 0.6444835939930658 Seed: 9857\n",
      "Testing: 0.6463082527350337 Training: 0.6446557745368142 Seed: 9859\n",
      "Testing: 0.6473012499720595 Training: 0.6449205122306965 Seed: 9860\n",
      "Testing: 0.6608535131404086 Training: 0.6415355139118996 Seed: 9861\n",
      "Testing: 0.6487235501989399 Training: 0.6445436209726487 Seed: 9863\n",
      "Testing: 0.6514495277116211 Training: 0.6438593745464969 Seed: 9865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6525165877658304 Training: 0.643503590483855 Seed: 9870\n",
      "Testing: 0.6604007676207468 Training: 0.6414734361899426 Seed: 9871\n",
      "Testing: 0.6478368753290797 Training: 0.6448111415639877 Seed: 9872\n",
      "Testing: 0.6463929384556533 Training: 0.6452111900059972 Seed: 9873\n",
      "Testing: 0.6586658488110093 Training: 0.6417584311354989 Seed: 9874\n",
      "Testing: 0.6457706062850268 Training: 0.6452386361604985 Seed: 9876\n",
      "Testing: 0.6465421609441965 Training: 0.6445461208335411 Seed: 9878\n",
      "Testing: 0.6539026492330168 Training: 0.6430406783859012 Seed: 9879\n",
      "Testing: 0.6513894670910563 Training: 0.6439123531439864 Seed: 9880\n",
      "Testing: 0.6457582957847374 Training: 0.6453020000861615 Seed: 9883\n",
      "Testing: 0.6486214007456171 Training: 0.6444595242269844 Seed: 9885\n",
      "Testing: 0.6543713140475939 Training: 0.6430936204395944 Seed: 9886\n",
      "Testing: 0.6561072606275533 Training: 0.6427210555992781 Seed: 9889\n",
      "Testing: 0.6535288822391336 Training: 0.643295696529744 Seed: 9891\n",
      "Testing: 0.6463118622073619 Training: 0.645168783407709 Seed: 9893\n",
      "Testing: 0.6507671673716617 Training: 0.6439979149883615 Seed: 9894\n",
      "Testing: 0.6518581889535161 Training: 0.6437199972265134 Seed: 9895\n",
      "Testing: 0.6526096377443062 Training: 0.6434467725749526 Seed: 9897\n",
      "Testing: 0.6538597265711149 Training: 0.6428370071872209 Seed: 9898\n",
      "Testing: 0.6496664463120505 Training: 0.6441860369924053 Seed: 9901\n",
      "Testing: 0.6459427910064259 Training: 0.6451878998084916 Seed: 9902\n",
      "Testing: 0.6607056014482491 Training: 0.6414791321628153 Seed: 9907\n",
      "Testing: 0.6548367101933463 Training: 0.6430567663806865 Seed: 9908\n",
      "Testing: 0.6575120982427891 Training: 0.6425560390345728 Seed: 9909\n",
      "Testing: 0.6466809036766092 Training: 0.6450785257447477 Seed: 9910\n",
      "Testing: 0.6577235975505802 Training: 0.6422394241234411 Seed: 9912\n",
      "Testing: 0.651795966332703 Training: 0.643802934369064 Seed: 9915\n",
      "Testing: 0.6482648652482212 Training: 0.6446827623085346 Seed: 9916\n",
      "Testing: 0.6521169317685398 Training: 0.6436724474128009 Seed: 9918\n",
      "Testing: 0.6558703158985986 Training: 0.6427684503920135 Seed: 9919\n",
      "Testing: 0.6540313535960077 Training: 0.6430117456411788 Seed: 9922\n",
      "Testing: 0.6499160073946746 Training: 0.6442528414463276 Seed: 9923\n",
      "Testing: 0.6470721857481032 Training: 0.6450011425006081 Seed: 9924\n",
      "Testing: 0.648540337932301 Training: 0.6439770007043821 Seed: 9926\n",
      "Testing: 0.6481091874051046 Training: 0.643763790309239 Seed: 9927\n",
      "Testing: 0.6469281289919693 Training: 0.6450326462803507 Seed: 9928\n",
      "Testing: 0.6558080685160953 Training: 0.6426557222048874 Seed: 9931\n",
      "Testing: 0.6623508430547741 Training: 0.641149758281683 Seed: 9933\n",
      "Testing: 0.6482376955784616 Training: 0.6446232450944592 Seed: 9935\n",
      "Testing: 0.6479183027872306 Training: 0.6444167394323448 Seed: 9936\n",
      "Testing: 0.6466703030728875 Training: 0.6447585526459952 Seed: 9937\n",
      "Testing: 0.6534908117440423 Training: 0.6430932870640722 Seed: 9938\n",
      "Testing: 0.6532422869317306 Training: 0.6434422141150917 Seed: 9939\n",
      "Testing: 0.6482623721041049 Training: 0.6445978381504875 Seed: 9940\n",
      "Testing: 0.6469820624993109 Training: 0.6449624450615833 Seed: 9943\n",
      "Testing: 0.6515796915453691 Training: 0.6436176616544391 Seed: 9944\n",
      "Testing: 0.6491057691837043 Training: 0.6443540309523845 Seed: 9945\n",
      "Testing: 0.6589132637182098 Training: 0.6419170194101418 Seed: 9947\n",
      "Testing: 0.651734212901294 Training: 0.6438826848071597 Seed: 9948\n",
      "Testing: 0.6572703854014308 Training: 0.6425036786860793 Seed: 9949\n",
      "Testing: 0.6530500167598234 Training: 0.643450686559272 Seed: 9951\n",
      "Testing: 0.6529579019465 Training: 0.6435375978890276 Seed: 9952\n",
      "Testing: 0.6600094375829016 Training: 0.6417601393376131 Seed: 9953\n",
      "Testing: 0.6594330380537399 Training: 0.641766668925458 Seed: 9957\n",
      "Testing: 0.656054634396047 Training: 0.642727887381038 Seed: 9958\n",
      "Testing: 0.6498749643935638 Training: 0.644266706151049 Seed: 9959\n",
      "Testing: 0.6477018734415729 Training: 0.6447367386331886 Seed: 9960\n",
      "Testing: 0.6508709713416704 Training: 0.644038519485155 Seed: 9961\n",
      "Testing: 0.647446560715065 Training: 0.6448171417065279 Seed: 9963\n",
      "Testing: 0.6541439057781343 Training: 0.6430810872027155 Seed: 9964\n",
      "Testing: 0.6544142022254097 Training: 0.6430277680534163 Seed: 9965\n",
      "Testing: 0.6516997048732818 Training: 0.6438803170420745 Seed: 9966\n",
      "Testing: 0.6611213255247526 Training: 0.64145955583848 Seed: 9968\n",
      "Testing: 0.6632350243999184 Training: 0.6409778830837645 Seed: 9970\n",
      "Testing: 0.6563484262385386 Training: 0.6426106302461485 Seed: 9972\n",
      "Testing: 0.6533588962731867 Training: 0.6434389721367342 Seed: 9976\n",
      "Testing: 0.6684934629537781 Training: 0.6398051690083361 Seed: 9978\n",
      "Testing: 0.6596648365927055 Training: 0.6419451521422299 Seed: 9980\n",
      "Testing: 0.6573601332899826 Training: 0.6423344909721858 Seed: 9987\n",
      "Testing: 0.6478143012676378 Training: 0.6447310259985901 Seed: 9991\n",
      "Testing: 0.6516450323877269 Training: 0.643688800224056 Seed: 9992\n",
      "Testing: 0.6605597927498026 Training: 0.6414872693633319 Seed: 9995\n",
      "Testing: 0.6567852747673717 Training: 0.6421600354537269 Seed: 9998\n",
      "Testing: 0.6570796826440884 Training: 0.642473966923473 Seed: 9999\n",
      "Testing: 0.6592995763540218 Training: 0.6417658485405064 Seed: 10001\n",
      "Testing: 0.6511220031639843 Training: 0.6438604901651732 Seed: 10006\n",
      "Testing: 0.6455109278243727 Training: 0.6453702814824134 Seed: 10007\n",
      "Testing: 0.6508104929083343 Training: 0.6438473229861515 Seed: 10010\n",
      "Testing: 0.6548502305368801 Training: 0.6428610247291884 Seed: 10011\n",
      "Testing: 0.6570069870266133 Training: 0.6422959555139296 Seed: 10013\n",
      "Testing: 0.6556365822623623 Training: 0.6428729150280945 Seed: 10016\n",
      "Testing: 0.6621710282196864 Training: 0.6411743933640495 Seed: 10019\n",
      "Testing: 0.6577341416188274 Training: 0.6422748754781908 Seed: 10020\n",
      "Testing: 0.6539595199700929 Training: 0.6432787851748306 Seed: 10021\n",
      "Testing: 0.6488955295353827 Training: 0.6444025783978811 Seed: 10022\n",
      "Testing: 0.6495396557627485 Training: 0.6442567926999876 Seed: 10026\n",
      "Testing: 0.6498256741393974 Training: 0.6442079006419783 Seed: 10027\n",
      "Testing: 0.6461561063485203 Training: 0.6451339454890956 Seed: 10028\n",
      "Testing: 0.6533659600330944 Training: 0.6433948123022246 Seed: 10029\n",
      "Testing: 0.6639789842157816 Training: 0.640545251253686 Seed: 10031\n",
      "Testing: 0.6488818839027186 Training: 0.644531263280993 Seed: 10033\n",
      "Testing: 0.6480195236810165 Training: 0.6446724462997687 Seed: 10034\n",
      "Testing: 0.6607789654867928 Training: 0.6410070626008102 Seed: 10036\n",
      "Testing: 0.6590823984330022 Training: 0.6420230229860917 Seed: 10037\n",
      "Testing: 0.6476461563507897 Training: 0.644736193455191 Seed: 10038\n",
      "Testing: 0.6460844310765208 Training: 0.6452439901269167 Seed: 10039\n",
      "Testing: 0.652694326425955 Training: 0.643696222818279 Seed: 10040\n",
      "Testing: 0.6477454050852691 Training: 0.6447742371452287 Seed: 10046\n",
      "Testing: 0.65256772343353 Training: 0.6435099798249362 Seed: 10047\n",
      "Testing: 0.6499452496017402 Training: 0.6443204072448199 Seed: 10048\n",
      "Testing: 0.6476608145821973 Training: 0.6448443264576674 Seed: 10049\n",
      "Testing: 0.6543089069217931 Training: 0.6429302519953953 Seed: 10051\n",
      "Testing: 0.6558965125443615 Training: 0.6426944839073643 Seed: 10053\n",
      "Testing: 0.6523564351657778 Training: 0.6436773561955954 Seed: 10056\n",
      "Testing: 0.6518254435631623 Training: 0.6438012663812755 Seed: 10058\n",
      "Testing: 0.6583661698302212 Training: 0.6420311923574491 Seed: 10063\n",
      "Testing: 0.6492696097221957 Training: 0.6444402967642359 Seed: 10064\n",
      "Testing: 0.6708213502540314 Training: 0.638861552581593 Seed: 10068\n",
      "Testing: 0.6565801403494144 Training: 0.6425257051804877 Seed: 10070\n",
      "Testing: 0.6612235599911132 Training: 0.6413410729427997 Seed: 10075\n",
      "Testing: 0.6636586077569857 Training: 0.6407161943756456 Seed: 10077\n",
      "Testing: 0.6575880456787899 Training: 0.6420922116611117 Seed: 10078\n",
      "Testing: 0.6552569594536435 Training: 0.6428964275738744 Seed: 10080\n",
      "Testing: 0.646079472451895 Training: 0.6452030780004405 Seed: 10082\n",
      "Testing: 0.6473928437911559 Training: 0.6448807772197007 Seed: 10094\n",
      "Testing: 0.6523732054174309 Training: 0.6436457439385872 Seed: 10095\n",
      "Testing: 0.6618016318861917 Training: 0.641254105887121 Seed: 10098\n",
      "Testing: 0.6476769871153611 Training: 0.6448168179863156 Seed: 10102\n",
      "Testing: 0.6500917973266054 Training: 0.6442173721668933 Seed: 10106\n",
      "Testing: 0.6546750382699223 Training: 0.6429951958333204 Seed: 10109\n",
      "Testing: 0.6457631429342271 Training: 0.6452848405143883 Seed: 10112\n",
      "Testing: 0.648949461023959 Training: 0.6444567571531106 Seed: 10115\n",
      "Testing: 0.6638518385959493 Training: 0.6406863348463334 Seed: 10116\n",
      "Testing: 0.6486260558957716 Training: 0.6445661049802524 Seed: 10117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6460668077388787 Training: 0.6450499539027641 Seed: 10118\n",
      "Testing: 0.6561056199257016 Training: 0.6427623580908368 Seed: 10119\n",
      "Testing: 0.6474493537449642 Training: 0.644888783485855 Seed: 10120\n",
      "Testing: 0.6623720610137698 Training: 0.6411364468184321 Seed: 10124\n",
      "Testing: 0.6462142526522221 Training: 0.645140102636109 Seed: 10125\n",
      "Testing: 0.6523459618374838 Training: 0.6435172288157182 Seed: 10128\n",
      "Testing: 0.6496559950722327 Training: 0.6442143400281962 Seed: 10129\n",
      "Testing: 0.6654564671337476 Training: 0.6403795833679724 Seed: 10131\n",
      "Testing: 0.6604388371620409 Training: 0.6415535603508817 Seed: 10134\n",
      "Testing: 0.650454552272911 Training: 0.6440278384902176 Seed: 10138\n",
      "Testing: 0.6555620628532969 Training: 0.64290537690036 Seed: 10142\n",
      "Testing: 0.6536939112311814 Training: 0.6433165848323968 Seed: 10144\n",
      "Testing: 0.6562607762653543 Training: 0.6427709900723507 Seed: 10145\n",
      "Testing: 0.6479056459116332 Training: 0.6442296769167506 Seed: 10146\n",
      "Testing: 0.6582042747587774 Training: 0.6421621249119251 Seed: 10151\n",
      "Testing: 0.6511391482421994 Training: 0.643938449466219 Seed: 10154\n",
      "Testing: 0.6489971965549961 Training: 0.644446375640845 Seed: 10156\n",
      "Testing: 0.6494731448112993 Training: 0.6443672925073968 Seed: 10158\n",
      "Testing: 0.6503977932895956 Training: 0.6441214613249312 Seed: 10160\n",
      "Testing: 0.6483143746898319 Training: 0.6443449769836623 Seed: 10164\n",
      "Testing: 0.6484681453542447 Training: 0.6445563864033226 Seed: 10165\n",
      "Testing: 0.6517980798787029 Training: 0.6437902667528527 Seed: 10166\n",
      "Testing: 0.6474856153839141 Training: 0.6448782228718594 Seed: 10168\n",
      "Testing: 0.64588566362142 Training: 0.645049705837581 Seed: 10169\n",
      "Testing: 0.6471089052335991 Training: 0.6449205261189204 Seed: 10170\n",
      "Testing: 0.6573552030121784 Training: 0.6420025815543816 Seed: 10171\n",
      "Testing: 0.6499883483302328 Training: 0.6439789775382185 Seed: 10172\n",
      "Testing: 0.6553666387502566 Training: 0.6426965629817671 Seed: 10174\n",
      "Testing: 0.6481106208412133 Training: 0.6447405812969255 Seed: 10176\n",
      "Testing: 0.6517872211285627 Training: 0.6436700392787226 Seed: 10179\n",
      "Testing: 0.655332057445867 Training: 0.6428186086846024 Seed: 10180\n",
      "Testing: 0.6530886426716375 Training: 0.6433932120688683 Seed: 10181\n",
      "Testing: 0.6469107547010648 Training: 0.6449862184896756 Seed: 10182\n",
      "Testing: 0.6501065993619305 Training: 0.6440979640829698 Seed: 10185\n",
      "Testing: 0.6484199612149989 Training: 0.6445872644666893 Seed: 10186\n",
      "Testing: 0.6690813258786432 Training: 0.6393221465109463 Seed: 10190\n",
      "Testing: 0.6529323801546492 Training: 0.6433365571720787 Seed: 10191\n",
      "Testing: 0.6512029426656778 Training: 0.643439260412928 Seed: 10192\n",
      "Testing: 0.6454202817113533 Training: 0.645250703173405 Seed: 10193\n",
      "Testing: 0.6518244364692186 Training: 0.6437810583212784 Seed: 10194\n",
      "Testing: 0.6557556675640631 Training: 0.6427507164973991 Seed: 10195\n",
      "Testing: 0.6619357364672465 Training: 0.640854486104615 Seed: 10196\n",
      "Testing: 0.6519683580230546 Training: 0.6435011176738964 Seed: 10197\n",
      "Testing: 0.6531851772094569 Training: 0.6434141993709559 Seed: 10198\n",
      "Testing: 0.6612311283933452 Training: 0.6414747074336301 Seed: 10199\n",
      "Testing: 0.6569106904942839 Training: 0.6424602513258091 Seed: 10200\n",
      "Testing: 0.6679399600090442 Training: 0.6394879940947477 Seed: 10204\n",
      "Testing: 0.6479478434591165 Training: 0.6447126726498287 Seed: 10211\n",
      "Testing: 0.6484030297997432 Training: 0.6436902063009411 Seed: 10212\n",
      "Testing: 0.6553235191006048 Training: 0.6425402198126511 Seed: 10214\n",
      "Testing: 0.6499783836151086 Training: 0.6442017381407563 Seed: 10215\n",
      "Testing: 0.6580726955255007 Training: 0.6420519735922622 Seed: 10216\n",
      "Testing: 0.656581528909302 Training: 0.642605911647718 Seed: 10217\n",
      "Testing: 0.6458994654920558 Training: 0.6452656401312639 Seed: 10219\n",
      "Testing: 0.6550062692383267 Training: 0.6429334032070786 Seed: 10221\n",
      "Testing: 0.6564650839897619 Training: 0.6424432987771058 Seed: 10225\n",
      "Testing: 0.649826588899622 Training: 0.6442231488198158 Seed: 10226\n",
      "Testing: 0.6479040053106766 Training: 0.6447459927102765 Seed: 10227\n",
      "Testing: 0.649950207097637 Training: 0.644117099721754 Seed: 10228\n",
      "Testing: 0.6499106897093799 Training: 0.6435251037489157 Seed: 10229\n",
      "Testing: 0.6531055367095004 Training: 0.6434108992894596 Seed: 10230\n",
      "Testing: 0.6535543283722896 Training: 0.6432867633652765 Seed: 10235\n",
      "Testing: 0.6470519643806336 Training: 0.6448298056276066 Seed: 10236\n",
      "Testing: 0.6503054585096535 Training: 0.6441436749725131 Seed: 10237\n",
      "Testing: 0.6469805143201854 Training: 0.6448644305689681 Seed: 10240\n",
      "Testing: 0.6529717565868145 Training: 0.6430885258312318 Seed: 10241\n",
      "Testing: 0.6685942743624261 Training: 0.6395189363818696 Seed: 10245\n",
      "Testing: 0.6585541538639423 Training: 0.6420364563502444 Seed: 10246\n",
      "Testing: 0.6543635010334476 Training: 0.6431792638640679 Seed: 10250\n",
      "Testing: 0.6483511480366356 Training: 0.6445191307495736 Seed: 10252\n",
      "Testing: 0.6542922404807845 Training: 0.643045434776778 Seed: 10253\n",
      "Testing: 0.6466667172601819 Training: 0.6450784206641917 Seed: 10255\n",
      "Testing: 0.6528802702565456 Training: 0.6435271873988422 Seed: 10256\n",
      "Testing: 0.6656211013576232 Training: 0.6401227018131235 Seed: 10259\n",
      "Testing: 0.6606733129754863 Training: 0.6415652475121089 Seed: 10262\n",
      "Testing: 0.6516498723616696 Training: 0.6438188025799304 Seed: 10263\n",
      "Testing: 0.645862129636608 Training: 0.6451820359673175 Seed: 10264\n",
      "Testing: 0.6536738120687073 Training: 0.643302209421413 Seed: 10265\n",
      "Testing: 0.6490989270393565 Training: 0.6444750591987606 Seed: 10268\n",
      "Testing: 0.6482688009343952 Training: 0.6443913221230599 Seed: 10269\n",
      "Testing: 0.6580991689283638 Training: 0.6420889252873281 Seed: 10273\n",
      "Testing: 0.657271680972999 Training: 0.642281176234359 Seed: 10278\n",
      "Testing: 0.6578240145889467 Training: 0.6422949147689502 Seed: 10281\n",
      "Testing: 0.6542133388581295 Training: 0.6430502941171965 Seed: 10283\n",
      "Testing: 0.6616599982437678 Training: 0.6412127155172616 Seed: 10284\n",
      "Testing: 0.6554713750991754 Training: 0.6427316159704335 Seed: 10285\n",
      "Testing: 0.6636836901627216 Training: 0.6406851355057159 Seed: 10291\n",
      "Testing: 0.6524720829725578 Training: 0.6436509936205906 Seed: 10292\n",
      "Testing: 0.6553796458906033 Training: 0.6429613562403563 Seed: 10293\n",
      "Testing: 0.6605553124767154 Training: 0.6416325839705734 Seed: 10295\n",
      "Testing: 0.64598353663687 Training: 0.6451693998926815 Seed: 10296\n",
      "Testing: 0.6464247993781554 Training: 0.6446446746694572 Seed: 10299\n",
      "Testing: 0.6536398079318727 Training: 0.6433211784799955 Seed: 10300\n",
      "Testing: 0.6495332652674751 Training: 0.6443451528744424 Seed: 10302\n",
      "Testing: 0.6485493480344825 Training: 0.6445769428236006 Seed: 10304\n",
      "Testing: 0.6570564220546979 Training: 0.642494904750573 Seed: 10305\n",
      "Testing: 0.6636806075020829 Training: 0.6408862071766772 Seed: 10306\n",
      "Testing: 0.6500888696786289 Training: 0.6440542202921664 Seed: 10307\n",
      "Testing: 0.6534878879232144 Training: 0.6430169291035012 Seed: 10308\n",
      "Testing: 0.65860113230053 Training: 0.6420252314176931 Seed: 10310\n",
      "Testing: 0.6560426048392533 Training: 0.642717477353985 Seed: 10312\n",
      "Testing: 0.6560560532818294 Training: 0.642611689388651 Seed: 10319\n",
      "Testing: 0.6575977993665456 Training: 0.642214902199266 Seed: 10320\n",
      "Testing: 0.6475998713755613 Training: 0.6448046245289432 Seed: 10322\n",
      "Testing: 0.645537680123307 Training: 0.6453917602916072 Seed: 10324\n",
      "Testing: 0.652492331794504 Training: 0.6436686295497931 Seed: 10325\n",
      "Testing: 0.6509710879015834 Training: 0.6439600267914031 Seed: 10326\n",
      "Testing: 0.6461281450547949 Training: 0.6450892074229573 Seed: 10327\n",
      "Testing: 0.6466053677903596 Training: 0.6451011233177777 Seed: 10331\n",
      "Testing: 0.6557726434952276 Training: 0.6428630834873023 Seed: 10334\n",
      "Testing: 0.6535182035593292 Training: 0.6433979872038746 Seed: 10337\n",
      "Testing: 0.6516098531374763 Training: 0.6438498793891412 Seed: 10338\n",
      "Testing: 0.6486642686440991 Training: 0.6443841833172723 Seed: 10339\n",
      "Testing: 0.6541335833090264 Training: 0.6432389432154725 Seed: 10340\n",
      "Testing: 0.6472321614630637 Training: 0.6449657349884813 Seed: 10343\n",
      "Testing: 0.6644448890469239 Training: 0.6398970057113962 Seed: 10348\n",
      "Testing: 0.6500918378389685 Training: 0.6441763752457553 Seed: 10349\n",
      "Testing: 0.6528056799563038 Training: 0.6435189134570234 Seed: 10350\n",
      "Testing: 0.6693781839982825 Training: 0.6395267948414518 Seed: 10352\n",
      "Testing: 0.6612279066779425 Training: 0.6413500994534347 Seed: 10354\n",
      "Testing: 0.6515855116482427 Training: 0.6431186625025332 Seed: 10360\n",
      "Testing: 0.6458960288207095 Training: 0.6451901966813434 Seed: 10361\n",
      "Testing: 0.6461585074862636 Training: 0.645217990428818 Seed: 10362\n",
      "Testing: 0.6533028286175429 Training: 0.6434512163832348 Seed: 10364\n",
      "Testing: 0.669246519464703 Training: 0.6391772031080435 Seed: 10366\n",
      "Testing: 0.659269627886746 Training: 0.6415155001556196 Seed: 10367\n",
      "Testing: 0.6503284145458299 Training: 0.6441756559783351 Seed: 10368\n",
      "Testing: 0.6490845436521451 Training: 0.6444094061582678 Seed: 10369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6579434253196393 Training: 0.6421982098651342 Seed: 10370\n",
      "Testing: 0.6497015239302 Training: 0.6442554665041276 Seed: 10372\n",
      "Testing: 0.6495515246698426 Training: 0.6443654195718946 Seed: 10374\n",
      "Testing: 0.6537700758508029 Training: 0.6432975369752499 Seed: 10376\n",
      "Testing: 0.6561553108188092 Training: 0.6427689438229122 Seed: 10377\n",
      "Testing: 0.6490552034464115 Training: 0.6444711515187733 Seed: 10379\n",
      "Testing: 0.6519801518399071 Training: 0.6437451359909336 Seed: 10380\n",
      "Testing: 0.6529015743255555 Training: 0.6434486578003773 Seed: 10381\n",
      "Testing: 0.6563713821696848 Training: 0.6427561596689919 Seed: 10383\n",
      "Testing: 0.6619886106757378 Training: 0.64117483884861 Seed: 10385\n",
      "Testing: 0.6484661172426037 Training: 0.644511060449197 Seed: 10387\n",
      "Testing: 0.6495577918378188 Training: 0.6443174595181568 Seed: 10389\n",
      "Testing: 0.6500239834100918 Training: 0.6441482427110591 Seed: 10390\n",
      "Testing: 0.6586676906892537 Training: 0.6418813750343507 Seed: 10391\n",
      "Testing: 0.6567536190638228 Training: 0.6424931541553087 Seed: 10392\n",
      "Testing: 0.6642574234068439 Training: 0.6405202476484555 Seed: 10393\n",
      "Testing: 0.6578506585126855 Training: 0.6423882530630961 Seed: 10397\n",
      "Testing: 0.6559713107475698 Training: 0.6426115059217091 Seed: 10398\n",
      "Testing: 0.6633627885903886 Training: 0.6409975736022568 Seed: 10399\n",
      "Testing: 0.6569705448721118 Training: 0.6424586926844887 Seed: 10400\n",
      "Testing: 0.6457066419587715 Training: 0.6452724899738275 Seed: 10401\n",
      "Testing: 0.6554160267064534 Training: 0.6429408153327023 Seed: 10404\n",
      "Testing: 0.6507819260062757 Training: 0.6438902004395874 Seed: 10405\n",
      "Testing: 0.6499911743457492 Training: 0.6441783443830269 Seed: 10407\n",
      "Testing: 0.6562389027393761 Training: 0.6421444461615373 Seed: 10408\n",
      "Testing: 0.6482278273521713 Training: 0.6443659166194201 Seed: 10409\n",
      "Testing: 0.6590008164748196 Training: 0.641972110352887 Seed: 10410\n",
      "Testing: 0.6511087540564258 Training: 0.6439131963093937 Seed: 10412\n",
      "Testing: 0.6564608455201405 Training: 0.6426525585561011 Seed: 10413\n",
      "Testing: 0.6526426786214569 Training: 0.6436057434868393 Seed: 10417\n",
      "Testing: 0.6674472661627764 Training: 0.6397723729501036 Seed: 10418\n",
      "Testing: 0.6518713682706136 Training: 0.6438294613609827 Seed: 10420\n",
      "Testing: 0.645404509431721 Training: 0.645290233614189 Seed: 10421\n",
      "Testing: 0.6637830009010725 Training: 0.6407275371645412 Seed: 10424\n",
      "Testing: 0.6663122002664839 Training: 0.6398371016669213 Seed: 10425\n",
      "Testing: 0.6457730963865177 Training: 0.6450536305897563 Seed: 10426\n",
      "Testing: 0.6609107131333516 Training: 0.6412884959035854 Seed: 10428\n",
      "Testing: 0.6467630278349948 Training: 0.6449732263414889 Seed: 10432\n",
      "Testing: 0.6542327126327369 Training: 0.6429671171682503 Seed: 10433\n",
      "Testing: 0.6550298199113483 Training: 0.6428575029967806 Seed: 10434\n",
      "Testing: 0.6574562960636698 Training: 0.6421419953650315 Seed: 10436\n",
      "Testing: 0.6472532768221338 Training: 0.6445395783834662 Seed: 10437\n",
      "Testing: 0.6571530945383715 Training: 0.642472929380457 Seed: 10438\n",
      "Testing: 0.6512911659726497 Training: 0.643973393620105 Seed: 10439\n",
      "Testing: 0.6525556814575374 Training: 0.6435540022736832 Seed: 10440\n",
      "Testing: 0.6530556645673387 Training: 0.6435137239692148 Seed: 10441\n",
      "Testing: 0.652530598561389 Training: 0.6435039095345461 Seed: 10442\n",
      "Testing: 0.6514887488358222 Training: 0.6437000864258084 Seed: 10444\n",
      "Testing: 0.6614103441333452 Training: 0.6412507162176408 Seed: 10446\n",
      "Testing: 0.6545082934869623 Training: 0.642976394447305 Seed: 10447\n",
      "Testing: 0.6555464902952799 Training: 0.6427988106719899 Seed: 10448\n",
      "Testing: 0.6473476505115824 Training: 0.6448868960413706 Seed: 10449\n",
      "Testing: 0.6468805570293592 Training: 0.6450619137382675 Seed: 10451\n",
      "Testing: 0.6605186395851741 Training: 0.6416530361449758 Seed: 10453\n",
      "Testing: 0.65176997423437 Training: 0.6436609267952471 Seed: 10455\n",
      "Testing: 0.6532384703473003 Training: 0.6433741649677751 Seed: 10456\n",
      "Testing: 0.6510292498115245 Training: 0.6438204656649914 Seed: 10457\n",
      "Testing: 0.6464775768407787 Training: 0.6450014963370652 Seed: 10458\n",
      "Testing: 0.6487229186478909 Training: 0.6445038107332213 Seed: 10460\n",
      "Testing: 0.6513901862185711 Training: 0.6439524792102924 Seed: 10463\n",
      "Testing: 0.6599744836224412 Training: 0.6415748872010494 Seed: 10464\n",
      "Testing: 0.6663180882060524 Training: 0.6401803881952418 Seed: 10473\n",
      "Testing: 0.6612587249051527 Training: 0.6413963081486183 Seed: 10475\n",
      "Testing: 0.6494977485677742 Training: 0.6443972692331619 Seed: 10477\n",
      "Testing: 0.6514906797877533 Training: 0.6439057290204357 Seed: 10483\n",
      "Testing: 0.6583280964481388 Training: 0.642133057599753 Seed: 10484\n",
      "Testing: 0.6470439000400087 Training: 0.6444727645267129 Seed: 10485\n",
      "Testing: 0.6484884596363109 Training: 0.6446580574915068 Seed: 10488\n",
      "Testing: 0.6511623296231585 Training: 0.6439156878818526 Seed: 10489\n",
      "Testing: 0.653454532636347 Training: 0.6433989745062156 Seed: 10490\n",
      "Testing: 0.6491485827966441 Training: 0.6444953125615656 Seed: 10493\n",
      "Testing: 0.6497438379023674 Training: 0.6443089050734065 Seed: 10496\n",
      "Testing: 0.6544636922663252 Training: 0.6430771012315917 Seed: 10497\n",
      "Testing: 0.6476633543442064 Training: 0.6447917016458058 Seed: 10498\n",
      "Testing: 0.6489500418572494 Training: 0.6445306555641231 Seed: 10500\n",
      "Testing: 0.6581909784397942 Training: 0.642198593567417 Seed: 10501\n",
      "Testing: 0.6541044144134701 Training: 0.6432456084981168 Seed: 10502\n",
      "Testing: 0.6577035311738423 Training: 0.6423729652031941 Seed: 10504\n",
      "Testing: 0.6482317189872858 Training: 0.6446460985170807 Seed: 10505\n",
      "Testing: 0.6464953928294361 Training: 0.6447208851233155 Seed: 10506\n",
      "Testing: 0.6624875103135996 Training: 0.6409235970713406 Seed: 10508\n",
      "Testing: 0.6538709012088627 Training: 0.6432712601025672 Seed: 10512\n",
      "Testing: 0.6467334001440335 Training: 0.6450399402176509 Seed: 10516\n",
      "Testing: 0.6455513663616338 Training: 0.6454087670195047 Seed: 10517\n",
      "Testing: 0.6499763429779482 Training: 0.6441716998593794 Seed: 10518\n",
      "Testing: 0.6466378667757884 Training: 0.6450554831872719 Seed: 10520\n",
      "Testing: 0.6461213254053972 Training: 0.6452521598247244 Seed: 10521\n",
      "Testing: 0.6463328193983222 Training: 0.6450662767904385 Seed: 10522\n",
      "Testing: 0.6456328370632916 Training: 0.64527442357462 Seed: 10529\n",
      "Testing: 0.6541490194476067 Training: 0.6431775375801109 Seed: 10531\n",
      "Testing: 0.6564109145440917 Training: 0.6426394315467308 Seed: 10532\n",
      "Testing: 0.6482906520083422 Training: 0.6446140406462842 Seed: 10533\n",
      "Testing: 0.6502934346556482 Training: 0.6442047053907739 Seed: 10534\n",
      "Testing: 0.6451844478978752 Training: 0.6450560181951481 Seed: 10536\n",
      "Testing: 0.6645231089891477 Training: 0.640405443166737 Seed: 10542\n",
      "Testing: 0.6558919692767955 Training: 0.6427259492052668 Seed: 10543\n",
      "Testing: 0.6504355768367444 Training: 0.6432980973474967 Seed: 10544\n",
      "Testing: 0.648340484102643 Training: 0.6444977754751373 Seed: 10546\n",
      "Testing: 0.6592113220616346 Training: 0.6419574924009462 Seed: 10548\n",
      "Testing: 0.6604732348789139 Training: 0.6416331270505191 Seed: 10550\n",
      "Testing: 0.6564035693534811 Training: 0.642497229968847 Seed: 10554\n",
      "Testing: 0.6524458225506892 Training: 0.6435598599872465 Seed: 10558\n",
      "Testing: 0.6662667301418674 Training: 0.6400996746537593 Seed: 10559\n",
      "Testing: 0.6526121298411821 Training: 0.6436107118238358 Seed: 10560\n",
      "Testing: 0.6535382120368605 Training: 0.6433267002228304 Seed: 10561\n",
      "Testing: 0.6539826259095773 Training: 0.6430818448749979 Seed: 10562\n",
      "Testing: 0.6540411615551441 Training: 0.6432212412721804 Seed: 10565\n",
      "Testing: 0.6480341548766383 Training: 0.6446613132404104 Seed: 10567\n",
      "Testing: 0.6474789027063784 Training: 0.6447159400569333 Seed: 10568\n",
      "Testing: 0.6616444736412084 Training: 0.6413170839831879 Seed: 10573\n",
      "Testing: 0.6514756382331414 Training: 0.643722220753105 Seed: 10574\n",
      "Testing: 0.6462393392002597 Training: 0.6450330491781702 Seed: 10575\n",
      "Testing: 0.6515073069922303 Training: 0.6437558379747342 Seed: 10576\n",
      "Testing: 0.6550616442638566 Training: 0.6429159933610419 Seed: 10579\n",
      "Testing: 0.6532821130876751 Training: 0.6433906281357298 Seed: 10580\n",
      "Testing: 0.6462141651564958 Training: 0.6451609662133349 Seed: 10582\n",
      "Testing: 0.6454449548702248 Training: 0.6453479006286682 Seed: 10585\n",
      "Testing: 0.6651031104635255 Training: 0.6401983533624831 Seed: 10590\n",
      "Testing: 0.6459281689664347 Training: 0.6448016554177398 Seed: 10594\n",
      "Testing: 0.6530767524888644 Training: 0.6435204719629574 Seed: 10596\n",
      "Testing: 0.6518875696852082 Training: 0.6437996454912436 Seed: 10598\n",
      "Testing: 0.6539074876295051 Training: 0.6430775446354896 Seed: 10600\n",
      "Testing: 0.6461961762010198 Training: 0.6450347189146799 Seed: 10601\n",
      "Testing: 0.6615172492016673 Training: 0.6409985757943818 Seed: 10606\n",
      "Testing: 0.654607043922212 Training: 0.6429036005534189 Seed: 10607\n",
      "Testing: 0.659255857101367 Training: 0.6417136680507352 Seed: 10610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6583357435720095 Training: 0.6417635607160178 Seed: 10611\n",
      "Testing: 0.6515392539315603 Training: 0.6432935814676226 Seed: 10612\n",
      "Testing: 0.6530213544503446 Training: 0.6435309702840455 Seed: 10615\n",
      "Testing: 0.6591302586243818 Training: 0.6419785333251264 Seed: 10616\n",
      "Testing: 0.6580235454548405 Training: 0.6422674709091666 Seed: 10617\n",
      "Testing: 0.6485037371805977 Training: 0.6446451067489931 Seed: 10618\n",
      "Testing: 0.651278501816257 Training: 0.6438713813468552 Seed: 10619\n",
      "Testing: 0.6553084535714142 Training: 0.642837402969194 Seed: 10624\n",
      "Testing: 0.6511953580172977 Training: 0.6438597664454686 Seed: 10625\n",
      "Testing: 0.6623587099386765 Training: 0.6408473382116385 Seed: 10627\n",
      "Testing: 0.6468742028729255 Training: 0.6450513414215728 Seed: 10628\n",
      "Testing: 0.6572484448895801 Training: 0.6423874027752876 Seed: 10630\n",
      "Testing: 0.6464021792611483 Training: 0.6450403263611666 Seed: 10637\n",
      "Testing: 0.648244684192624 Training: 0.644487371431159 Seed: 10640\n",
      "Testing: 0.6456519348576528 Training: 0.6452714067555406 Seed: 10644\n",
      "Testing: 0.6557696040876264 Training: 0.6426089451343975 Seed: 10646\n",
      "Testing: 0.6566017279138537 Training: 0.6426135179554036 Seed: 10647\n",
      "Testing: 0.6727508921347003 Training: 0.6384324615969239 Seed: 10650\n",
      "Testing: 0.6515795047515478 Training: 0.6437184469466898 Seed: 10651\n",
      "Testing: 0.6546031176319729 Training: 0.6430318877298602 Seed: 10654\n",
      "Testing: 0.660149794950229 Training: 0.6416599376067198 Seed: 10655\n",
      "Testing: 0.6512489717709737 Training: 0.6438949489677306 Seed: 10657\n",
      "Testing: 0.647921054341082 Training: 0.6445238610384674 Seed: 10663\n",
      "Testing: 0.6498007204929923 Training: 0.6442362053958863 Seed: 10664\n",
      "Testing: 0.6495994112483435 Training: 0.6441007465586306 Seed: 10665\n",
      "Testing: 0.6473103732392348 Training: 0.6449182559460123 Seed: 10667\n",
      "Testing: 0.647709495090194 Training: 0.6447203716116696 Seed: 10668\n",
      "Testing: 0.6590637276988548 Training: 0.6418908628758068 Seed: 10670\n",
      "Testing: 0.6553393548493304 Training: 0.6429602826343331 Seed: 10678\n",
      "Testing: 0.6476889000689109 Training: 0.6447339631497059 Seed: 10680\n",
      "Testing: 0.6532528067435025 Training: 0.6433143633013048 Seed: 10682\n",
      "Testing: 0.6470797100043534 Training: 0.6449567549029138 Seed: 10686\n",
      "Testing: 0.6498084376410674 Training: 0.6442531489776968 Seed: 10689\n",
      "Testing: 0.6545829675749467 Training: 0.6430127248233358 Seed: 10690\n",
      "Testing: 0.6496282800412275 Training: 0.6440049576108555 Seed: 10694\n",
      "Testing: 0.662266921057784 Training: 0.6411253909795087 Seed: 10701\n",
      "Testing: 0.6472983940844247 Training: 0.6449263089350488 Seed: 10702\n",
      "Testing: 0.6489054325727874 Training: 0.6444005429144469 Seed: 10703\n",
      "Testing: 0.6505450630556732 Training: 0.6441251336735905 Seed: 10705\n",
      "Testing: 0.6573210047388044 Training: 0.6423607779690451 Seed: 10706\n",
      "Testing: 0.6469041117018524 Training: 0.6450186447916699 Seed: 10709\n",
      "Testing: 0.6478708940605116 Training: 0.6447437310794228 Seed: 10710\n",
      "Testing: 0.6734955204492548 Training: 0.6380827225554729 Seed: 10711\n",
      "Testing: 0.6569715389922435 Training: 0.6424767418789308 Seed: 10714\n",
      "Testing: 0.6463042430992736 Training: 0.6451112760247245 Seed: 10715\n",
      "Testing: 0.6480188468405296 Training: 0.6443305189127337 Seed: 10719\n",
      "Testing: 0.66108298959409 Training: 0.6413603411240538 Seed: 10720\n",
      "Testing: 0.6555813560275838 Training: 0.642301243517762 Seed: 10721\n",
      "Testing: 0.6582925512185233 Training: 0.6420444000750232 Seed: 10722\n",
      "Testing: 0.6545418306822183 Training: 0.6428420250542484 Seed: 10723\n",
      "Testing: 0.6490881461129427 Training: 0.644470020786386 Seed: 10724\n",
      "Testing: 0.6556469101099166 Training: 0.6428146112788227 Seed: 10730\n",
      "Testing: 0.6523677377965224 Training: 0.6435502515797356 Seed: 10735\n",
      "Testing: 0.6472901512196678 Training: 0.6449074177127638 Seed: 10736\n",
      "Testing: 0.6613187609867376 Training: 0.6413387631662246 Seed: 10737\n",
      "Testing: 0.6516721699970515 Training: 0.6436117973502455 Seed: 10741\n",
      "Testing: 0.6504497928196982 Training: 0.6441249946629558 Seed: 10742\n",
      "Testing: 0.6584363775561739 Training: 0.6421200048486357 Seed: 10751\n",
      "Testing: 0.6520142256775454 Training: 0.6435851574200939 Seed: 10753\n",
      "Testing: 0.6549049437737184 Training: 0.6428961024476909 Seed: 10757\n",
      "Testing: 0.6488969509073343 Training: 0.644495167929929 Seed: 10761\n",
      "Testing: 0.6632995391245775 Training: 0.6409154380119519 Seed: 10762\n",
      "Testing: 0.6598639182720587 Training: 0.641600300405907 Seed: 10763\n",
      "Testing: 0.6502133872430389 Training: 0.6440785369088065 Seed: 10764\n",
      "Testing: 0.6548663394157019 Training: 0.6430465532321181 Seed: 10765\n",
      "Testing: 0.6469768148434856 Training: 0.6448574480942209 Seed: 10767\n",
      "Testing: 0.6457887481207734 Training: 0.6452378079413607 Seed: 10769\n",
      "Testing: 0.6509276628630154 Training: 0.644095955937424 Seed: 10770\n",
      "Testing: 0.6542685533573853 Training: 0.6431738944448231 Seed: 10774\n",
      "Testing: 0.6499061964355797 Training: 0.6442683815677314 Seed: 10776\n",
      "Testing: 0.651278912847078 Training: 0.643938644133951 Seed: 10778\n",
      "Testing: 0.647779596895965 Training: 0.6448010828815499 Seed: 10780\n",
      "Testing: 0.6617014902442822 Training: 0.6412024210737021 Seed: 10783\n",
      "Testing: 0.650790029893256 Training: 0.6439171811016159 Seed: 10784\n",
      "Testing: 0.6462654008470189 Training: 0.6451762834599886 Seed: 10785\n",
      "Testing: 0.6523920589596136 Training: 0.6435813178002159 Seed: 10788\n",
      "Testing: 0.652263430518417 Training: 0.6436044141167425 Seed: 10789\n",
      "Testing: 0.6505801100865157 Training: 0.6440374839639718 Seed: 10793\n",
      "Testing: 0.6509131581104081 Training: 0.6439724742783036 Seed: 10796\n",
      "Testing: 0.6626752413143453 Training: 0.6410417486853415 Seed: 10797\n",
      "Testing: 0.6536009281957207 Training: 0.6433594096610804 Seed: 10799\n",
      "Testing: 0.6610962286419789 Training: 0.641271144969399 Seed: 10802\n",
      "Testing: 0.6475278194042404 Training: 0.6447969436121552 Seed: 10803\n",
      "Testing: 0.6485982312962624 Training: 0.6445959054769145 Seed: 10806\n",
      "Testing: 0.6518523502758304 Training: 0.6437717700389696 Seed: 10808\n",
      "Testing: 0.6514539517387186 Training: 0.6438843638375144 Seed: 10809\n",
      "Testing: 0.6639146877871615 Training: 0.6399651238771824 Seed: 10810\n",
      "Testing: 0.6508275603699551 Training: 0.6440461753595872 Seed: 10811\n",
      "Testing: 0.6512368913454969 Training: 0.6439128000798001 Seed: 10813\n",
      "Testing: 0.6545424286835813 Training: 0.6429533361051591 Seed: 10814\n",
      "Testing: 0.6488922028967066 Training: 0.6445214469321505 Seed: 10815\n",
      "Testing: 0.6507258507315609 Training: 0.6440302865197853 Seed: 10817\n",
      "Testing: 0.6481300127130776 Training: 0.6446658026924648 Seed: 10819\n",
      "Testing: 0.645595932219746 Training: 0.6447863105482232 Seed: 10820\n",
      "Testing: 0.6586170451123133 Training: 0.6420180948484282 Seed: 10822\n",
      "Testing: 0.6491931137353153 Training: 0.6444262121257621 Seed: 10826\n",
      "Testing: 0.6473076246390809 Training: 0.6449193051807918 Seed: 10836\n",
      "Testing: 0.6524466723853299 Training: 0.6435621405935505 Seed: 10839\n",
      "Testing: 0.6657642806684155 Training: 0.639982469031396 Seed: 10840\n",
      "Testing: 0.649764662368909 Training: 0.6442324772645514 Seed: 10843\n",
      "Testing: 0.6463671407998388 Training: 0.6450573244081943 Seed: 10844\n",
      "Testing: 0.6475840507723127 Training: 0.6446333323384451 Seed: 10845\n",
      "Testing: 0.6491981328296781 Training: 0.644247244319047 Seed: 10846\n",
      "Testing: 0.6511273700143798 Training: 0.6439652862753348 Seed: 10850\n",
      "Testing: 0.6497698051033048 Training: 0.64436240499682 Seed: 10855\n",
      "Testing: 0.6500873119434263 Training: 0.6442650323650384 Seed: 10858\n",
      "Testing: 0.6456921833729575 Training: 0.645263063273357 Seed: 10859\n",
      "Testing: 0.666592417322712 Training: 0.6401782139999409 Seed: 10860\n",
      "Testing: 0.6483092477316744 Training: 0.6446087973441189 Seed: 10861\n",
      "Testing: 0.6458652699500289 Training: 0.645254167743622 Seed: 10862\n",
      "Testing: 0.6465078647083475 Training: 0.6449261520722074 Seed: 10863\n",
      "Testing: 0.6503748587133262 Training: 0.644063889057162 Seed: 10864\n",
      "Testing: 0.6468160518091773 Training: 0.644988703107001 Seed: 10870\n",
      "Testing: 0.649988738388389 Training: 0.6441451629457833 Seed: 10872\n",
      "Testing: 0.6607547099083511 Training: 0.6415519725115038 Seed: 10873\n",
      "Testing: 0.6547619956815703 Training: 0.6429775192316588 Seed: 10874\n",
      "Testing: 0.6485387285013403 Training: 0.6445184427986341 Seed: 10877\n",
      "Testing: 0.6507530491685805 Training: 0.6438681560325713 Seed: 10878\n",
      "Testing: 0.6577882729068569 Training: 0.6422729895899366 Seed: 10879\n",
      "Testing: 0.655430655608512 Training: 0.6422691375400961 Seed: 10881\n",
      "Testing: 0.6552377530617532 Training: 0.6424878948630204 Seed: 10885\n",
      "Testing: 0.6514518171042922 Training: 0.6438384120522147 Seed: 10886\n",
      "Testing: 0.6635231238551061 Training: 0.6408927130667643 Seed: 10887\n",
      "Testing: 0.6510016413042035 Training: 0.6438985652321774 Seed: 10890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6553562766121516 Training: 0.6429242066201015 Seed: 10893\n",
      "Testing: 0.6462250608556372 Training: 0.6451333759542593 Seed: 10894\n",
      "Testing: 0.6559932281426344 Training: 0.642776521127641 Seed: 10900\n",
      "Testing: 0.6467946157862641 Training: 0.644769325287963 Seed: 10901\n",
      "Testing: 0.6467387995771998 Training: 0.6450087544284493 Seed: 10902\n",
      "Testing: 0.6506436854079347 Training: 0.6440283412748034 Seed: 10904\n",
      "Testing: 0.6541811276157725 Training: 0.6429998735523352 Seed: 10905\n",
      "Testing: 0.6644817710662315 Training: 0.6405550949012349 Seed: 10906\n",
      "Testing: 0.6670997606018786 Training: 0.639868626894055 Seed: 10909\n",
      "Testing: 0.6611451639196506 Training: 0.641452838019054 Seed: 10911\n",
      "Testing: 0.6599277255931985 Training: 0.6416346877956132 Seed: 10913\n",
      "Testing: 0.6521586301100091 Training: 0.643643346036644 Seed: 10914\n",
      "Testing: 0.6631639030745455 Training: 0.6405050003887107 Seed: 10915\n",
      "Testing: 0.6502192216244593 Training: 0.6441630597375476 Seed: 10916\n",
      "Testing: 0.647448915644198 Training: 0.6448575431576558 Seed: 10918\n",
      "Testing: 0.6554750036841225 Training: 0.6429789694345222 Seed: 10919\n",
      "Testing: 0.6511752028756319 Training: 0.6439585160869824 Seed: 10920\n",
      "Testing: 0.647839580952461 Training: 0.644815908808029 Seed: 10921\n",
      "Testing: 0.6553117086872177 Training: 0.6428699516904387 Seed: 10922\n",
      "Testing: 0.6571366789579527 Training: 0.6424893604019686 Seed: 10924\n",
      "Testing: 0.6464042917433913 Training: 0.6451034322672533 Seed: 10927\n",
      "Testing: 0.6571576315746234 Training: 0.6424223040487897 Seed: 10930\n",
      "Testing: 0.653182565507016 Training: 0.643407444483443 Seed: 10931\n",
      "Testing: 0.6602509870660754 Training: 0.6414788214899805 Seed: 10932\n",
      "Testing: 0.6555370882648732 Training: 0.6428833862778239 Seed: 10938\n",
      "Testing: 0.648209147612827 Training: 0.6444442492086933 Seed: 10941\n",
      "Testing: 0.6543876341776116 Training: 0.6430481333033622 Seed: 10943\n",
      "Testing: 0.6483447088221174 Training: 0.6446922544890515 Seed: 10945\n",
      "Testing: 0.6468879118381032 Training: 0.6449964689344518 Seed: 10946\n",
      "Testing: 0.6522463986477426 Training: 0.6436818515366998 Seed: 10949\n",
      "Testing: 0.6476201373649986 Training: 0.6446391838877039 Seed: 10951\n",
      "Testing: 0.6539082296766832 Training: 0.6430570287336206 Seed: 10952\n",
      "Testing: 0.6502916662366234 Training: 0.6441686017544321 Seed: 10953\n",
      "Testing: 0.649908741596783 Training: 0.6442589509517039 Seed: 10955\n",
      "Testing: 0.6468371866142922 Training: 0.6449913734697446 Seed: 10956\n",
      "Testing: 0.6514327541073511 Training: 0.6439050123643524 Seed: 10958\n",
      "Testing: 0.6504694638576876 Training: 0.6441649649766686 Seed: 10962\n",
      "Testing: 0.6497968022725973 Training: 0.6442512090724977 Seed: 10963\n",
      "Testing: 0.6490366190084161 Training: 0.6445386935478259 Seed: 10964\n",
      "Testing: 0.6530481405869795 Training: 0.6434429327577049 Seed: 10966\n",
      "Testing: 0.653493993475504 Training: 0.6432540809279921 Seed: 10967\n",
      "Testing: 0.66340223459268 Training: 0.6408582664469397 Seed: 10970\n",
      "Testing: 0.6676645781602933 Training: 0.6399436918130186 Seed: 10972\n",
      "Testing: 0.6493490104246563 Training: 0.6444586400419143 Seed: 10975\n",
      "Testing: 0.6494085405298573 Training: 0.6442778947608864 Seed: 10977\n",
      "Testing: 0.6478246936838956 Training: 0.6441649595157033 Seed: 10981\n",
      "Testing: 0.6502748044218535 Training: 0.6440742881132815 Seed: 10982\n",
      "Testing: 0.6575484799768379 Training: 0.6424215909753731 Seed: 10983\n",
      "Testing: 0.6561926882393603 Training: 0.6426393561445003 Seed: 10984\n",
      "Testing: 0.6469346382747736 Training: 0.6450439879189566 Seed: 10986\n",
      "Testing: 0.6472551025563675 Training: 0.6449569896302667 Seed: 10987\n",
      "Testing: 0.6465391473541886 Training: 0.6450024603125389 Seed: 10990\n",
      "Testing: 0.6558889961790138 Training: 0.6427166572440429 Seed: 10991\n",
      "Testing: 0.6455616235626892 Training: 0.6450021728185368 Seed: 10992\n",
      "Testing: 0.6486097246358651 Training: 0.6445133207009363 Seed: 10993\n",
      "Testing: 0.647939306940201 Training: 0.6446835508757608 Seed: 10995\n",
      "Testing: 0.6454635371941385 Training: 0.6452241866206418 Seed: 10997\n",
      "Testing: 0.6510258792524506 Training: 0.6437713678031494 Seed: 11000\n",
      "Testing: 0.6540581548691491 Training: 0.6432020280785211 Seed: 11004\n",
      "Testing: 0.6501686960912645 Training: 0.6441509077533525 Seed: 11005\n",
      "Testing: 0.6575460037946366 Training: 0.6422471361656202 Seed: 11006\n",
      "Testing: 0.6604442047063067 Training: 0.6415353398614492 Seed: 11010\n",
      "Testing: 0.6456174870107083 Training: 0.6449752366431917 Seed: 11012\n",
      "Testing: 0.6606143508755018 Training: 0.6413777153136746 Seed: 11017\n",
      "Testing: 0.6584422473212322 Training: 0.6420741890902011 Seed: 11019\n",
      "Testing: 0.6482901697973782 Training: 0.6446793088720639 Seed: 11020\n",
      "Testing: 0.6555880007085555 Training: 0.6428164879560259 Seed: 11021\n",
      "Testing: 0.6503706613192345 Training: 0.6441945450703028 Seed: 11023\n",
      "Testing: 0.6641959701229346 Training: 0.6404560962614013 Seed: 11025\n",
      "Testing: 0.660633797730489 Training: 0.6415395378859761 Seed: 11029\n",
      "Testing: 0.6519738812866178 Training: 0.6436632429776845 Seed: 11032\n",
      "Testing: 0.6501515176138958 Training: 0.6440854318706382 Seed: 11034\n",
      "Testing: 0.645601914983173 Training: 0.6453261045429285 Seed: 11035\n",
      "Testing: 0.649469181442481 Training: 0.6443618015875152 Seed: 11036\n",
      "Testing: 0.6469260420836531 Training: 0.644970566334607 Seed: 11038\n",
      "Testing: 0.6489482820848456 Training: 0.6444886779559044 Seed: 11039\n",
      "Testing: 0.6497584676667361 Training: 0.6443654631812311 Seed: 11040\n",
      "Testing: 0.6670663031589827 Training: 0.6398349054707744 Seed: 11041\n",
      "Testing: 0.6526857434879314 Training: 0.6429205215549891 Seed: 11047\n",
      "Testing: 0.6470428999149458 Training: 0.644927646346732 Seed: 11050\n",
      "Testing: 0.6631238533411326 Training: 0.6408906383005548 Seed: 11052\n",
      "Testing: 0.6628596540985721 Training: 0.6410396641303975 Seed: 11055\n",
      "Testing: 0.6472748022946657 Training: 0.6449137072658655 Seed: 11057\n",
      "Testing: 0.6705256320775463 Training: 0.6390248710581627 Seed: 11059\n",
      "Testing: 0.652623581995309 Training: 0.6436601367574338 Seed: 11060\n",
      "Testing: 0.6562339304938312 Training: 0.6426605290988603 Seed: 11062\n",
      "Testing: 0.6550072297432339 Training: 0.6429728986076346 Seed: 11067\n",
      "Testing: 0.6578876573672015 Training: 0.6421701494650847 Seed: 11070\n",
      "Testing: 0.6481018554270305 Training: 0.6447627074273952 Seed: 11073\n",
      "Testing: 0.6661677307904135 Training: 0.6402177925459916 Seed: 11074\n",
      "Testing: 0.6485553228567845 Training: 0.6445437143468051 Seed: 11078\n",
      "Testing: 0.6482737132084277 Training: 0.644610658737271 Seed: 11082\n",
      "Testing: 0.6465255672184733 Training: 0.6451507974227697 Seed: 11085\n",
      "Testing: 0.6529284123814757 Training: 0.6434737508766706 Seed: 11086\n",
      "Testing: 0.6463320034016337 Training: 0.6450976948994478 Seed: 11087\n",
      "Testing: 0.6517528476877592 Training: 0.6438282826825351 Seed: 11088\n",
      "Testing: 0.6513250948704591 Training: 0.6438966656937024 Seed: 11092\n",
      "Testing: 0.6586807038013769 Training: 0.6418577114721922 Seed: 11095\n",
      "Testing: 0.6580162847975108 Training: 0.6422842938057683 Seed: 11098\n",
      "Testing: 0.6561021622131215 Training: 0.6427469475472488 Seed: 11100\n",
      "Testing: 0.6512490837654648 Training: 0.643956032293356 Seed: 11102\n",
      "Testing: 0.6635129828892501 Training: 0.6408751041346564 Seed: 11103\n",
      "Testing: 0.6467453444292537 Training: 0.6450221525002973 Seed: 11104\n",
      "Testing: 0.646111857521684 Training: 0.6451688707996173 Seed: 11108\n",
      "Testing: 0.6460161609416295 Training: 0.6452255770901709 Seed: 11109\n",
      "Testing: 0.655246264470274 Training: 0.6429497924884437 Seed: 11110\n",
      "Testing: 0.6480316040200357 Training: 0.6447077527013574 Seed: 11113\n",
      "Testing: 0.6454495639940685 Training: 0.645354146829516 Seed: 11116\n",
      "Testing: 0.6696990824091564 Training: 0.639262372548184 Seed: 11118\n",
      "Testing: 0.6574025068989737 Training: 0.6422505576041337 Seed: 11119\n",
      "Testing: 0.6473741199557415 Training: 0.6448763037919113 Seed: 11120\n",
      "Testing: 0.6597041195796123 Training: 0.6415851001702295 Seed: 11121\n",
      "Testing: 0.6532741643619264 Training: 0.6433937165341779 Seed: 11123\n",
      "Testing: 0.6552366482526134 Training: 0.6429365470482385 Seed: 11130\n",
      "Testing: 0.6493755141292216 Training: 0.6443502592628071 Seed: 11131\n",
      "Testing: 0.6481418713758405 Training: 0.644668120288545 Seed: 11132\n",
      "Testing: 0.6543599851321242 Training: 0.6431419674310817 Seed: 11134\n",
      "Testing: 0.6504845333431102 Training: 0.6441152897767384 Seed: 11135\n",
      "Testing: 0.6461277144988851 Training: 0.6451476877339803 Seed: 11136\n",
      "Testing: 0.650212090613785 Training: 0.6442129418263388 Seed: 11138\n",
      "Testing: 0.6625759753861968 Training: 0.6412396734222933 Seed: 11139\n",
      "Testing: 0.6527133469407123 Training: 0.6435545341106965 Seed: 11141\n",
      "Testing: 0.648274288021756 Training: 0.6446899894507304 Seed: 11145\n",
      "Testing: 0.6517410840471896 Training: 0.6437691616851898 Seed: 11146\n",
      "Testing: 0.6549525093855678 Training: 0.6428082803108175 Seed: 11148\n",
      "Testing: 0.6515210969560588 Training: 0.6436218120818255 Seed: 11150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.657851546922896 Training: 0.6422676598303143 Seed: 11154\n",
      "Testing: 0.6509170370946682 Training: 0.6439943073812504 Seed: 11155\n",
      "Testing: 0.6548962542159601 Training: 0.6430019388788843 Seed: 11158\n",
      "Testing: 0.6539750366618735 Training: 0.6431981347945889 Seed: 11162\n",
      "Testing: 0.652616450509136 Training: 0.6435654319870234 Seed: 11163\n",
      "Testing: 0.6459222530833724 Training: 0.645090440822207 Seed: 11165\n",
      "Testing: 0.6547869767372243 Training: 0.642910723538148 Seed: 11166\n",
      "Testing: 0.6591603813745656 Training: 0.6418577180464398 Seed: 11167\n",
      "Testing: 0.6556704474357946 Training: 0.6428088888322467 Seed: 11168\n",
      "Testing: 0.6570579858804023 Training: 0.642547246227859 Seed: 11170\n",
      "Testing: 0.651868689896967 Training: 0.6437656547451454 Seed: 11173\n",
      "Testing: 0.6589381863491257 Training: 0.6419264323103533 Seed: 11174\n",
      "Testing: 0.647573364661472 Training: 0.6447109629533208 Seed: 11179\n",
      "Testing: 0.6545973174137271 Training: 0.6429580654883759 Seed: 11180\n",
      "Testing: 0.6587351212119065 Training: 0.6420169952620639 Seed: 11181\n",
      "Testing: 0.647150961552424 Training: 0.6447937884094155 Seed: 11182\n",
      "Testing: 0.6532300144493958 Training: 0.6433373717918849 Seed: 11183\n",
      "Testing: 0.6482890489568118 Training: 0.6446865918296805 Seed: 11186\n",
      "Testing: 0.6495612148036306 Training: 0.6444022634104483 Seed: 11187\n",
      "Testing: 0.6563391359105151 Training: 0.642659204254227 Seed: 11189\n",
      "Testing: 0.6502844823084034 Training: 0.6441307378874467 Seed: 11191\n",
      "Testing: 0.6461932384540261 Training: 0.6452078177981355 Seed: 11192\n",
      "Testing: 0.6550084841362767 Training: 0.6430078409712315 Seed: 11197\n",
      "Testing: 0.6649209764281527 Training: 0.6405988125870309 Seed: 11198\n",
      "Testing: 0.647438052270174 Training: 0.6447483629802674 Seed: 11199\n",
      "Testing: 0.664501230176387 Training: 0.6405029678819936 Seed: 11205\n",
      "Testing: 0.6469944696994596 Training: 0.6450037746445314 Seed: 11208\n",
      "Testing: 0.6497685504509519 Training: 0.6441818817729259 Seed: 11209\n",
      "Testing: 0.6477408795367297 Training: 0.6447366313734004 Seed: 11211\n",
      "Testing: 0.6462256441321554 Training: 0.6451358914194127 Seed: 11214\n",
      "Testing: 0.6509663609899897 Training: 0.6439805417119717 Seed: 11216\n",
      "Testing: 0.65003665396638 Training: 0.6442426285561731 Seed: 11219\n",
      "Testing: 0.6492792284856561 Training: 0.6444302854249299 Seed: 11221\n",
      "Testing: 0.6464512146147681 Training: 0.6450975333948062 Seed: 11222\n",
      "Testing: 0.652923904428861 Training: 0.6434294954567276 Seed: 11223\n",
      "Testing: 0.6546954644334017 Training: 0.643005336268515 Seed: 11227\n",
      "Testing: 0.6459044125925444 Training: 0.6452276079957602 Seed: 11228\n",
      "Testing: 0.6479487949389364 Training: 0.6447563269702054 Seed: 11229\n",
      "Testing: 0.6558526955039498 Training: 0.6427519902779539 Seed: 11230\n",
      "Testing: 0.6673492737167563 Training: 0.6398314971723902 Seed: 11231\n",
      "Testing: 0.6490107134920763 Training: 0.6444028090508989 Seed: 11232\n",
      "Testing: 0.6476840406098348 Training: 0.6447787382423362 Seed: 11234\n",
      "Testing: 0.6511809639220649 Training: 0.6439645966084959 Seed: 11237\n",
      "Testing: 0.6526795210783596 Training: 0.643546948793217 Seed: 11238\n",
      "Testing: 0.6565217595383182 Training: 0.6426059791981213 Seed: 11241\n",
      "Testing: 0.667437886234018 Training: 0.6400094432182384 Seed: 11245\n",
      "Testing: 0.6604741232653256 Training: 0.641368185276119 Seed: 11248\n",
      "Testing: 0.6481279603297243 Training: 0.644756805065793 Seed: 11251\n",
      "Testing: 0.6492178356784527 Training: 0.6444195968216325 Seed: 11255\n",
      "Testing: 0.6659924785285223 Training: 0.6402686578701272 Seed: 11258\n",
      "Testing: 0.6548705959315156 Training: 0.6430275931451133 Seed: 11259\n",
      "Testing: 0.6474808279027198 Training: 0.6447040088353431 Seed: 11263\n",
      "Testing: 0.646600408755466 Training: 0.6450822231778597 Seed: 11264\n",
      "Testing: 0.6631875111824787 Training: 0.6407775234926619 Seed: 11268\n",
      "Testing: 0.6516206884932325 Training: 0.6437879874073672 Seed: 11271\n",
      "Testing: 0.6617545938343973 Training: 0.6411614902131031 Seed: 11272\n",
      "Testing: 0.6507721021503095 Training: 0.6439754445568909 Seed: 11273\n",
      "Testing: 0.6503052788152353 Training: 0.6440508529099547 Seed: 11274\n",
      "Testing: 0.6471850315399175 Training: 0.6448838333665328 Seed: 11276\n",
      "Testing: 0.6591716059162247 Training: 0.6419407000338586 Seed: 11277\n",
      "Testing: 0.6473906208409503 Training: 0.6448286953625089 Seed: 11278\n",
      "Testing: 0.6643763627953214 Training: 0.6407397327960952 Seed: 11279\n",
      "Testing: 0.6517421509402703 Training: 0.6437402904170336 Seed: 11280\n",
      "Testing: 0.6533845366754902 Training: 0.6432531288197612 Seed: 11282\n",
      "Testing: 0.6504571834185925 Training: 0.6441722905559362 Seed: 11283\n",
      "Testing: 0.6491589118810719 Training: 0.6444826191184606 Seed: 11284\n",
      "Testing: 0.6462475391217067 Training: 0.6451027895999879 Seed: 11285\n",
      "Testing: 0.6495912317538005 Training: 0.644241610176216 Seed: 11291\n",
      "Testing: 0.6516744064703703 Training: 0.643798646175918 Seed: 11292\n",
      "Testing: 0.6606825838401914 Training: 0.6416222524986026 Seed: 11294\n",
      "Testing: 0.6523499687174757 Training: 0.6436148270625941 Seed: 11298\n",
      "Testing: 0.6531242565573667 Training: 0.643432231281773 Seed: 11300\n",
      "Testing: 0.6762177836883289 Training: 0.6378562363726188 Seed: 11301\n",
      "Testing: 0.6535671083052809 Training: 0.6433426260375641 Seed: 11302\n",
      "Testing: 0.651645214996424 Training: 0.6438500357625377 Seed: 11308\n",
      "Testing: 0.6564330036604182 Training: 0.6425718568026995 Seed: 11310\n",
      "Testing: 0.6610911885148791 Training: 0.6412209366109779 Seed: 11313\n",
      "Testing: 0.6541641619395155 Training: 0.643179185326072 Seed: 11315\n",
      "Testing: 0.6504478836548868 Training: 0.644048314515266 Seed: 11316\n",
      "Testing: 0.6456694793155341 Training: 0.6452301629410448 Seed: 11318\n",
      "Testing: 0.6560363088105119 Training: 0.6428527983270833 Seed: 11322\n",
      "Testing: 0.6601432960492852 Training: 0.6416597655069218 Seed: 11323\n",
      "Testing: 0.6565094770950403 Training: 0.6426729924892232 Seed: 11324\n",
      "Testing: 0.6597431115501083 Training: 0.6417638027272333 Seed: 11325\n",
      "Testing: 0.6499321504666097 Training: 0.6442149752830904 Seed: 11329\n",
      "Testing: 0.6456896506343726 Training: 0.645337368071229 Seed: 11331\n",
      "Testing: 0.6492336043021113 Training: 0.6443152632362559 Seed: 11338\n",
      "Testing: 0.6458914253801424 Training: 0.6452834085595238 Seed: 11339\n",
      "Testing: 0.6573304818324383 Training: 0.6423893431751486 Seed: 11343\n",
      "Testing: 0.6501860124192401 Training: 0.6441337127028604 Seed: 11344\n",
      "Testing: 0.6529185401964086 Training: 0.6435060787675841 Seed: 11345\n",
      "Testing: 0.6492749609097828 Training: 0.6443760839046242 Seed: 11346\n",
      "Testing: 0.6486176698795614 Training: 0.6445182563046653 Seed: 11351\n",
      "Testing: 0.6532088400388072 Training: 0.643320667852623 Seed: 11353\n",
      "Testing: 0.6529961719610544 Training: 0.6435303478144478 Seed: 11354\n",
      "Testing: 0.6458811813489109 Training: 0.6453227945839052 Seed: 11357\n",
      "Testing: 0.6560720030105565 Training: 0.6424423808090768 Seed: 11358\n",
      "Testing: 0.6650998686898966 Training: 0.6402950554104108 Seed: 11361\n",
      "Testing: 0.6470840463774878 Training: 0.6449641660296376 Seed: 11362\n",
      "Testing: 0.6487168908560665 Training: 0.6445509300940325 Seed: 11363\n",
      "Testing: 0.65965545065995 Training: 0.6416272688777758 Seed: 11366\n",
      "Testing: 0.6496107231976103 Training: 0.6443301178222437 Seed: 11367\n",
      "Testing: 0.6469354621545801 Training: 0.644982979525287 Seed: 11371\n",
      "Testing: 0.6517118516141158 Training: 0.6437323212799224 Seed: 11372\n",
      "Testing: 0.6458279493587491 Training: 0.6450311386433473 Seed: 11377\n",
      "Testing: 0.6469990479491661 Training: 0.6449341045316792 Seed: 11378\n",
      "Testing: 0.6467152845976354 Training: 0.6447913184833038 Seed: 11379\n",
      "Testing: 0.6646657711080918 Training: 0.6405336457660796 Seed: 11382\n",
      "Testing: 0.6593773453338893 Training: 0.6416396273435674 Seed: 11383\n",
      "Testing: 0.6554645154579005 Training: 0.6427558534718918 Seed: 11385\n",
      "Testing: 0.6504735024485189 Training: 0.6440505845726923 Seed: 11389\n",
      "Testing: 0.6599536649643949 Training: 0.6415406599113733 Seed: 11391\n",
      "Testing: 0.6524090155086717 Training: 0.6436155051159783 Seed: 11392\n",
      "Testing: 0.6648631043818034 Training: 0.6405624703113768 Seed: 11394\n",
      "Testing: 0.6472681387121555 Training: 0.6449520808988631 Seed: 11396\n",
      "Testing: 0.6582879350891219 Training: 0.6419847052393745 Seed: 11397\n",
      "Testing: 0.6554267041179526 Training: 0.6427518862894929 Seed: 11398\n",
      "Testing: 0.6504573923289196 Training: 0.6439300012708655 Seed: 11401\n",
      "Testing: 0.6633651839817721 Training: 0.6403016379372194 Seed: 11402\n",
      "Testing: 0.6551164500490162 Training: 0.6429878819028586 Seed: 11404\n",
      "Testing: 0.6514299590304277 Training: 0.6438720269644049 Seed: 11406\n",
      "Testing: 0.6553569491493291 Training: 0.6428423673582426 Seed: 11408\n",
      "Testing: 0.6459795703165986 Training: 0.645217165563829 Seed: 11409\n",
      "Testing: 0.6475006145800573 Training: 0.6447741219388107 Seed: 11414\n",
      "Testing: 0.653512586797915 Training: 0.6432457736650339 Seed: 11417\n",
      "Testing: 0.6636061411210041 Training: 0.6408110956954307 Seed: 11418\n",
      "Testing: 0.651713662678296 Training: 0.6436583420806081 Seed: 11419\n",
      "Testing: 0.6498320012231669 Training: 0.6441903610213049 Seed: 11421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6478478031150766 Training: 0.6447025863061764 Seed: 11424\n",
      "Testing: 0.6564500225159768 Training: 0.6424894204907908 Seed: 11428\n",
      "Testing: 0.6477840596494452 Training: 0.6448145693813687 Seed: 11429\n",
      "Testing: 0.6580688087701405 Training: 0.6422753651559864 Seed: 11432\n",
      "Testing: 0.65326366883376 Training: 0.6434426533802334 Seed: 11434\n",
      "Testing: 0.653567243078877 Training: 0.6432795636980304 Seed: 11439\n",
      "Testing: 0.6485950759301455 Training: 0.6445917113944775 Seed: 11440\n",
      "Testing: 0.6461772214541621 Training: 0.6451719402046079 Seed: 11441\n",
      "Testing: 0.6652178181999165 Training: 0.6402563368894668 Seed: 11443\n",
      "Testing: 0.6735659135466843 Training: 0.6382047030697608 Seed: 11447\n",
      "Testing: 0.66973464079596 Training: 0.6391926410840456 Seed: 11448\n",
      "Testing: 0.6455299804153116 Training: 0.6449634383594182 Seed: 11452\n",
      "Testing: 0.6575666912300392 Training: 0.6422999236800131 Seed: 11455\n",
      "Testing: 0.6506391568348515 Training: 0.6440645158049273 Seed: 11456\n",
      "Testing: 0.6494098964492285 Training: 0.6441566421679262 Seed: 11457\n",
      "Testing: 0.646904614500784 Training: 0.6449283476184515 Seed: 11460\n",
      "Testing: 0.6539052849082891 Training: 0.6433519960256474 Seed: 11462\n",
      "Testing: 0.65666200810205 Training: 0.6425729079295333 Seed: 11463\n",
      "Testing: 0.6505825414841975 Training: 0.6439347458772717 Seed: 11464\n",
      "Testing: 0.6482501086328644 Training: 0.6446521271317569 Seed: 11466\n",
      "Testing: 0.6541548225164969 Training: 0.6432156041977894 Seed: 11467\n",
      "Testing: 0.6506238239392271 Training: 0.6440295686403805 Seed: 11468\n",
      "Testing: 0.6536345764998124 Training: 0.6432560064717414 Seed: 11469\n",
      "Testing: 0.6654785527891686 Training: 0.6403324128703385 Seed: 11470\n",
      "Testing: 0.6595530699008876 Training: 0.6419742690228738 Seed: 11471\n",
      "Testing: 0.6490310403156055 Training: 0.6444465697266839 Seed: 11472\n",
      "Testing: 0.6469803639458707 Training: 0.6449623411130576 Seed: 11473\n",
      "Testing: 0.649065784914373 Training: 0.6443716157569659 Seed: 11474\n",
      "Testing: 0.6473829882194114 Training: 0.6449096914304641 Seed: 11477\n",
      "Testing: 0.6499966075446754 Training: 0.6442092954822695 Seed: 11478\n",
      "Testing: 0.6562847794771655 Training: 0.6424339174419178 Seed: 11479\n",
      "Testing: 0.6508994365954849 Training: 0.6440332733053383 Seed: 11481\n",
      "Testing: 0.652377084831278 Training: 0.6436076762736807 Seed: 11482\n",
      "Testing: 0.6619394410137786 Training: 0.6412867482227579 Seed: 11483\n",
      "Testing: 0.6607444231776477 Training: 0.6412564051755416 Seed: 11487\n",
      "Testing: 0.6518108925827177 Training: 0.6436680160251878 Seed: 11490\n",
      "Testing: 0.6485948881602006 Training: 0.6438177240413172 Seed: 11495\n",
      "Testing: 0.6497984299181596 Training: 0.6442691028020326 Seed: 11496\n",
      "Testing: 0.6541708941698907 Training: 0.643230576677297 Seed: 11500\n",
      "Testing: 0.6517388392173923 Training: 0.643636350766003 Seed: 11502\n",
      "Testing: 0.660096454097395 Training: 0.6416165740358418 Seed: 11503\n",
      "Testing: 0.660787608184135 Training: 0.6415640792276536 Seed: 11506\n",
      "Testing: 0.6480818313364256 Training: 0.6446656898302578 Seed: 11507\n",
      "Testing: 0.6472090353275587 Training: 0.6449041777785597 Seed: 11509\n",
      "Testing: 0.6508783481438163 Training: 0.6440051382231663 Seed: 11511\n",
      "Testing: 0.6613183352479799 Training: 0.6413445470749849 Seed: 11512\n",
      "Testing: 0.6471604644562132 Training: 0.6448492337920703 Seed: 11515\n",
      "Testing: 0.6730911862000099 Training: 0.6382489379568669 Seed: 11518\n",
      "Testing: 0.6517559942960337 Training: 0.6438514217467233 Seed: 11526\n",
      "Testing: 0.6454360768887977 Training: 0.6454064818475447 Seed: 11530\n",
      "Testing: 0.650062360682827 Training: 0.6440992487991186 Seed: 11532\n",
      "Testing: 0.6563726673332123 Training: 0.6427335906566041 Seed: 11538\n",
      "Testing: 0.6555965154968704 Training: 0.6424527365209595 Seed: 11540\n",
      "Testing: 0.6530474356547659 Training: 0.6435362178178508 Seed: 11543\n",
      "Testing: 0.6517589663977638 Training: 0.6437172820872092 Seed: 11544\n",
      "Testing: 0.6597545622988622 Training: 0.6417248658598382 Seed: 11550\n",
      "Testing: 0.6492998920856912 Training: 0.6444109382648168 Seed: 11551\n",
      "Testing: 0.6494467020107746 Training: 0.6443409703811828 Seed: 11552\n",
      "Testing: 0.650481179533665 Training: 0.644115400978134 Seed: 11553\n",
      "Testing: 0.6616122781831856 Training: 0.6408497640483328 Seed: 11554\n",
      "Testing: 0.6461789840438563 Training: 0.6451465286765139 Seed: 11556\n",
      "Testing: 0.6514939649132799 Training: 0.6437822047578 Seed: 11559\n",
      "Testing: 0.655976843204569 Training: 0.6426450829189154 Seed: 11561\n",
      "Testing: 0.6554697043006297 Training: 0.6428851434361932 Seed: 11562\n",
      "Testing: 0.6592683149696867 Training: 0.6417809952879365 Seed: 11564\n",
      "Testing: 0.6562191323381974 Training: 0.6427172473876566 Seed: 11566\n",
      "Testing: 0.6468964950711015 Training: 0.644983627848416 Seed: 11571\n",
      "Testing: 0.6505005223316768 Training: 0.644131979548238 Seed: 11572\n",
      "Testing: 0.6462286726048349 Training: 0.645175740028004 Seed: 11575\n",
      "Testing: 0.6463600366183684 Training: 0.6451528068047214 Seed: 11576\n",
      "Testing: 0.6514655971427764 Training: 0.643883312665618 Seed: 11579\n",
      "Testing: 0.6468359510026367 Training: 0.6450145100517337 Seed: 11582\n",
      "Testing: 0.6481026731727255 Training: 0.6447383406318332 Seed: 11587\n",
      "Testing: 0.6491753515321631 Training: 0.6443456193055761 Seed: 11589\n",
      "Testing: 0.6477130912792926 Training: 0.6448353228157147 Seed: 11590\n",
      "Testing: 0.6492074407673634 Training: 0.6442820770161866 Seed: 11591\n",
      "Testing: 0.6478408980536017 Training: 0.6447412873113353 Seed: 11595\n",
      "Testing: 0.653018436496779 Training: 0.6435867861802167 Seed: 11597\n",
      "Testing: 0.6467071037451076 Training: 0.6450834632177452 Seed: 11598\n",
      "Testing: 0.6506766244589848 Training: 0.6439973710451039 Seed: 11599\n",
      "Testing: 0.6616092439526339 Training: 0.6412612340349301 Seed: 11600\n",
      "Testing: 0.6459247959726044 Training: 0.6452741439888972 Seed: 11602\n",
      "Testing: 0.6524823326434215 Training: 0.6436499817286664 Seed: 11604\n",
      "Testing: 0.6575799754366696 Training: 0.6423456657060737 Seed: 11605\n",
      "Testing: 0.6548699190613023 Training: 0.6429762824379193 Seed: 11606\n",
      "Testing: 0.6637423910046951 Training: 0.6407329857491716 Seed: 11607\n",
      "Testing: 0.6460496577604802 Training: 0.6449429092392416 Seed: 11610\n",
      "Testing: 0.6611122348018064 Training: 0.641564359580005 Seed: 11613\n",
      "Testing: 0.6480274337790274 Training: 0.6447450476925011 Seed: 11617\n",
      "Testing: 0.645866804654073 Training: 0.6451501098222973 Seed: 11619\n",
      "Testing: 0.6506029596201053 Training: 0.6441892400123109 Seed: 11621\n",
      "Testing: 0.6571513082832011 Training: 0.6424971466913412 Seed: 11622\n",
      "Testing: 0.6523784363189138 Training: 0.6435459502969552 Seed: 11627\n",
      "Testing: 0.6698224774352322 Training: 0.6390970514956253 Seed: 11628\n",
      "Testing: 0.6469710419045631 Training: 0.6449211417284512 Seed: 11629\n",
      "Testing: 0.6469554583797164 Training: 0.645014411987003 Seed: 11636\n",
      "Testing: 0.6581882588291242 Training: 0.6421403934236293 Seed: 11638\n",
      "Testing: 0.6468983466088738 Training: 0.6449665843813103 Seed: 11640\n",
      "Testing: 0.6461021074686972 Training: 0.6451975930794338 Seed: 11643\n",
      "Testing: 0.6561421851035665 Training: 0.6426443696727908 Seed: 11645\n",
      "Testing: 0.6455383270131254 Training: 0.6453197814046029 Seed: 11646\n",
      "Testing: 0.6594024351461055 Training: 0.6417513090534268 Seed: 11648\n",
      "Testing: 0.6543149592966098 Training: 0.6426321980062624 Seed: 11649\n",
      "Testing: 0.6473596867756374 Training: 0.6446038161296898 Seed: 11650\n",
      "Testing: 0.648479254652623 Training: 0.6445626656579289 Seed: 11653\n",
      "Testing: 0.6629036347884587 Training: 0.6412330087118063 Seed: 11656\n",
      "Testing: 0.648788562559591 Training: 0.6443589817259919 Seed: 11657\n",
      "Testing: 0.6511737838184617 Training: 0.6438821281474127 Seed: 11658\n",
      "Testing: 0.6463279869670058 Training: 0.6451322378454872 Seed: 11659\n",
      "Testing: 0.6472259414046422 Training: 0.6449856108202043 Seed: 11660\n",
      "Testing: 0.6456789889125254 Training: 0.6452122322185079 Seed: 11667\n",
      "Testing: 0.6592326596980524 Training: 0.6417609031464028 Seed: 11668\n",
      "Testing: 0.6609788888383841 Training: 0.6415001283564701 Seed: 11669\n",
      "Testing: 0.6464822099119923 Training: 0.645011799503477 Seed: 11672\n",
      "Testing: 0.6625724933128546 Training: 0.6410192893661582 Seed: 11673\n",
      "Testing: 0.6483150396807635 Training: 0.6445246754534039 Seed: 11674\n",
      "Testing: 0.6647064546362854 Training: 0.6403771357458711 Seed: 11675\n",
      "Testing: 0.6545581124678373 Training: 0.6430542542209787 Seed: 11678\n",
      "Testing: 0.6574100496039983 Training: 0.6421838131775729 Seed: 11680\n",
      "Testing: 0.6649985490241477 Training: 0.6404010836773519 Seed: 11681\n",
      "Testing: 0.6455666717046786 Training: 0.6451514749298181 Seed: 11682\n",
      "Testing: 0.6474258297256775 Training: 0.6448264459515123 Seed: 11683\n",
      "Testing: 0.6546857651721294 Training: 0.6429860960162153 Seed: 11684\n",
      "Testing: 0.6583446183961186 Training: 0.6419561762146526 Seed: 11685\n",
      "Testing: 0.6551054053864624 Training: 0.642832918100684 Seed: 11687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6469893558020965 Training: 0.6449755223917671 Seed: 11688\n",
      "Testing: 0.6472653562244222 Training: 0.6449189104044328 Seed: 11689\n",
      "Testing: 0.651378041142264 Training: 0.6437963596594292 Seed: 11691\n",
      "Testing: 0.6458266832738493 Training: 0.6453188190569527 Seed: 11692\n",
      "Testing: 0.6526906618315101 Training: 0.6435636016137903 Seed: 11693\n",
      "Testing: 0.6509520608155177 Training: 0.6440708502102545 Seed: 11696\n",
      "Testing: 0.6473954066695362 Training: 0.6449238949495666 Seed: 11697\n",
      "Testing: 0.6564429180440126 Training: 0.6427738643975246 Seed: 11701\n",
      "Testing: 0.6593703789509855 Training: 0.6418266089051713 Seed: 11702\n",
      "Testing: 0.6580408150367829 Training: 0.6423054813288107 Seed: 11703\n",
      "Testing: 0.6480006146680992 Training: 0.6447263328356024 Seed: 11705\n",
      "Testing: 0.6513793796391125 Training: 0.6439238253897129 Seed: 11706\n",
      "Testing: 0.651654870072192 Training: 0.6436679546237891 Seed: 11707\n",
      "Testing: 0.6623314648320658 Training: 0.6406718302226511 Seed: 11708\n",
      "Testing: 0.6493081617131881 Training: 0.6443996937334058 Seed: 11709\n",
      "Testing: 0.6540928817670504 Training: 0.6431712004643494 Seed: 11712\n",
      "Testing: 0.6462092570624633 Training: 0.6452543484452927 Seed: 11714\n",
      "Testing: 0.661294627999213 Training: 0.6413920027282322 Seed: 11716\n",
      "Testing: 0.6472745495119592 Training: 0.6448026022110197 Seed: 11718\n",
      "Testing: 0.6665440676369739 Training: 0.639913903040345 Seed: 11719\n",
      "Testing: 0.6455794804240457 Training: 0.6452580480772978 Seed: 11724\n",
      "Testing: 0.6502523203405729 Training: 0.6440876779164755 Seed: 11725\n",
      "Testing: 0.6548191195905055 Training: 0.6430665887757491 Seed: 11726\n",
      "Testing: 0.646625601517413 Training: 0.6450744732787473 Seed: 11730\n",
      "Testing: 0.648757456974919 Training: 0.6440299089109662 Seed: 11731\n",
      "Testing: 0.6537399278610243 Training: 0.6433304942294757 Seed: 11732\n",
      "Testing: 0.6660542877291051 Training: 0.6402866951811477 Seed: 11733\n",
      "Testing: 0.6469596476376938 Training: 0.6450073941966639 Seed: 11736\n",
      "Testing: 0.65089418771778 Training: 0.6439967487826634 Seed: 11738\n",
      "Testing: 0.6560157749388599 Training: 0.6426102447332847 Seed: 11739\n",
      "Testing: 0.6619302327962793 Training: 0.6413554383882374 Seed: 11741\n",
      "Testing: 0.6598843573504638 Training: 0.6418271976421901 Seed: 11743\n",
      "Testing: 0.6497934350133365 Training: 0.6442726333707158 Seed: 11753\n",
      "Testing: 0.6541612411847784 Training: 0.6431664046730224 Seed: 11754\n",
      "Testing: 0.6555993613045014 Training: 0.6427855382494602 Seed: 11755\n",
      "Testing: 0.6479664273419055 Training: 0.6447399700454189 Seed: 11759\n",
      "Testing: 0.6592803303647209 Training: 0.6418286804599751 Seed: 11760\n",
      "Testing: 0.6474967079249232 Training: 0.6448788384963122 Seed: 11764\n",
      "Testing: 0.6630493451793166 Training: 0.6410801987548613 Seed: 11766\n",
      "Testing: 0.6475514114008951 Training: 0.6449052700392712 Seed: 11770\n",
      "Testing: 0.6560742964523765 Training: 0.6426521682899791 Seed: 11772\n",
      "Testing: 0.657191074938114 Training: 0.642463045023684 Seed: 11773\n",
      "Testing: 0.6614555578672625 Training: 0.6412667025400385 Seed: 11777\n",
      "Testing: 0.6476295980479418 Training: 0.6448777803920099 Seed: 11781\n",
      "Testing: 0.6521415568749493 Training: 0.64366474807089 Seed: 11782\n",
      "Testing: 0.645439474729771 Training: 0.6454074054262632 Seed: 11783\n",
      "Testing: 0.6458257837699966 Training: 0.6452654858360383 Seed: 11787\n",
      "Testing: 0.6492414611983741 Training: 0.6442912523173393 Seed: 11792\n",
      "Testing: 0.6556436468019953 Training: 0.6427762960753522 Seed: 11793\n",
      "Testing: 0.6509660736047522 Training: 0.6438114220296645 Seed: 11794\n",
      "Testing: 0.658507032941883 Training: 0.6416316560797007 Seed: 11795\n",
      "Testing: 0.6506667412557575 Training: 0.6439699630578914 Seed: 11796\n",
      "Testing: 0.6560074794423897 Training: 0.6428100585736919 Seed: 11799\n",
      "Testing: 0.6494198860893021 Training: 0.6443630141093294 Seed: 11800\n",
      "Testing: 0.6459135910795126 Training: 0.6452610187555603 Seed: 11802\n",
      "Testing: 0.6499216415160063 Training: 0.6442007297179408 Seed: 11808\n",
      "Testing: 0.6455185568608206 Training: 0.6453252880196585 Seed: 11810\n",
      "Testing: 0.6499848323323006 Training: 0.6441907474765094 Seed: 11813\n",
      "Testing: 0.6518014880513072 Training: 0.6437098346105081 Seed: 11814\n",
      "Testing: 0.6580936453743917 Training: 0.6422563074517555 Seed: 11823\n",
      "Testing: 0.6548682549631653 Training: 0.6428834697609894 Seed: 11824\n",
      "Testing: 0.6464159588393272 Training: 0.6451138869130356 Seed: 11832\n",
      "Testing: 0.6492589351210973 Training: 0.6443484563773283 Seed: 11833\n",
      "Testing: 0.6463256174630769 Training: 0.6452051202934745 Seed: 11837\n",
      "Testing: 0.6589857194896679 Training: 0.6418696529699299 Seed: 11838\n",
      "Testing: 0.6708197020989057 Training: 0.6386100912920037 Seed: 11839\n",
      "Testing: 0.650068850712178 Training: 0.6441732424318144 Seed: 11840\n",
      "Testing: 0.6478063987892136 Training: 0.6448475577385441 Seed: 11842\n",
      "Testing: 0.6558901421568226 Training: 0.6426247373048722 Seed: 11843\n",
      "Testing: 0.6533010465314576 Training: 0.6432823891535692 Seed: 11844\n",
      "Testing: 0.6483059698625666 Training: 0.6446280155427031 Seed: 11845\n",
      "Testing: 0.6529036309720413 Training: 0.6434212870076419 Seed: 11846\n",
      "Testing: 0.6497379647239658 Training: 0.6439689366973829 Seed: 11849\n",
      "Testing: 0.6494697407148383 Training: 0.6442454457597159 Seed: 11851\n",
      "Testing: 0.6615637107541377 Training: 0.6410788737445565 Seed: 11859\n",
      "Testing: 0.6597076864430029 Training: 0.6415643353933975 Seed: 11861\n",
      "Testing: 0.6699472440779529 Training: 0.6388832500606507 Seed: 11864\n",
      "Testing: 0.6565063280404929 Training: 0.6426859859336682 Seed: 11865\n",
      "Testing: 0.657715248356579 Training: 0.6422728430791669 Seed: 11870\n",
      "Testing: 0.6666956382944582 Training: 0.639893149244726 Seed: 11872\n",
      "Testing: 0.6677885464928301 Training: 0.6397216886385839 Seed: 11873\n",
      "Testing: 0.6485872354882636 Training: 0.6445486696756915 Seed: 11874\n",
      "Testing: 0.6524304402839785 Training: 0.6436715016036963 Seed: 11876\n",
      "Testing: 0.6645910927783691 Training: 0.6404687465297194 Seed: 11877\n",
      "Testing: 0.647484558305971 Training: 0.644815243281492 Seed: 11879\n",
      "Testing: 0.6459100981393597 Training: 0.645282159473073 Seed: 11883\n",
      "Testing: 0.6493034334670411 Training: 0.6443274068457021 Seed: 11884\n",
      "Testing: 0.6583365454008515 Training: 0.6419441643629491 Seed: 11885\n",
      "Testing: 0.6592968889057633 Training: 0.6416882517158873 Seed: 11886\n",
      "Testing: 0.6462467237009576 Training: 0.6451623208016727 Seed: 11887\n",
      "Testing: 0.6474399150018083 Training: 0.6448331063079366 Seed: 11890\n",
      "Testing: 0.6500851821081416 Training: 0.6442844673776189 Seed: 11891\n",
      "Testing: 0.6641265979894755 Training: 0.6407605146582289 Seed: 11892\n",
      "Testing: 0.64910859318397 Training: 0.6444629090419625 Seed: 11893\n",
      "Testing: 0.6557613001238644 Training: 0.6427825119033703 Seed: 11894\n",
      "Testing: 0.6453919409212618 Training: 0.6453710277718668 Seed: 11897\n",
      "Testing: 0.6469662107679404 Training: 0.6450327610515553 Seed: 11898\n",
      "Testing: 0.6482930253550868 Training: 0.644599669341777 Seed: 11899\n",
      "Testing: 0.6500259371530783 Training: 0.6441945863193074 Seed: 11900\n",
      "Testing: 0.6634253991328007 Training: 0.6406150045532462 Seed: 11901\n",
      "Testing: 0.6515094794268352 Training: 0.6435028757879833 Seed: 11902\n",
      "Testing: 0.6645780420100019 Training: 0.6404523980037105 Seed: 11903\n",
      "Testing: 0.6496790106088539 Training: 0.6441811467954632 Seed: 11904\n",
      "Testing: 0.6540989195227134 Training: 0.6432039931293541 Seed: 11906\n",
      "Testing: 0.6668997040355155 Training: 0.6400761903185985 Seed: 11907\n",
      "Testing: 0.6514107898100754 Training: 0.6439073024609461 Seed: 11908\n",
      "Testing: 0.6461490619039866 Training: 0.6451616274060066 Seed: 11909\n",
      "Testing: 0.6558988950411027 Training: 0.642827611362 Seed: 11910\n",
      "Testing: 0.659430519198539 Training: 0.6417924118955012 Seed: 11911\n",
      "Testing: 0.6486291258974686 Training: 0.6445805419558787 Seed: 11914\n",
      "Testing: 0.6461025540486348 Training: 0.6451707762096015 Seed: 11916\n",
      "Testing: 0.660994031000389 Training: 0.6414844803833519 Seed: 11917\n",
      "Testing: 0.6526007518030447 Training: 0.6434922277705637 Seed: 11919\n",
      "Testing: 0.665770377303431 Training: 0.64044840711791 Seed: 11920\n",
      "Testing: 0.6576677725582863 Training: 0.642257083985857 Seed: 11924\n",
      "Testing: 0.6667039174057526 Training: 0.6398075384172788 Seed: 11927\n",
      "Testing: 0.6581836312363857 Training: 0.6420190742013195 Seed: 11928\n",
      "Testing: 0.6520747177365154 Training: 0.6436711294875956 Seed: 11929\n",
      "Testing: 0.6492175400144213 Training: 0.6444622738712737 Seed: 11930\n",
      "Testing: 0.6608975204808059 Training: 0.6415597317547314 Seed: 11931\n",
      "Testing: 0.666374272721186 Training: 0.6400544811213862 Seed: 11935\n",
      "Testing: 0.656378516943725 Training: 0.6426969480904309 Seed: 11937\n",
      "Testing: 0.6545248916284778 Training: 0.6429720490978967 Seed: 11938\n",
      "Testing: 0.6542923743488276 Training: 0.6432604273570777 Seed: 11939\n",
      "Testing: 0.6553765325718552 Training: 0.6428643366335862 Seed: 11940\n",
      "Testing: 0.6536771845145469 Training: 0.6431060176830367 Seed: 11941\n",
      "Testing: 0.6574563993445546 Training: 0.6423822839208166 Seed: 11943\n",
      "Testing: 0.6604974003341088 Training: 0.6416702858147736 Seed: 11945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6611541779372127 Training: 0.6414686659834891 Seed: 11947\n",
      "Testing: 0.6530252296648794 Training: 0.6433977617632204 Seed: 11948\n",
      "Testing: 0.6571523483522466 Training: 0.6425939435803061 Seed: 11950\n",
      "Testing: 0.6454656488118791 Training: 0.6453291479549612 Seed: 11953\n",
      "Testing: 0.645673300955355 Training: 0.6453530745823701 Seed: 11954\n",
      "Testing: 0.6494741165690866 Training: 0.6443982666555178 Seed: 11956\n",
      "Testing: 0.6615034561029598 Training: 0.6407788390569545 Seed: 11957\n",
      "Testing: 0.6696822720804978 Training: 0.6390361052579228 Seed: 11960\n",
      "Testing: 0.6575959165248785 Training: 0.6422893079212324 Seed: 11961\n",
      "Testing: 0.6611957237995332 Training: 0.64133209079102 Seed: 11962\n",
      "Testing: 0.656366628296972 Training: 0.642471438997521 Seed: 11964\n",
      "Testing: 0.6500766066522123 Training: 0.6442265336437938 Seed: 11966\n",
      "Testing: 0.6515570702350899 Training: 0.6435828102678516 Seed: 11967\n",
      "Testing: 0.6518388981216258 Training: 0.6436833752365536 Seed: 11968\n",
      "Testing: 0.6456950874289369 Training: 0.6451330037508787 Seed: 11970\n",
      "Testing: 0.6460158753880256 Training: 0.6450415587794434 Seed: 11971\n",
      "Testing: 0.6499060841834066 Training: 0.6438808878163175 Seed: 11974\n",
      "Testing: 0.6649699748201217 Training: 0.6403546054145669 Seed: 11981\n",
      "Testing: 0.6468439444242102 Training: 0.6450758881427714 Seed: 11982\n",
      "Testing: 0.6654576802652793 Training: 0.6401283128478121 Seed: 11984\n",
      "Testing: 0.6645421463568975 Training: 0.6405020724594962 Seed: 11988\n",
      "Testing: 0.6481878707069495 Training: 0.6447468841035474 Seed: 11990\n",
      "Testing: 0.6615133923947543 Training: 0.6413488656859727 Seed: 11993\n",
      "Testing: 0.6589698430021333 Training: 0.6418147108591807 Seed: 11995\n",
      "Testing: 0.6472179067233312 Training: 0.6449623897706303 Seed: 11998\n",
      "Testing: 0.649933850142628 Training: 0.6442816759210526 Seed: 11999\n",
      "Testing: 0.6545010613953863 Training: 0.6428391165948815 Seed: 12000\n",
      "Testing: 0.6509644727860893 Training: 0.6440255009078708 Seed: 12001\n",
      "Testing: 0.6614494816244806 Training: 0.6413759272757271 Seed: 12002\n",
      "Testing: 0.6483211001924518 Training: 0.6446007677871636 Seed: 12004\n",
      "Testing: 0.645885070732244 Training: 0.6453175833508727 Seed: 12005\n",
      "Testing: 0.655107456683003 Training: 0.6428918644429651 Seed: 12006\n",
      "Testing: 0.6590642522721564 Training: 0.6421285866953703 Seed: 12007\n",
      "Testing: 0.6480313427065044 Training: 0.6447712118730858 Seed: 12008\n",
      "Testing: 0.648214128819654 Training: 0.6447403358954434 Seed: 12009\n",
      "Testing: 0.6486656670956834 Training: 0.6445219669303864 Seed: 12012\n",
      "Testing: 0.6535418138018534 Training: 0.6434331160678949 Seed: 12013\n",
      "Testing: 0.6546934737599475 Training: 0.6430571640480828 Seed: 12018\n",
      "Testing: 0.6667777796688114 Training: 0.6397686149467048 Seed: 12019\n",
      "Testing: 0.662099736874064 Training: 0.6412201909315043 Seed: 12021\n",
      "Testing: 0.655067103937014 Training: 0.6428245722224384 Seed: 12022\n",
      "Testing: 0.657617510687493 Training: 0.6422668125124789 Seed: 12023\n",
      "Testing: 0.6483467698440634 Training: 0.644160897366326 Seed: 12024\n",
      "Testing: 0.6502267802953394 Training: 0.6441080067751512 Seed: 12025\n",
      "Testing: 0.6589902055007673 Training: 0.6419762861061982 Seed: 12027\n",
      "Testing: 0.6501045831185249 Training: 0.644084449388387 Seed: 12031\n",
      "Testing: 0.6502009108725594 Training: 0.6439321962942262 Seed: 12034\n",
      "Testing: 0.649568005178993 Training: 0.644299653197884 Seed: 12035\n",
      "Testing: 0.6503825573579127 Training: 0.6440925063281696 Seed: 12038\n",
      "Testing: 0.6495182252455641 Training: 0.6443957679636805 Seed: 12039\n",
      "Testing: 0.6540518427419789 Training: 0.6432206425731726 Seed: 12044\n",
      "Testing: 0.6455009884751088 Training: 0.6453222357461305 Seed: 12046\n",
      "Testing: 0.6518507947463336 Training: 0.6438230742377808 Seed: 12048\n",
      "Testing: 0.6473414005844289 Training: 0.6449234475068009 Seed: 12052\n",
      "Testing: 0.6549254741677671 Training: 0.6430239254174356 Seed: 12054\n",
      "Testing: 0.6588257885415012 Training: 0.6419787315863155 Seed: 12058\n",
      "Testing: 0.651672796076203 Training: 0.6436851075310605 Seed: 12060\n",
      "Testing: 0.6477625328635455 Training: 0.6447309561595689 Seed: 12061\n",
      "Testing: 0.6488046685190663 Training: 0.6445098195157009 Seed: 12062\n",
      "Testing: 0.6535845874634574 Training: 0.6432352054714783 Seed: 12063\n",
      "Testing: 0.6595693196046066 Training: 0.6418186321198507 Seed: 12064\n",
      "Testing: 0.6517614292929558 Training: 0.6437753387323405 Seed: 12065\n",
      "Testing: 0.6487309452734867 Training: 0.6445608892154174 Seed: 12066\n",
      "Testing: 0.649499034319929 Training: 0.6443236422428922 Seed: 12069\n",
      "Testing: 0.6528226249010726 Training: 0.6435233611094292 Seed: 12071\n",
      "Testing: 0.6630546102143768 Training: 0.6408747194997881 Seed: 12073\n",
      "Testing: 0.6623896760968873 Training: 0.6411086159937274 Seed: 12075\n",
      "Testing: 0.6517099565185442 Training: 0.6437989610705545 Seed: 12077\n",
      "Testing: 0.6489800323170976 Training: 0.6444679819910388 Seed: 12078\n",
      "Testing: 0.6524175487715278 Training: 0.6435930759861019 Seed: 12079\n",
      "Testing: 0.645580999088922 Training: 0.6453180200235886 Seed: 12084\n",
      "Testing: 0.6461693850723518 Training: 0.6451700427479753 Seed: 12088\n",
      "Testing: 0.6454053317676841 Training: 0.6453223162013806 Seed: 12089\n",
      "Testing: 0.6471422119715305 Training: 0.6449068595248024 Seed: 12091\n",
      "Testing: 0.648081828342091 Training: 0.6446085106403128 Seed: 12094\n",
      "Testing: 0.6608970019048195 Training: 0.6412139029347644 Seed: 12096\n",
      "Testing: 0.6460772168635245 Training: 0.6451457725948062 Seed: 12097\n",
      "Testing: 0.6569267755821433 Training: 0.6425173243123684 Seed: 12098\n",
      "Testing: 0.6495485451221775 Training: 0.6443401500997028 Seed: 12099\n",
      "Testing: 0.6517777720571051 Training: 0.6437040876254272 Seed: 12100\n",
      "Testing: 0.6478221310845087 Training: 0.6447620284153748 Seed: 12105\n",
      "Testing: 0.6499749062876707 Training: 0.6440821464581886 Seed: 12107\n",
      "Testing: 0.6521940423202719 Training: 0.6434313209158353 Seed: 12108\n",
      "Testing: 0.6464068024509333 Training: 0.6450718379312914 Seed: 12110\n",
      "Testing: 0.6541098672882939 Training: 0.6431992370759192 Seed: 12111\n",
      "Testing: 0.6481965352663565 Training: 0.6446701094583458 Seed: 12114\n",
      "Testing: 0.6458708877999861 Training: 0.6452999208101466 Seed: 12115\n",
      "Testing: 0.6472122548352075 Training: 0.6449673077060498 Seed: 12116\n",
      "Testing: 0.6489500635562712 Training: 0.644484718586664 Seed: 12117\n",
      "Testing: 0.6563936343673284 Training: 0.6426684713114176 Seed: 12121\n",
      "Testing: 0.6504010541160987 Training: 0.6440988121923286 Seed: 12122\n",
      "Testing: 0.6460079088202934 Training: 0.6452207168875551 Seed: 12123\n",
      "Testing: 0.6506115994921243 Training: 0.6440700702429543 Seed: 12124\n",
      "Testing: 0.6504172782353971 Training: 0.6440854014694162 Seed: 12127\n",
      "Testing: 0.6685651824971391 Training: 0.6388328389168019 Seed: 12129\n",
      "Testing: 0.6471114529824005 Training: 0.6449843495163898 Seed: 12130\n",
      "Testing: 0.6458018106629766 Training: 0.6452471738905686 Seed: 12131\n",
      "Testing: 0.6487124984677912 Training: 0.64457515080494 Seed: 12133\n",
      "Testing: 0.6611073196615986 Training: 0.6413386005522377 Seed: 12134\n",
      "Testing: 0.6515068464406643 Training: 0.6438636384238553 Seed: 12135\n",
      "Testing: 0.6623115811549093 Training: 0.6409797068078786 Seed: 12139\n",
      "Testing: 0.6516453298844159 Training: 0.6438200470618081 Seed: 12140\n",
      "Testing: 0.6556637453977157 Training: 0.6428064181334868 Seed: 12141\n",
      "Testing: 0.6533788848181011 Training: 0.6434882341043868 Seed: 12142\n",
      "Testing: 0.6462996318556675 Training: 0.6451406686733651 Seed: 12145\n",
      "Testing: 0.6465467046699597 Training: 0.6451110624601178 Seed: 12146\n",
      "Testing: 0.6466394322548544 Training: 0.6449918046020938 Seed: 12147\n",
      "Testing: 0.6638453088003183 Training: 0.6410593151911556 Seed: 12148\n",
      "Testing: 0.6514946051905204 Training: 0.6438459551777858 Seed: 12149\n",
      "Testing: 0.6541137605180986 Training: 0.6432802700831353 Seed: 12155\n",
      "Testing: 0.6607779282044158 Training: 0.6414156123513561 Seed: 12158\n",
      "Testing: 0.6544400164524453 Training: 0.6431606969201662 Seed: 12165\n",
      "Testing: 0.6659245109850993 Training: 0.6403001314184402 Seed: 12166\n",
      "Testing: 0.6678210090476886 Training: 0.6395946122988104 Seed: 12168\n",
      "Testing: 0.6608993343600561 Training: 0.6414242992328422 Seed: 12170\n",
      "Testing: 0.6512923322823341 Training: 0.6439892264574483 Seed: 12172\n",
      "Testing: 0.6572735067804565 Training: 0.642449376309253 Seed: 12173\n",
      "Testing: 0.6503523773331852 Training: 0.6440486569864596 Seed: 12177\n",
      "Testing: 0.6520892019791732 Training: 0.6436214609763071 Seed: 12178\n",
      "Testing: 0.6559947222428125 Training: 0.6427951540336686 Seed: 12179\n",
      "Testing: 0.6537009242111491 Training: 0.6432876332787342 Seed: 12180\n",
      "Testing: 0.6520870793452282 Training: 0.6438301826123685 Seed: 12181\n",
      "Testing: 0.6492984539605944 Training: 0.6444184214848729 Seed: 12182\n",
      "Testing: 0.6525768669805441 Training: 0.643607988120966 Seed: 12183\n",
      "Testing: 0.6492343632553694 Training: 0.6443189057032788 Seed: 12184\n",
      "Testing: 0.6525899855919643 Training: 0.6435885959200323 Seed: 12190\n",
      "Testing: 0.6462760498969521 Training: 0.6451392068614978 Seed: 12193\n",
      "Testing: 0.6556371051138845 Training: 0.6428250329304677 Seed: 12194\n",
      "Testing: 0.6521660421448593 Training: 0.6435354573502776 Seed: 12195\n",
      "Testing: 0.6472407185620193 Training: 0.6448383246850666 Seed: 12197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6567028331323909 Training: 0.6425507683108496 Seed: 12199\n",
      "Testing: 0.6497593876396419 Training: 0.6443002542332716 Seed: 12200\n",
      "Testing: 0.654782726706962 Training: 0.6428361206444398 Seed: 12201\n",
      "Testing: 0.6637454227637134 Training: 0.6404651440858824 Seed: 12203\n",
      "Testing: 0.6521587869181384 Training: 0.6437979926177708 Seed: 12204\n",
      "Testing: 0.6523401590619528 Training: 0.6435970536991165 Seed: 12205\n",
      "Testing: 0.6613113981927745 Training: 0.6412637582358369 Seed: 12209\n",
      "Testing: 0.6512424609377201 Training: 0.6439738837751194 Seed: 12210\n",
      "Testing: 0.6520727439757456 Training: 0.6436711566351614 Seed: 12213\n",
      "Testing: 0.6574354627650084 Training: 0.6424128485271038 Seed: 12214\n",
      "Testing: 0.6675073800997722 Training: 0.6396716780712699 Seed: 12216\n",
      "Testing: 0.6462217625638785 Training: 0.6449859142837899 Seed: 12217\n",
      "Testing: 0.6636294967564783 Training: 0.6409241420260294 Seed: 12218\n",
      "Testing: 0.6488275343842985 Training: 0.6444824821633612 Seed: 12221\n",
      "Testing: 0.6502577245972994 Training: 0.6440980249506735 Seed: 12222\n",
      "Testing: 0.6476624399637603 Training: 0.6447694626098914 Seed: 12223\n",
      "Testing: 0.6475935384514971 Training: 0.6447960781352712 Seed: 12225\n",
      "Testing: 0.6470700233111446 Training: 0.6450175121719413 Seed: 12226\n",
      "Testing: 0.6575038818079645 Training: 0.6421198526685632 Seed: 12227\n",
      "Testing: 0.6530942963913424 Training: 0.6432457130258207 Seed: 12233\n",
      "Testing: 0.647369569254286 Training: 0.6449105357193772 Seed: 12234\n",
      "Testing: 0.6630601510352049 Training: 0.64070543959373 Seed: 12235\n",
      "Testing: 0.6510147545063978 Training: 0.6439772216311247 Seed: 12238\n",
      "Testing: 0.647784394679922 Training: 0.6448398238598939 Seed: 12240\n",
      "Testing: 0.6454837631843393 Training: 0.6453240053767766 Seed: 12242\n",
      "Testing: 0.6583452419542279 Training: 0.6419975737875676 Seed: 12244\n",
      "Testing: 0.6510165008198102 Training: 0.6439320626990794 Seed: 12246\n",
      "Testing: 0.6481239360103944 Training: 0.6444530798509456 Seed: 12247\n",
      "Testing: 0.6516703246123726 Training: 0.6438598493110993 Seed: 12248\n",
      "Testing: 0.6488887953743334 Training: 0.6444548152371281 Seed: 12249\n",
      "Testing: 0.6502397683480551 Training: 0.6441983486877256 Seed: 12252\n",
      "Testing: 0.6515891085247016 Training: 0.6438753209224944 Seed: 12253\n",
      "Testing: 0.64787388416943 Training: 0.6447730028034117 Seed: 12256\n",
      "Testing: 0.6487843236949857 Training: 0.6445755171712007 Seed: 12257\n",
      "Testing: 0.6636297162173426 Training: 0.6406543908427627 Seed: 12258\n",
      "Testing: 0.6571948203705641 Training: 0.6424372918262652 Seed: 12259\n",
      "Testing: 0.6591462291803085 Training: 0.641867703163957 Seed: 12260\n",
      "Testing: 0.6547907027422084 Training: 0.6429737142443606 Seed: 12261\n",
      "Testing: 0.6458869903270057 Training: 0.6453172028209408 Seed: 12262\n",
      "Testing: 0.6482258570800875 Training: 0.6444935055842205 Seed: 12263\n",
      "Testing: 0.6635706459124747 Training: 0.6408858816774999 Seed: 12264\n",
      "Testing: 0.6529252617117669 Training: 0.6435147619611988 Seed: 12268\n",
      "Testing: 0.6563423847064054 Training: 0.6426964255031655 Seed: 12269\n",
      "Testing: 0.6560304405448684 Training: 0.6425581556750154 Seed: 12270\n",
      "Testing: 0.6495071654854545 Training: 0.644332056368143 Seed: 12271\n",
      "Testing: 0.6449965116045616 Training: 0.6447205132671016 Seed: 12273\n",
      "Testing: 0.6541672672249043 Training: 0.6430993787751672 Seed: 12274\n",
      "Testing: 0.6470272745606483 Training: 0.6448736659826698 Seed: 12276\n",
      "Testing: 0.6564350051663835 Training: 0.6426196834868081 Seed: 12277\n",
      "Testing: 0.6540394970346121 Training: 0.6432683958250133 Seed: 12278\n",
      "Testing: 0.6544257383111491 Training: 0.6430147376524776 Seed: 12282\n",
      "Testing: 0.6483530953855378 Training: 0.6446803765162735 Seed: 12284\n",
      "Testing: 0.6477710515200392 Training: 0.644752685533412 Seed: 12286\n",
      "Testing: 0.6486807628663895 Training: 0.6444760330017542 Seed: 12287\n",
      "Testing: 0.6466826848033852 Training: 0.6450702436788353 Seed: 12288\n",
      "Testing: 0.6513958660829482 Training: 0.6437429714808467 Seed: 12289\n",
      "Testing: 0.6586885967675968 Training: 0.642039374427882 Seed: 12290\n",
      "Testing: 0.6497891868581062 Training: 0.6442817038246452 Seed: 12292\n",
      "Testing: 0.6453934312556409 Training: 0.645194916583493 Seed: 12293\n",
      "Testing: 0.6533755034765651 Training: 0.6432584837071317 Seed: 12295\n",
      "Testing: 0.6613402278988452 Training: 0.6413618251514845 Seed: 12296\n",
      "Testing: 0.6481358076008866 Training: 0.6446198860421223 Seed: 12298\n",
      "Testing: 0.6571704610122533 Training: 0.6423792778362525 Seed: 12300\n",
      "Testing: 0.6481158464776982 Training: 0.6446287610194213 Seed: 12302\n",
      "Testing: 0.645414573514663 Training: 0.6452927837795388 Seed: 12303\n",
      "Testing: 0.6529571980386322 Training: 0.6435721035447407 Seed: 12304\n",
      "Testing: 0.6569949972789638 Training: 0.642321053035375 Seed: 12310\n",
      "Testing: 0.6533172462653902 Training: 0.6434437247531668 Seed: 12313\n",
      "Testing: 0.6568101528381308 Training: 0.64250929637561 Seed: 12316\n",
      "Testing: 0.6564486461856556 Training: 0.6426070443674368 Seed: 12317\n",
      "Testing: 0.6497987166222718 Training: 0.6443194888629732 Seed: 12321\n",
      "Testing: 0.6457779560321836 Training: 0.6453196464606133 Seed: 12324\n",
      "Testing: 0.658134037323764 Training: 0.6422330891852389 Seed: 12325\n",
      "Testing: 0.6510619507438244 Training: 0.6439185175629418 Seed: 12326\n",
      "Testing: 0.6553015601081056 Training: 0.6428821709447403 Seed: 12328\n",
      "Testing: 0.6535120479566493 Training: 0.6432845881122706 Seed: 12333\n",
      "Testing: 0.6537404089877973 Training: 0.6432260161033615 Seed: 12341\n",
      "Testing: 0.6607286126852728 Training: 0.6412596449513022 Seed: 12343\n",
      "Testing: 0.6518440190910714 Training: 0.6437334012759972 Seed: 12344\n",
      "Testing: 0.6617826803430471 Training: 0.6411359765584703 Seed: 12346\n",
      "Testing: 0.6517453902754284 Training: 0.6437966898659837 Seed: 12348\n",
      "Testing: 0.6602602447918946 Training: 0.6417212606839784 Seed: 12349\n",
      "Testing: 0.6514798961248368 Training: 0.6439379701286956 Seed: 12352\n",
      "Testing: 0.656049730037058 Training: 0.6426087760942443 Seed: 12356\n",
      "Testing: 0.6504389689281806 Training: 0.6441317489404592 Seed: 12360\n",
      "Testing: 0.6541267039020855 Training: 0.6432503749560509 Seed: 12361\n",
      "Testing: 0.6566887447002796 Training: 0.6425513890037352 Seed: 12375\n",
      "Testing: 0.6580362058965409 Training: 0.6422612414037311 Seed: 12376\n",
      "Testing: 0.6518582061463729 Training: 0.6438775450881908 Seed: 12377\n",
      "Testing: 0.6561614747637017 Training: 0.6426919095108445 Seed: 12379\n",
      "Testing: 0.6475086956040935 Training: 0.6447904580485395 Seed: 12380\n",
      "Testing: 0.6537945679541379 Training: 0.6432475712637657 Seed: 12381\n",
      "Testing: 0.6559530516138391 Training: 0.6427466830521824 Seed: 12385\n",
      "Testing: 0.6523991600712971 Training: 0.6435082386912382 Seed: 12386\n",
      "Testing: 0.6456941494810656 Training: 0.6453765596075771 Seed: 12388\n",
      "Testing: 0.6524067504852405 Training: 0.6436668076854699 Seed: 12391\n",
      "Testing: 0.6508715507597519 Training: 0.6440764451637083 Seed: 12394\n",
      "Testing: 0.6594380372664804 Training: 0.6419313204432473 Seed: 12400\n",
      "Testing: 0.6516248604953238 Training: 0.6437566468937627 Seed: 12401\n",
      "Testing: 0.6572897747030757 Training: 0.6423567898693366 Seed: 12402\n",
      "Testing: 0.6524790075629493 Training: 0.6435671360186027 Seed: 12404\n",
      "Testing: 0.6684892718530152 Training: 0.6397071659833269 Seed: 12406\n",
      "Testing: 0.6519814748911774 Training: 0.6436210313597928 Seed: 12408\n",
      "Testing: 0.6586081123880283 Training: 0.6421829953697531 Seed: 12411\n",
      "Testing: 0.6530009629891969 Training: 0.6433600768353224 Seed: 12413\n",
      "Testing: 0.6490865196046804 Training: 0.6443548978285022 Seed: 12414\n",
      "Testing: 0.6501561473471662 Training: 0.6441473109173603 Seed: 12419\n",
      "Testing: 0.6656152223257953 Training: 0.6400748591962354 Seed: 12420\n",
      "Testing: 0.651769726178556 Training: 0.6438099197541406 Seed: 12422\n",
      "Testing: 0.6503406350825456 Training: 0.6440804391945734 Seed: 12423\n",
      "Testing: 0.6546012030957031 Training: 0.6431113433997356 Seed: 12426\n",
      "Testing: 0.6548834376104506 Training: 0.6429705571605309 Seed: 12427\n",
      "Testing: 0.6475833221477697 Training: 0.6446863575983253 Seed: 12428\n",
      "Testing: 0.646699517874892 Training: 0.6449118478532986 Seed: 12429\n",
      "Testing: 0.651059490531951 Training: 0.6438879687698622 Seed: 12431\n",
      "Testing: 0.6541512670077594 Training: 0.6432055573905446 Seed: 12432\n",
      "Testing: 0.6461275305781282 Training: 0.6450645406698964 Seed: 12433\n",
      "Testing: 0.6610713237686416 Training: 0.6413950580131662 Seed: 12434\n",
      "Testing: 0.6482959496259377 Training: 0.6444625805007349 Seed: 12437\n",
      "Testing: 0.6560015553636563 Training: 0.6427608201602732 Seed: 12438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6482333908428234 Training: 0.6446664328316578 Seed: 12441\n",
      "Testing: 0.6523738328863153 Training: 0.6434942280550356 Seed: 12443\n",
      "Testing: 0.652100921806515 Training: 0.6435757281464293 Seed: 12445\n",
      "Testing: 0.6486090201384054 Training: 0.6445385866897924 Seed: 12446\n",
      "Testing: 0.6624251552194881 Training: 0.6407223942245077 Seed: 12447\n",
      "Testing: 0.6660596194176279 Training: 0.6401984245279155 Seed: 12451\n",
      "Testing: 0.6510011404225687 Training: 0.6439871703802987 Seed: 12452\n",
      "Testing: 0.6653405115754455 Training: 0.64034117086129 Seed: 12453\n",
      "Testing: 0.6665123915211274 Training: 0.6399907837837668 Seed: 12454\n",
      "Testing: 0.6480409869909162 Training: 0.6447225756346265 Seed: 12455\n",
      "Testing: 0.6482114961585677 Training: 0.6447382339010473 Seed: 12457\n",
      "Testing: 0.6500892297437427 Training: 0.6441526535191102 Seed: 12458\n",
      "Testing: 0.6512241590184304 Training: 0.6439808183215859 Seed: 12459\n",
      "Testing: 0.6539407365058809 Training: 0.6432710396230557 Seed: 12460\n",
      "Testing: 0.6512290978415182 Training: 0.6437380882352363 Seed: 12463\n",
      "Testing: 0.6506160014482513 Training: 0.6438409555429596 Seed: 12465\n",
      "Testing: 0.6590038527209698 Training: 0.6418038136562368 Seed: 12470\n",
      "Testing: 0.6466974485632836 Training: 0.6449939503259339 Seed: 12471\n",
      "Testing: 0.6508453833400687 Training: 0.6438138541489467 Seed: 12473\n",
      "Testing: 0.6547732200597016 Training: 0.6429356395592818 Seed: 12475\n",
      "Testing: 0.6484469692662532 Training: 0.6446171329589145 Seed: 12477\n",
      "Testing: 0.6592777698119284 Training: 0.6418034434550195 Seed: 12480\n",
      "Testing: 0.6487631565977469 Training: 0.6440430142297406 Seed: 12481\n",
      "Testing: 0.6479672424635983 Training: 0.6447610383679733 Seed: 12482\n",
      "Testing: 0.6602743840295469 Training: 0.641612039152501 Seed: 12484\n",
      "Testing: 0.6479335881345718 Training: 0.6447408897010397 Seed: 12485\n",
      "Testing: 0.6544955474279998 Training: 0.6431661104488233 Seed: 12488\n",
      "Testing: 0.6497388412666052 Training: 0.6442167215400181 Seed: 12493\n",
      "Testing: 0.6562657470421661 Training: 0.6426560427786293 Seed: 12495\n",
      "Testing: 0.6573004331981114 Training: 0.6422370847029053 Seed: 12496\n",
      "Testing: 0.658407072286762 Training: 0.6419054272105922 Seed: 12498\n",
      "Testing: 0.6586111599628904 Training: 0.6420452697202527 Seed: 12501\n",
      "Testing: 0.6558554281763462 Training: 0.6426798817321211 Seed: 12502\n",
      "Testing: 0.6548655773979118 Training: 0.6429850733160174 Seed: 12503\n",
      "Testing: 0.6541703151107731 Training: 0.6430085349581717 Seed: 12506\n",
      "Testing: 0.6458711757871392 Training: 0.6452674739838398 Seed: 12508\n",
      "Testing: 0.6516889759048763 Training: 0.6438179318930284 Seed: 12511\n",
      "Testing: 0.6476561530980928 Training: 0.644838027727237 Seed: 12516\n",
      "Testing: 0.6543921133305135 Training: 0.6430020975877231 Seed: 12517\n",
      "Testing: 0.6555153839567479 Training: 0.6427907573400158 Seed: 12521\n",
      "Testing: 0.6557160497196614 Training: 0.6428798759414059 Seed: 12522\n",
      "Testing: 0.6453384083270851 Training: 0.6452868391829657 Seed: 12525\n",
      "Testing: 0.6490427728558568 Training: 0.6443673998841799 Seed: 12527\n",
      "Testing: 0.6521525144905861 Training: 0.6436466682311168 Seed: 12528\n",
      "Testing: 0.6597893396066894 Training: 0.6417092781022078 Seed: 12529\n",
      "Testing: 0.6468920600492114 Training: 0.6447497612876579 Seed: 12531\n",
      "Testing: 0.650158712355578 Training: 0.6439947090164566 Seed: 12532\n",
      "Testing: 0.6552239555958221 Training: 0.6427850283562042 Seed: 12533\n",
      "Testing: 0.6502246691974748 Training: 0.6437485430065979 Seed: 12536\n",
      "Testing: 0.651475837824963 Training: 0.6437249720308048 Seed: 12540\n",
      "Testing: 0.6486719001719162 Training: 0.6444946156936322 Seed: 12541\n",
      "Testing: 0.6564510479753103 Training: 0.6427437933553646 Seed: 12542\n",
      "Testing: 0.6503445885358977 Training: 0.6440612124004572 Seed: 12546\n",
      "Testing: 0.6454340686055342 Training: 0.6453645769120397 Seed: 12548\n",
      "Testing: 0.6503203275441363 Training: 0.6440205459752308 Seed: 12549\n",
      "Testing: 0.6560646024088982 Training: 0.6425573808897556 Seed: 12552\n",
      "Testing: 0.6470886415986088 Training: 0.6449081834729241 Seed: 12556\n",
      "Testing: 0.6535990318793282 Training: 0.6432634731817743 Seed: 12557\n",
      "Testing: 0.650209033965151 Training: 0.6441288743733826 Seed: 12558\n",
      "Testing: 0.647792683665023 Training: 0.6447526716441823 Seed: 12560\n",
      "Testing: 0.6595327761305112 Training: 0.6418715648249963 Seed: 12561\n",
      "Testing: 0.6549554677548737 Training: 0.6429329423464963 Seed: 12563\n",
      "Testing: 0.6498722920505778 Training: 0.6442811268256556 Seed: 12564\n",
      "Testing: 0.6492989157930138 Training: 0.6443912950621595 Seed: 12566\n",
      "Testing: 0.6488048945332069 Training: 0.6445487926991491 Seed: 12572\n",
      "Testing: 0.654674566904808 Training: 0.6425766120265177 Seed: 12573\n",
      "Testing: 0.6461200259524237 Training: 0.645012600309207 Seed: 12574\n",
      "Testing: 0.6470921286157625 Training: 0.644956955747942 Seed: 12575\n",
      "Testing: 0.6455433440051558 Training: 0.6453492918446062 Seed: 12576\n",
      "Testing: 0.6489233974007469 Training: 0.644497553384554 Seed: 12579\n",
      "Testing: 0.6653629546929888 Training: 0.6402832506819707 Seed: 12583\n",
      "Testing: 0.6574629372878029 Training: 0.6419016823528361 Seed: 12584\n",
      "Testing: 0.652778209393595 Training: 0.643427287019864 Seed: 12593\n",
      "Testing: 0.6479034534013091 Training: 0.644768169323216 Seed: 12596\n",
      "Testing: 0.6475669388905976 Training: 0.644412999586866 Seed: 12597\n",
      "Testing: 0.6487209780892851 Training: 0.6445698086017789 Seed: 12602\n",
      "Testing: 0.6484426106983703 Training: 0.6444781319516681 Seed: 12605\n",
      "Testing: 0.6529796475246036 Training: 0.643427233634613 Seed: 12606\n",
      "Testing: 0.6461498828817405 Training: 0.6452215137391588 Seed: 12607\n",
      "Testing: 0.6523711494109768 Training: 0.6436998952641 Seed: 12608\n",
      "Testing: 0.65172311104416 Training: 0.6438347420646892 Seed: 12609\n",
      "Testing: 0.6502976697662697 Training: 0.6441736259536759 Seed: 12610\n",
      "Testing: 0.645925681249839 Training: 0.645167833885264 Seed: 12613\n",
      "Testing: 0.6534561122274057 Training: 0.6433901371637714 Seed: 12620\n",
      "Testing: 0.6494354795047574 Training: 0.6443616595088534 Seed: 12628\n",
      "Testing: 0.6524296197267921 Training: 0.6435270047777981 Seed: 12629\n",
      "Testing: 0.6544952285071208 Training: 0.6431612870087351 Seed: 12631\n",
      "Testing: 0.6527403285952056 Training: 0.6434623407295116 Seed: 12632\n",
      "Testing: 0.6561702913517367 Training: 0.6424887379203557 Seed: 12636\n",
      "Testing: 0.654985401599213 Training: 0.6429372635707868 Seed: 12637\n",
      "Testing: 0.6499477676759637 Training: 0.6440454899603334 Seed: 12639\n",
      "Testing: 0.6549587510808447 Training: 0.6430179897388195 Seed: 12641\n",
      "Testing: 0.6461562965578558 Training: 0.6451054903491807 Seed: 12642\n",
      "Testing: 0.6492147266801156 Training: 0.6444451930327303 Seed: 12644\n",
      "Testing: 0.6528680849508828 Training: 0.6433860100973257 Seed: 12645\n",
      "Testing: 0.6553229148794582 Training: 0.6428683410851912 Seed: 12651\n",
      "Testing: 0.6466963053206514 Training: 0.6448387801617363 Seed: 12653\n",
      "Testing: 0.6610337006445794 Training: 0.6413833920054159 Seed: 12657\n",
      "Testing: 0.6505126716137247 Training: 0.6440771225180587 Seed: 12660\n",
      "Testing: 0.6579559965576725 Training: 0.6423070742736631 Seed: 12661\n",
      "Testing: 0.6561618654654823 Training: 0.6425948468103673 Seed: 12662\n",
      "Testing: 0.6577592075903743 Training: 0.6417357153085137 Seed: 12667\n",
      "Testing: 0.6511988151077411 Training: 0.6438209246871731 Seed: 12668\n",
      "Testing: 0.6596726744389377 Training: 0.641886089658804 Seed: 12670\n",
      "Testing: 0.669150215638914 Training: 0.6396928390083477 Seed: 12671\n",
      "Testing: 0.6479687934593942 Training: 0.6447768443074691 Seed: 12672\n",
      "Testing: 0.6488822086763102 Training: 0.6445088622812225 Seed: 12673\n",
      "Testing: 0.647005204185891 Training: 0.644957776303081 Seed: 12674\n",
      "Testing: 0.6487727342494017 Training: 0.6444635979988731 Seed: 12675\n",
      "Testing: 0.645906203658611 Training: 0.6450529457785007 Seed: 12676\n",
      "Testing: 0.655409882020627 Training: 0.6425317414225588 Seed: 12679\n",
      "Testing: 0.6505390463137283 Training: 0.643999825114824 Seed: 12681\n",
      "Testing: 0.6574174430945556 Training: 0.642084930858495 Seed: 12682\n",
      "Testing: 0.6538175924166512 Training: 0.6432610119339196 Seed: 12688\n",
      "Testing: 0.6455846516390014 Training: 0.6452707153235668 Seed: 12690\n",
      "Testing: 0.6532965057068452 Training: 0.643070621258246 Seed: 12695\n",
      "Testing: 0.6502457862460408 Training: 0.6440590480313153 Seed: 12696\n",
      "Testing: 0.6560707070097184 Training: 0.6425022546083115 Seed: 12701\n",
      "Testing: 0.6498663122393705 Training: 0.6442924893124136 Seed: 12702\n",
      "Testing: 0.6650474769148103 Training: 0.6403042073051086 Seed: 12704\n",
      "Testing: 0.6460176953064433 Training: 0.6452179653843677 Seed: 12705\n",
      "Testing: 0.6481419746276407 Training: 0.6447027352979969 Seed: 12707\n",
      "Testing: 0.6486918240355931 Training: 0.6444771302007658 Seed: 12709\n",
      "Testing: 0.6489987674754991 Training: 0.6444119737250837 Seed: 12710\n",
      "Testing: 0.6606189719396527 Training: 0.6413909244862238 Seed: 12711\n",
      "Testing: 0.6526129064822541 Training: 0.6435588975616587 Seed: 12712\n",
      "Testing: 0.6474611227841429 Training: 0.6447311351250966 Seed: 12713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6560337899299992 Training: 0.6427020233835141 Seed: 12714\n",
      "Testing: 0.6730098411203889 Training: 0.6385979406525419 Seed: 12715\n",
      "Testing: 0.6609915735459974 Training: 0.6412278930995576 Seed: 12716\n",
      "Testing: 0.6595524463744151 Training: 0.6417252555140417 Seed: 12718\n",
      "Testing: 0.6692831609078679 Training: 0.6396797393434217 Seed: 12721\n",
      "Testing: 0.6484753398357926 Training: 0.6446813432261445 Seed: 12728\n",
      "Testing: 0.6546155833889896 Training: 0.6430363270340735 Seed: 12729\n",
      "Testing: 0.6586867545581964 Training: 0.641919181706004 Seed: 12730\n",
      "Testing: 0.6510693896209405 Training: 0.6437489773732573 Seed: 12731\n",
      "Testing: 0.6462123394426874 Training: 0.6451996236091861 Seed: 12736\n",
      "Testing: 0.6640289589667445 Training: 0.6403706277502175 Seed: 12737\n",
      "Testing: 0.6471725116664118 Training: 0.6449728738024342 Seed: 12738\n",
      "Testing: 0.661203023485023 Training: 0.6414533596014621 Seed: 12740\n",
      "Testing: 0.660032071115011 Training: 0.6417105294297288 Seed: 12742\n",
      "Testing: 0.6481684450029568 Training: 0.644598208107775 Seed: 12745\n",
      "Testing: 0.6508840549838241 Training: 0.6440436084986426 Seed: 12746\n",
      "Testing: 0.6742246547237608 Training: 0.638230306475531 Seed: 12747\n",
      "Testing: 0.6477121472438864 Training: 0.6447190128847897 Seed: 12750\n",
      "Testing: 0.6483198233151757 Training: 0.6447001462816305 Seed: 12756\n",
      "Testing: 0.6554269297601405 Training: 0.6429220092428436 Seed: 12758\n",
      "Testing: 0.6617773196713579 Training: 0.6412008880841003 Seed: 12761\n",
      "Testing: 0.6554483117122479 Training: 0.6426640273631418 Seed: 12765\n",
      "Testing: 0.6526393597741195 Training: 0.643276114169717 Seed: 12766\n",
      "Testing: 0.6512744289727157 Training: 0.6438195746477212 Seed: 12767\n",
      "Testing: 0.6589048304448324 Training: 0.6420345108318335 Seed: 12768\n",
      "Testing: 0.6486438646861836 Training: 0.6445991304923335 Seed: 12770\n",
      "Testing: 0.6478165393623241 Training: 0.6447818880432458 Seed: 12777\n",
      "Testing: 0.6582322452649695 Training: 0.6421526424368893 Seed: 12779\n",
      "Testing: 0.6508127145426488 Training: 0.6440252434012782 Seed: 12782\n",
      "Testing: 0.6453988236017401 Training: 0.6452792751221949 Seed: 12784\n",
      "Testing: 0.648214179954735 Training: 0.6446128627054404 Seed: 12785\n",
      "Testing: 0.6578363866441482 Training: 0.642255497313011 Seed: 12786\n",
      "Testing: 0.6501618007579677 Training: 0.6441024627595158 Seed: 12789\n",
      "Testing: 0.6474875291581434 Training: 0.6448110976670572 Seed: 12792\n",
      "Testing: 0.6535028139196704 Training: 0.6432887783483208 Seed: 12796\n",
      "Testing: 0.654441654094831 Training: 0.6431665039416035 Seed: 12797\n",
      "Testing: 0.6487866931544373 Training: 0.6445764684475084 Seed: 12800\n",
      "Testing: 0.6585861275740752 Training: 0.6418804890414131 Seed: 12801\n",
      "Testing: 0.6497566677547557 Training: 0.6443477697752416 Seed: 12802\n",
      "Testing: 0.6465403754015798 Training: 0.6450618745404884 Seed: 12803\n",
      "Testing: 0.663023795152329 Training: 0.6410212727510205 Seed: 12806\n",
      "Testing: 0.6554914634440155 Training: 0.6425545143390607 Seed: 12807\n",
      "Testing: 0.6467290022429125 Training: 0.6451033579340829 Seed: 12810\n",
      "Testing: 0.6467917454900037 Training: 0.6449715766710175 Seed: 12811\n",
      "Testing: 0.6546340017549453 Training: 0.6430653971960122 Seed: 12812\n",
      "Testing: 0.6474201776036091 Training: 0.6449162074715251 Seed: 12813\n",
      "Testing: 0.6473851695501621 Training: 0.6443772002860715 Seed: 12816\n",
      "Testing: 0.6520088800442756 Training: 0.6436921416243109 Seed: 12817\n",
      "Testing: 0.669166110092341 Training: 0.6390628318171534 Seed: 12818\n",
      "Testing: 0.6609759836077381 Training: 0.6413127986453626 Seed: 12820\n",
      "Testing: 0.6530240957330848 Training: 0.6434901174187656 Seed: 12821\n",
      "Testing: 0.6547225474501434 Training: 0.6429417162093697 Seed: 12822\n",
      "Testing: 0.64873734043263 Training: 0.6445497411467367 Seed: 12824\n",
      "Testing: 0.653391778180033 Training: 0.6432778008161306 Seed: 12826\n",
      "Testing: 0.6494088518713351 Training: 0.6442989398677506 Seed: 12829\n",
      "Testing: 0.657483126552867 Training: 0.6421517542807019 Seed: 12830\n",
      "Testing: 0.6527118929583948 Training: 0.6435916951060879 Seed: 12831\n",
      "Testing: 0.6490908869322273 Training: 0.6444413083979874 Seed: 12833\n",
      "Testing: 0.6593486599918524 Training: 0.6419188926627979 Seed: 12834\n",
      "Testing: 0.6501878382289008 Training: 0.6440987585464535 Seed: 12835\n",
      "Testing: 0.647984167425619 Training: 0.6447153952948306 Seed: 12836\n",
      "Testing: 0.6534380860999927 Training: 0.643309370778763 Seed: 12838\n",
      "Testing: 0.6549379292679398 Training: 0.6429252297019132 Seed: 12840\n",
      "Testing: 0.6509983600361373 Training: 0.6439599508720185 Seed: 12842\n",
      "Testing: 0.6479936039997553 Training: 0.6446526487456461 Seed: 12850\n",
      "Testing: 0.6605850838307699 Training: 0.6416406421857509 Seed: 12851\n",
      "Testing: 0.6643915998136435 Training: 0.6406402294439533 Seed: 12852\n",
      "Testing: 0.6474336037417336 Training: 0.6448277803884574 Seed: 12855\n",
      "Testing: 0.6528085921437162 Training: 0.6429709323489472 Seed: 12858\n",
      "Testing: 0.6576003976599969 Training: 0.6420092113815536 Seed: 12861\n",
      "Testing: 0.6577849052435957 Training: 0.6422833723308201 Seed: 12865\n",
      "Testing: 0.6530470372062467 Training: 0.6433485156281085 Seed: 12866\n",
      "Testing: 0.6656272357865122 Training: 0.6405446619072878 Seed: 12869\n",
      "Testing: 0.6501176711154327 Training: 0.6440727796708343 Seed: 12870\n",
      "Testing: 0.6547125990952221 Training: 0.6430786870001429 Seed: 12872\n",
      "Testing: 0.6508063230462457 Training: 0.6437881927941758 Seed: 12873\n",
      "Testing: 0.6563311139090838 Training: 0.6426262868838543 Seed: 12876\n",
      "Testing: 0.6481538308833684 Training: 0.6446235027880909 Seed: 12879\n",
      "Testing: 0.6615114492796665 Training: 0.6413120548922309 Seed: 12880\n",
      "Testing: 0.6479335578203747 Training: 0.6445962951037072 Seed: 12883\n",
      "Testing: 0.6715341144175501 Training: 0.6387653952304957 Seed: 12886\n",
      "Testing: 0.656126803300314 Training: 0.6426952562476765 Seed: 12887\n",
      "Testing: 0.667036296813269 Training: 0.6397574165647639 Seed: 12888\n",
      "Testing: 0.6470039054951187 Training: 0.644981378381202 Seed: 12891\n",
      "Testing: 0.6554508845728428 Training: 0.6425708626032882 Seed: 12892\n",
      "Testing: 0.6571975590691423 Training: 0.6423561811291812 Seed: 12894\n",
      "Testing: 0.6494921110651342 Training: 0.6444373690119058 Seed: 12895\n",
      "Testing: 0.6604766249867682 Training: 0.6416286675358839 Seed: 12896\n",
      "Testing: 0.6659849827703741 Training: 0.6403440610044203 Seed: 12900\n",
      "Testing: 0.6638894626255307 Training: 0.6407023952191407 Seed: 12911\n",
      "Testing: 0.6468027944299758 Training: 0.644973683193922 Seed: 12912\n",
      "Testing: 0.6457193722344274 Training: 0.6452769223253927 Seed: 12913\n",
      "Testing: 0.6633383003964233 Training: 0.6407302088948557 Seed: 12914\n",
      "Testing: 0.6501823808343146 Training: 0.6441206035856772 Seed: 12915\n",
      "Testing: 0.6527618303088596 Training: 0.6434778888871681 Seed: 12918\n",
      "Testing: 0.6531419587976118 Training: 0.6432886884897028 Seed: 12920\n",
      "Testing: 0.6478674626608116 Training: 0.6446935673617896 Seed: 12924\n",
      "Testing: 0.6611135444093869 Training: 0.6405865291134983 Seed: 12927\n",
      "Testing: 0.6535015034560484 Training: 0.6432989312677044 Seed: 12932\n",
      "Testing: 0.654469563257815 Training: 0.6430819960740058 Seed: 12934\n",
      "Testing: 0.6730429742293152 Training: 0.6385962181949115 Seed: 12936\n",
      "Testing: 0.6590065448295095 Training: 0.6418316434890523 Seed: 12938\n",
      "Testing: 0.6468416813061018 Training: 0.6449885110166664 Seed: 12944\n",
      "Testing: 0.6575803143352139 Training: 0.6422909330986142 Seed: 12946\n",
      "Testing: 0.6495621261295578 Training: 0.6443833006626021 Seed: 12948\n",
      "Testing: 0.6467304157732917 Training: 0.6450529740226064 Seed: 12950\n",
      "Testing: 0.6468635962046793 Training: 0.6448951958307574 Seed: 12952\n",
      "Testing: 0.6685443196161573 Training: 0.639512367043483 Seed: 12956\n",
      "Testing: 0.6578063628888632 Training: 0.6423796768223748 Seed: 12958\n",
      "Testing: 0.671116753952498 Training: 0.6388354651706478 Seed: 12959\n",
      "Testing: 0.6570794653723449 Training: 0.6425781405714627 Seed: 12961\n",
      "Testing: 0.6556153542785009 Training: 0.6426082799582513 Seed: 12964\n",
      "Testing: 0.6661861175202733 Training: 0.6398777678116883 Seed: 12965\n",
      "Testing: 0.6461022433798947 Training: 0.645137824867565 Seed: 12972\n",
      "Testing: 0.6479444883898853 Training: 0.6444604633050424 Seed: 12973\n",
      "Testing: 0.6576388194177862 Training: 0.6424549869148612 Seed: 12975\n",
      "Testing: 0.6464446971121209 Training: 0.645097584352455 Seed: 12977\n",
      "Testing: 0.6691175179781016 Training: 0.6394429075607015 Seed: 12980\n",
      "Testing: 0.653107107927756 Training: 0.6434712939005305 Seed: 12981\n",
      "Testing: 0.6559890300329692 Training: 0.6427490794218635 Seed: 12982\n",
      "Testing: 0.6544873427358249 Training: 0.6431501310732048 Seed: 12983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6562150577981748 Training: 0.6425029836128451 Seed: 12990\n",
      "Testing: 0.6472344630803627 Training: 0.6449711693211074 Seed: 12991\n",
      "Testing: 0.6560884269825391 Training: 0.6427456503477101 Seed: 12992\n",
      "Testing: 0.6458665317949426 Training: 0.6451248211272842 Seed: 12993\n",
      "Testing: 0.645854103146499 Training: 0.6450528316419166 Seed: 12996\n",
      "Testing: 0.6645266935517987 Training: 0.6404513255629116 Seed: 12997\n",
      "Testing: 0.6472485425947733 Training: 0.6447421556689739 Seed: 13000\n",
      "Testing: 0.6464846698461265 Training: 0.6450878625964687 Seed: 13002\n",
      "Testing: 0.659450822770632 Training: 0.6418129069787423 Seed: 13008\n",
      "Testing: 0.6513164630890168 Training: 0.6438841232911169 Seed: 13011\n",
      "Testing: 0.667646573938365 Training: 0.6397355111569487 Seed: 13013\n",
      "Testing: 0.6511963303235856 Training: 0.6439027679825577 Seed: 13014\n",
      "Testing: 0.6498724623595449 Training: 0.6441326181595775 Seed: 13015\n",
      "Testing: 0.6476546542914985 Training: 0.6448765404835447 Seed: 13018\n",
      "Testing: 0.6678195050040994 Training: 0.6399540998442652 Seed: 13020\n",
      "Testing: 0.6619882828075554 Training: 0.6410972714819478 Seed: 13026\n",
      "Testing: 0.6574514500543301 Training: 0.6422693982393091 Seed: 13027\n",
      "Testing: 0.645420854116252 Training: 0.6453778700080796 Seed: 13028\n",
      "Testing: 0.6585454121094367 Training: 0.6421507145791694 Seed: 13029\n",
      "Testing: 0.6623474550604925 Training: 0.6409275324487831 Seed: 13030\n",
      "Testing: 0.6498275519306692 Training: 0.6443451222549388 Seed: 13032\n",
      "Testing: 0.6565383654617305 Training: 0.6425866357181923 Seed: 13033\n",
      "Testing: 0.6508011073797682 Training: 0.643641975892627 Seed: 13034\n",
      "Testing: 0.6495489000951127 Training: 0.6442753326765359 Seed: 13036\n",
      "Testing: 0.6590505581232957 Training: 0.6419499995083464 Seed: 13037\n",
      "Testing: 0.647521066841749 Training: 0.644656600897555 Seed: 13038\n",
      "Testing: 0.6469697923855311 Training: 0.6446545931189922 Seed: 13042\n",
      "Testing: 0.6548069194651013 Training: 0.6429098724785316 Seed: 13044\n",
      "Testing: 0.6475382185034029 Training: 0.6448727664577412 Seed: 13049\n",
      "Testing: 0.6471838255650908 Training: 0.6449431377204171 Seed: 13050\n",
      "Testing: 0.6550527678011285 Training: 0.6427733403659489 Seed: 13052\n",
      "Testing: 0.6472843717873207 Training: 0.6449037096340818 Seed: 13053\n",
      "Testing: 0.6593205066613612 Training: 0.6419016422859969 Seed: 13054\n",
      "Testing: 0.6660317936566197 Training: 0.6400566505614425 Seed: 13058\n",
      "Testing: 0.6564643327203288 Training: 0.6423855270436614 Seed: 13061\n",
      "Testing: 0.659180633932679 Training: 0.6419110386537903 Seed: 13062\n",
      "Testing: 0.6489131671202077 Training: 0.6444915537090742 Seed: 13063\n",
      "Testing: 0.652349769785906 Training: 0.6437071921646348 Seed: 13065\n",
      "Testing: 0.6524671122634902 Training: 0.6436521782957454 Seed: 13067\n",
      "Testing: 0.6584230789463155 Training: 0.6417891215557985 Seed: 13068\n",
      "Testing: 0.6552863190864743 Training: 0.6428436131367226 Seed: 13070\n",
      "Testing: 0.6722089532068574 Training: 0.6387569230497658 Seed: 13071\n",
      "Testing: 0.6576856669435757 Training: 0.6422248921421383 Seed: 13076\n",
      "Testing: 0.6500805457571956 Training: 0.6441790234286812 Seed: 13077\n",
      "Testing: 0.6643592031281111 Training: 0.6408403729648655 Seed: 13078\n",
      "Testing: 0.6492523848057696 Training: 0.6442864582767935 Seed: 13080\n",
      "Testing: 0.6496001805535646 Training: 0.6443588898579077 Seed: 13085\n",
      "Testing: 0.6561087604820136 Training: 0.6425991195701675 Seed: 13087\n",
      "Testing: 0.6490445496618675 Training: 0.6443858392405097 Seed: 13088\n",
      "Testing: 0.6545022990478868 Training: 0.6430217452101955 Seed: 13093\n",
      "Testing: 0.6555761643298784 Training: 0.642825626452353 Seed: 13094\n",
      "Testing: 0.6485050622708108 Training: 0.6443380796378022 Seed: 13095\n",
      "Testing: 0.6520669412453536 Training: 0.6437270320523772 Seed: 13097\n",
      "Testing: 0.650975575580569 Training: 0.6439765893874495 Seed: 13099\n",
      "Testing: 0.6458671158262046 Training: 0.6451746798095073 Seed: 13100\n",
      "Testing: 0.6583881019808902 Training: 0.6420824200745192 Seed: 13103\n",
      "Testing: 0.6527169579989631 Training: 0.6433985115997661 Seed: 13104\n",
      "Testing: 0.6602136213937981 Training: 0.6417029279537076 Seed: 13106\n",
      "Testing: 0.6645528902688927 Training: 0.6406223356108548 Seed: 13107\n",
      "Testing: 0.6646501305320609 Training: 0.6404954993251996 Seed: 13108\n",
      "Testing: 0.6533901408422688 Training: 0.6433106660363641 Seed: 13109\n",
      "Testing: 0.6494079546904683 Training: 0.6444224126138239 Seed: 13111\n",
      "Testing: 0.6558520544586323 Training: 0.6427637489785751 Seed: 13112\n",
      "Testing: 0.6540237037329091 Training: 0.6430682935530219 Seed: 13115\n",
      "Testing: 0.6612558440355265 Training: 0.6412644560363268 Seed: 13117\n",
      "Testing: 0.6508391319139408 Training: 0.6440220287529971 Seed: 13118\n",
      "Testing: 0.6452899065135465 Training: 0.6452577484229951 Seed: 13120\n",
      "Testing: 0.647194954399455 Training: 0.644988095917729 Seed: 13124\n",
      "Testing: 0.6510499319740675 Training: 0.6440122810088948 Seed: 13128\n",
      "Testing: 0.656658997555392 Training: 0.6425200588903752 Seed: 13129\n",
      "Testing: 0.6481106534444699 Training: 0.6446988142324658 Seed: 13130\n",
      "Testing: 0.6473610799828402 Training: 0.6448205148356148 Seed: 13132\n",
      "Testing: 0.6601640685213164 Training: 0.6416455754386321 Seed: 13134\n",
      "Testing: 0.6470642352622162 Training: 0.644458005938817 Seed: 13137\n",
      "Testing: 0.6587090095310173 Training: 0.6421296709112336 Seed: 13147\n",
      "Testing: 0.6477764031731554 Training: 0.6448357007556018 Seed: 13148\n",
      "Testing: 0.6460993538940121 Training: 0.6451685117701369 Seed: 13151\n",
      "Testing: 0.6525630816155379 Training: 0.643576934244275 Seed: 13154\n",
      "Testing: 0.6478931172751219 Training: 0.644821114019791 Seed: 13155\n",
      "Testing: 0.6583397195493682 Training: 0.6420757452547641 Seed: 13160\n",
      "Testing: 0.6505366284739484 Training: 0.643953976135187 Seed: 13163\n",
      "Testing: 0.6689271729823654 Training: 0.6396065066407202 Seed: 13164\n",
      "Testing: 0.6641321906298421 Training: 0.64068361146284 Seed: 13165\n",
      "Testing: 0.64556701787992 Training: 0.6452761313925942 Seed: 13166\n",
      "Testing: 0.6657884374472536 Training: 0.6397800346844443 Seed: 13167\n",
      "Testing: 0.6504104329046333 Training: 0.6441899345406686 Seed: 13168\n",
      "Testing: 0.6565876561333491 Training: 0.6425677906360978 Seed: 13169\n",
      "Testing: 0.6540902383994074 Training: 0.6431841914851936 Seed: 13173\n",
      "Testing: 0.6580334815157644 Training: 0.6419680538685595 Seed: 13175\n",
      "Testing: 0.6460406758734466 Training: 0.6450254106420501 Seed: 13177\n",
      "Testing: 0.6477123717071498 Training: 0.6447210584631863 Seed: 13178\n",
      "Testing: 0.651025463921546 Training: 0.6439475865639342 Seed: 13179\n",
      "Testing: 0.6599067780523691 Training: 0.6418123869367124 Seed: 13180\n",
      "Testing: 0.6545794307017604 Training: 0.6427578828714736 Seed: 13181\n",
      "Testing: 0.6719753317813123 Training: 0.6386225957457445 Seed: 13182\n",
      "Testing: 0.6512772863310651 Training: 0.6438078720290568 Seed: 13183\n",
      "Testing: 0.6541531195334954 Training: 0.6432004021834014 Seed: 13187\n",
      "Testing: 0.6452151978831708 Training: 0.6449344919204533 Seed: 13188\n",
      "Testing: 0.6489711238053462 Training: 0.6444237798928144 Seed: 13190\n",
      "Testing: 0.6574798755880138 Training: 0.6419993998869843 Seed: 13193\n",
      "Testing: 0.6556381629643457 Training: 0.6427659806962756 Seed: 13194\n",
      "Testing: 0.647155061047943 Training: 0.6448519788572382 Seed: 13197\n",
      "Testing: 0.6602150949336043 Training: 0.6415815575611845 Seed: 13198\n",
      "Testing: 0.647873599214439 Training: 0.6447773681629607 Seed: 13200\n",
      "Testing: 0.6606280091303968 Training: 0.6416068225062491 Seed: 13203\n",
      "Testing: 0.6612198641824871 Training: 0.6410810719814015 Seed: 13204\n",
      "Testing: 0.6650509886159903 Training: 0.6404006186739399 Seed: 13205\n",
      "Testing: 0.6493931037886972 Training: 0.6443195437955402 Seed: 13208\n",
      "Testing: 0.6552328882393635 Training: 0.642951883515232 Seed: 13210\n",
      "Testing: 0.6502143557734323 Training: 0.6442413446579953 Seed: 13211\n",
      "Testing: 0.6465078457474254 Training: 0.6450087874128566 Seed: 13212\n",
      "Testing: 0.6585757385957209 Training: 0.6419301687236603 Seed: 13213\n",
      "Testing: 0.648075264471637 Training: 0.6446187378289832 Seed: 13215\n",
      "Testing: 0.645510358430432 Training: 0.6453143482131956 Seed: 13217\n",
      "Testing: 0.6621061622822375 Training: 0.6412887596460028 Seed: 13218\n",
      "Testing: 0.6599712455327925 Training: 0.64159659678123 Seed: 13219\n",
      "Testing: 0.6550652235625378 Training: 0.6429462039309126 Seed: 13221\n",
      "Testing: 0.6491801192655078 Training: 0.6444000033755093 Seed: 13224\n",
      "Testing: 0.657901810096329 Training: 0.6421590194840711 Seed: 13225\n",
      "Testing: 0.6540226905259154 Training: 0.6432189415525837 Seed: 13228\n",
      "Testing: 0.6472853256896688 Training: 0.6448966852586564 Seed: 13229\n",
      "Testing: 0.6513616671082442 Training: 0.6438810991429411 Seed: 13230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6582978272035873 Training: 0.6418450319776974 Seed: 13233\n",
      "Testing: 0.649079489559453 Training: 0.6441645423370883 Seed: 13234\n",
      "Testing: 0.6525980764591925 Training: 0.6434926364394402 Seed: 13235\n",
      "Testing: 0.6551427195104407 Training: 0.6428337281730105 Seed: 13238\n",
      "Testing: 0.6625544832013343 Training: 0.6409823835943058 Seed: 13242\n",
      "Testing: 0.6471870974351882 Training: 0.6448994638983275 Seed: 13244\n",
      "Testing: 0.6470868396338935 Training: 0.6449733521963897 Seed: 13245\n",
      "Testing: 0.6585110221081985 Training: 0.6421159808200357 Seed: 13246\n",
      "Testing: 0.6495582844142913 Training: 0.6443692125952848 Seed: 13248\n",
      "Testing: 0.6581979435622803 Training: 0.6422020128382546 Seed: 13250\n",
      "Testing: 0.6483245398782376 Training: 0.6445202871548376 Seed: 13251\n",
      "Testing: 0.6530974934890306 Training: 0.6435032651501368 Seed: 13254\n",
      "Testing: 0.6463184508024586 Training: 0.6451265197178356 Seed: 13255\n",
      "Testing: 0.647608334747594 Training: 0.6448148152790257 Seed: 13257\n",
      "Testing: 0.6510232661722769 Training: 0.6439554449864529 Seed: 13258\n",
      "Testing: 0.6572254877336292 Training: 0.642175541348804 Seed: 13259\n",
      "Testing: 0.653958601566146 Training: 0.6432138824203368 Seed: 13262\n",
      "Testing: 0.649264304995431 Training: 0.6444141027156864 Seed: 13265\n",
      "Testing: 0.647748662001993 Training: 0.6447736211889041 Seed: 13266\n",
      "Testing: 0.6532106682614344 Training: 0.6430887248105028 Seed: 13267\n",
      "Testing: 0.6488104318479283 Training: 0.6444751149921093 Seed: 13269\n",
      "Testing: 0.651775974228492 Training: 0.6438381107721196 Seed: 13270\n",
      "Testing: 0.6540070281110052 Training: 0.6431969176072042 Seed: 13271\n",
      "Testing: 0.6452061987693917 Training: 0.6446269121019218 Seed: 13272\n",
      "Testing: 0.6494886886437383 Training: 0.6443561794515749 Seed: 13273\n",
      "Testing: 0.6545951189277768 Training: 0.6430864392659502 Seed: 13274\n",
      "Testing: 0.6456050595869545 Training: 0.6447056681381742 Seed: 13275\n",
      "Testing: 0.6518206164862742 Training: 0.6438083718692735 Seed: 13278\n",
      "Testing: 0.6469485742493941 Training: 0.6449744800020484 Seed: 13279\n",
      "Testing: 0.6473471981575227 Training: 0.6449401592668984 Seed: 13281\n",
      "Testing: 0.6609015450275395 Training: 0.6414699025966971 Seed: 13282\n",
      "Testing: 0.6544874468597373 Training: 0.6430843330294798 Seed: 13292\n",
      "Testing: 0.653967772170513 Training: 0.6432448096742842 Seed: 13293\n",
      "Testing: 0.6517027118414407 Training: 0.6437693400122132 Seed: 13295\n",
      "Testing: 0.6533371407122349 Training: 0.6433015996164442 Seed: 13296\n",
      "Testing: 0.6555931715702167 Training: 0.6424014657221331 Seed: 13299\n",
      "Testing: 0.6639901932358535 Training: 0.6405944444988274 Seed: 13300\n",
      "Testing: 0.6542128527236141 Training: 0.6431326907963897 Seed: 13302\n",
      "Testing: 0.6485465427277008 Training: 0.6446501659115661 Seed: 13304\n",
      "Testing: 0.6688100935949941 Training: 0.6392626010487809 Seed: 13305\n",
      "Testing: 0.6660338984084897 Training: 0.6401579966935639 Seed: 13308\n",
      "Testing: 0.6488396342624461 Training: 0.6444873610929192 Seed: 13311\n",
      "Testing: 0.6555895393093389 Training: 0.6427331222651965 Seed: 13312\n",
      "Testing: 0.6596198456137266 Training: 0.6419261700399453 Seed: 13316\n",
      "Testing: 0.6504921912096779 Training: 0.6440660647562432 Seed: 13318\n",
      "Testing: 0.6561661232197488 Training: 0.6425243543660561 Seed: 13319\n",
      "Testing: 0.6481897964763123 Training: 0.6446356908704735 Seed: 13320\n",
      "Testing: 0.6570600389481437 Training: 0.6424215419454011 Seed: 13321\n",
      "Testing: 0.649585528854548 Training: 0.6442799577244213 Seed: 13325\n",
      "Testing: 0.6585845340631079 Training: 0.6421670313878676 Seed: 13327\n",
      "Testing: 0.6455473402590084 Training: 0.6451323697260134 Seed: 13329\n",
      "Testing: 0.6465093331435342 Training: 0.6449761956371456 Seed: 13331\n",
      "Testing: 0.6555835019952032 Training: 0.642733597589151 Seed: 13332\n",
      "Testing: 0.6630402384129366 Training: 0.6408567101506298 Seed: 13336\n",
      "Testing: 0.64840066034496 Training: 0.6441221604516001 Seed: 13344\n",
      "Testing: 0.6473696322425695 Training: 0.6449202340633334 Seed: 13346\n",
      "Testing: 0.6545676879791658 Training: 0.643072989819484 Seed: 13347\n",
      "Testing: 0.6476334551814861 Training: 0.6447313125553189 Seed: 13350\n",
      "Testing: 0.6518315356437429 Training: 0.6437242837674808 Seed: 13352\n",
      "Testing: 0.6482718502340094 Training: 0.644715371806621 Seed: 13355\n",
      "Testing: 0.6651711984131967 Training: 0.640489475614116 Seed: 13356\n",
      "Testing: 0.6555606798718764 Training: 0.6427935826690827 Seed: 13357\n",
      "Testing: 0.647684060271319 Training: 0.6447290598454045 Seed: 13363\n",
      "Testing: 0.6485569237441959 Training: 0.6442983211775982 Seed: 13364\n",
      "Testing: 0.6535179828898943 Training: 0.6431479526673242 Seed: 13367\n",
      "Testing: 0.648048210907786 Training: 0.6446959326953743 Seed: 13368\n",
      "Testing: 0.6493824290956364 Training: 0.6443680143171684 Seed: 13372\n",
      "Testing: 0.648292103485885 Training: 0.6447057660278264 Seed: 13375\n",
      "Testing: 0.6541096936989242 Training: 0.6431576013525653 Seed: 13377\n",
      "Testing: 0.6525216809457841 Training: 0.6434640747856625 Seed: 13380\n",
      "Testing: 0.6586237322561254 Training: 0.642025451465444 Seed: 13381\n",
      "Testing: 0.6536945328555495 Training: 0.6433171664989029 Seed: 13382\n",
      "Testing: 0.6514602711794303 Training: 0.6439004817954916 Seed: 13383\n",
      "Testing: 0.6466781833666554 Training: 0.6450385926284705 Seed: 13387\n",
      "Testing: 0.6522660431540075 Training: 0.6436412876253708 Seed: 13393\n",
      "Testing: 0.647007142004348 Training: 0.6450166924628782 Seed: 13395\n",
      "Testing: 0.6544145824931837 Training: 0.6430493690075645 Seed: 13396\n",
      "Testing: 0.6586910624583909 Training: 0.6419488395009925 Seed: 13399\n",
      "Testing: 0.6528501341357342 Training: 0.643466843581217 Seed: 13401\n",
      "Testing: 0.6566486843384969 Training: 0.6427079643221538 Seed: 13403\n",
      "Testing: 0.6534929287731434 Training: 0.64318514075486 Seed: 13404\n",
      "Testing: 0.6543876454930259 Training: 0.6431441849412523 Seed: 13407\n",
      "Testing: 0.6557880382645754 Training: 0.6426768118966164 Seed: 13409\n",
      "Testing: 0.6574888550076019 Training: 0.6423145217256431 Seed: 13410\n",
      "Testing: 0.6592116185827448 Training: 0.6419518048658341 Seed: 13411\n",
      "Testing: 0.6607317233239834 Training: 0.6416300617893291 Seed: 13413\n",
      "Testing: 0.6635846748831896 Training: 0.6409271090318696 Seed: 13415\n",
      "Testing: 0.662735610769955 Training: 0.641111143970487 Seed: 13416\n",
      "Testing: 0.648343915959263 Training: 0.6447278059440499 Seed: 13417\n",
      "Testing: 0.6488176837374804 Training: 0.6445036459285824 Seed: 13419\n",
      "Testing: 0.6488852446070441 Training: 0.6445437334786477 Seed: 13420\n",
      "Testing: 0.652967738769038 Training: 0.6434391136041782 Seed: 13425\n",
      "Testing: 0.6461607308667552 Training: 0.6452156133561683 Seed: 13427\n",
      "Testing: 0.6457968520088019 Training: 0.6452507555020068 Seed: 13433\n",
      "Testing: 0.651158015779029 Training: 0.6439525426310171 Seed: 13434\n",
      "Testing: 0.6579322335667904 Training: 0.6422836001590271 Seed: 13435\n",
      "Testing: 0.6658626369173238 Training: 0.6401361437676486 Seed: 13442\n",
      "Testing: 0.6589168899812212 Training: 0.6418918469096817 Seed: 13445\n",
      "Testing: 0.6509714273952222 Training: 0.6439248009815595 Seed: 13448\n",
      "Testing: 0.6455250081285533 Training: 0.6452935466661883 Seed: 13454\n",
      "Testing: 0.6492439541870187 Training: 0.6444144918025676 Seed: 13455\n",
      "Testing: 0.6465977053591306 Training: 0.6450584784098682 Seed: 13456\n",
      "Testing: 0.6513813178874605 Training: 0.6438711671403097 Seed: 13457\n",
      "Testing: 0.651242308053963 Training: 0.643932468267989 Seed: 13461\n",
      "Testing: 0.6535477567644634 Training: 0.6432939025162574 Seed: 13466\n",
      "Testing: 0.6599477098852923 Training: 0.6417616065930466 Seed: 13471\n",
      "Testing: 0.6494266678862312 Training: 0.6442141523859994 Seed: 13474\n",
      "Testing: 0.6522708435843255 Training: 0.6436071065030424 Seed: 13475\n",
      "Testing: 0.6497610780778471 Training: 0.6443603069983415 Seed: 13477\n",
      "Testing: 0.6500145841536488 Training: 0.6441188653456713 Seed: 13485\n",
      "Testing: 0.6581947607292861 Training: 0.6421556209517209 Seed: 13486\n",
      "Testing: 0.6474263717324581 Training: 0.6449090572140139 Seed: 13487\n",
      "Testing: 0.6597999707593956 Training: 0.6417774060013696 Seed: 13488\n",
      "Testing: 0.6559019023926879 Training: 0.6427858309985921 Seed: 13491\n",
      "Testing: 0.6478621486671557 Training: 0.6447570300071428 Seed: 13493\n",
      "Testing: 0.6542309669441492 Training: 0.6430852290999123 Seed: 13494\n",
      "Testing: 0.6501780495805214 Training: 0.6441352512826195 Seed: 13496\n",
      "Testing: 0.6492679262179673 Training: 0.6444807613950062 Seed: 13500\n",
      "Testing: 0.6540312019012711 Training: 0.6431906409043069 Seed: 13501\n",
      "Testing: 0.6473470565714703 Training: 0.6445425378022215 Seed: 13502\n",
      "Testing: 0.6546351461303136 Training: 0.6430542154265826 Seed: 13503\n",
      "Testing: 0.657090715242371 Training: 0.6424617924798832 Seed: 13506\n",
      "Testing: 0.6495057539060402 Training: 0.6442910802234201 Seed: 13509\n",
      "Testing: 0.6516643442587078 Training: 0.6438053669282924 Seed: 13512\n",
      "Testing: 0.6472822828482752 Training: 0.6448739305697583 Seed: 13514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6602304428639951 Training: 0.6417387561048937 Seed: 13517\n",
      "Testing: 0.6465220774555671 Training: 0.6450998330626603 Seed: 13519\n",
      "Testing: 0.6546727268993425 Training: 0.6429318512281978 Seed: 13524\n",
      "Testing: 0.6456324188794169 Training: 0.6451929191750935 Seed: 13525\n",
      "Testing: 0.647383290728013 Training: 0.6448966106196419 Seed: 13526\n",
      "Testing: 0.6559088125044287 Training: 0.6426549669928138 Seed: 13527\n",
      "Testing: 0.6514431266459026 Training: 0.6437732110849876 Seed: 13528\n",
      "Testing: 0.6457528785467906 Training: 0.6452796871756802 Seed: 13534\n",
      "Testing: 0.6507217399355975 Training: 0.6440668350679272 Seed: 13536\n",
      "Testing: 0.6517295385957618 Training: 0.6438548959212417 Seed: 13537\n",
      "Testing: 0.6477579770775689 Training: 0.644222125320226 Seed: 13538\n",
      "Testing: 0.6532654715910364 Training: 0.6434646243195914 Seed: 13541\n",
      "Testing: 0.6546777281157887 Training: 0.6431160778206481 Seed: 13542\n",
      "Testing: 0.6508679972592876 Training: 0.6439714081080974 Seed: 13545\n",
      "Testing: 0.6517413063409403 Training: 0.6437368173156937 Seed: 13547\n",
      "Testing: 0.6562655479549439 Training: 0.6427374580951647 Seed: 13548\n",
      "Testing: 0.6514342359706324 Training: 0.6439115660923461 Seed: 13552\n",
      "Testing: 0.6603981768818576 Training: 0.6414712474570408 Seed: 13554\n",
      "Testing: 0.6481355334240986 Training: 0.6446672092511431 Seed: 13556\n",
      "Testing: 0.6466370649507129 Training: 0.6450258156728298 Seed: 13562\n",
      "Testing: 0.6607780786959261 Training: 0.6415389844061475 Seed: 13565\n",
      "Testing: 0.6554077320812811 Training: 0.6429810254745069 Seed: 13566\n",
      "Testing: 0.650110651583195 Training: 0.6440544633482592 Seed: 13569\n",
      "Testing: 0.6594516427567007 Training: 0.6416034964766322 Seed: 13570\n",
      "Testing: 0.6635750922845094 Training: 0.6407300699267116 Seed: 13574\n",
      "Testing: 0.6614854327953602 Training: 0.6414945152756552 Seed: 13577\n",
      "Testing: 0.652364857872473 Training: 0.6436759726578997 Seed: 13580\n",
      "Testing: 0.6488893443448785 Training: 0.6444814147058888 Seed: 13581\n",
      "Testing: 0.6575127290887465 Training: 0.6424229121404325 Seed: 13583\n",
      "Testing: 0.6461920568655383 Training: 0.6451278168766605 Seed: 13585\n",
      "Testing: 0.6492565824526036 Training: 0.6443959250283554 Seed: 13586\n",
      "Testing: 0.6506177241521386 Training: 0.6440552143483739 Seed: 13587\n",
      "Testing: 0.6534261965727162 Training: 0.6432363839334676 Seed: 13588\n",
      "Testing: 0.6492782426650654 Training: 0.6444016860575059 Seed: 13589\n",
      "Testing: 0.6528871479709355 Training: 0.6435297513601252 Seed: 13590\n",
      "Testing: 0.6508465271232772 Training: 0.643918731899888 Seed: 13591\n",
      "Testing: 0.6519372681577034 Training: 0.643729560347637 Seed: 13592\n",
      "Testing: 0.6493495590813444 Training: 0.6444240442948426 Seed: 13595\n",
      "Testing: 0.6488910141682223 Training: 0.6444627467466265 Seed: 13598\n",
      "Testing: 0.6710943955100048 Training: 0.6390054134186325 Seed: 13602\n",
      "Testing: 0.6458450847953883 Training: 0.6452294014819107 Seed: 13604\n",
      "Testing: 0.6664823325445515 Training: 0.6402431415603769 Seed: 13607\n",
      "Testing: 0.6483841537094712 Training: 0.644626980335741 Seed: 13609\n",
      "Testing: 0.6512605058445984 Training: 0.6439073569582918 Seed: 13611\n",
      "Testing: 0.6545048198358937 Training: 0.643032544166323 Seed: 13614\n",
      "Testing: 0.6482216327258201 Training: 0.6445781286622096 Seed: 13615\n",
      "Testing: 0.6521196174763614 Training: 0.6437501176061947 Seed: 13616\n",
      "Testing: 0.6483186653978893 Training: 0.6446399195082275 Seed: 13617\n",
      "Testing: 0.6464994588253927 Training: 0.645086904647821 Seed: 13618\n",
      "Testing: 0.6542906505629175 Training: 0.6431951025920887 Seed: 13622\n",
      "Testing: 0.6646595541512892 Training: 0.6405731500806828 Seed: 13623\n",
      "Testing: 0.6627686470077188 Training: 0.6409557184271957 Seed: 13626\n",
      "Testing: 0.6638776793639852 Training: 0.6407941999174381 Seed: 13628\n",
      "Testing: 0.648585795327486 Training: 0.6445728657508256 Seed: 13633\n",
      "Testing: 0.6478683243802592 Training: 0.6447552836278123 Seed: 13635\n",
      "Testing: 0.6508716769129019 Training: 0.6440498631633657 Seed: 13636\n",
      "Testing: 0.6513768365107342 Training: 0.643883621519153 Seed: 13637\n",
      "Testing: 0.6462062405404274 Training: 0.6450276752826083 Seed: 13639\n",
      "Testing: 0.6505547059310776 Training: 0.6441302747985809 Seed: 13642\n",
      "Testing: 0.648440699845393 Training: 0.6445042529151354 Seed: 13643\n",
      "Testing: 0.6555582948189855 Training: 0.642820067030651 Seed: 13646\n",
      "Testing: 0.6591977843166457 Training: 0.6419367637288992 Seed: 13648\n",
      "Testing: 0.647462367671549 Training: 0.6448767601835236 Seed: 13651\n",
      "Testing: 0.6662417296948653 Training: 0.6402448275139747 Seed: 13655\n",
      "Testing: 0.6540702990904443 Training: 0.6430356470185821 Seed: 13658\n",
      "Testing: 0.6455913748163853 Training: 0.6453039646403886 Seed: 13662\n",
      "Testing: 0.6581305075871574 Training: 0.6422833148306294 Seed: 13663\n",
      "Testing: 0.6460934361448536 Training: 0.645277451969486 Seed: 13668\n",
      "Testing: 0.6518893734621977 Training: 0.6436082809503171 Seed: 13670\n",
      "Testing: 0.6463699172682267 Training: 0.6449801382417634 Seed: 13672\n",
      "Testing: 0.6590360897573244 Training: 0.6419712461825111 Seed: 13673\n",
      "Testing: 0.6561218464232903 Training: 0.6426514283002376 Seed: 13680\n",
      "Testing: 0.6525737492650798 Training: 0.6435713303953232 Seed: 13681\n",
      "Testing: 0.6456238023680064 Training: 0.645317636670636 Seed: 13682\n",
      "Testing: 0.650219078713047 Training: 0.6441681887745122 Seed: 13684\n",
      "Testing: 0.6559174183542048 Training: 0.6427841263932442 Seed: 13688\n",
      "Testing: 0.651349252563614 Training: 0.6439310379873537 Seed: 13689\n",
      "Testing: 0.6476939503155621 Training: 0.6447581504785621 Seed: 13690\n",
      "Testing: 0.6640544994698053 Training: 0.6407044357256036 Seed: 13691\n",
      "Testing: 0.647817156453727 Training: 0.6447911289644551 Seed: 13693\n",
      "Testing: 0.6598293595538417 Training: 0.6418022484788399 Seed: 13696\n",
      "Testing: 0.662911021734983 Training: 0.6409915615776591 Seed: 13699\n",
      "Testing: 0.6546437184908598 Training: 0.6431991418273941 Seed: 13700\n",
      "Testing: 0.6453092816157588 Training: 0.64528871846623 Seed: 13703\n",
      "Testing: 0.653363065580358 Training: 0.6427261659233161 Seed: 13704\n",
      "Testing: 0.6471743647337097 Training: 0.6448792584574958 Seed: 13705\n",
      "Testing: 0.6530318167780148 Training: 0.6434904604043314 Seed: 13713\n",
      "Testing: 0.6546357830171119 Training: 0.643045148850134 Seed: 13714\n",
      "Testing: 0.6468057772240658 Training: 0.6450794562029876 Seed: 13715\n",
      "Testing: 0.6496710561971641 Training: 0.6442883952838927 Seed: 13716\n",
      "Testing: 0.6606426101854509 Training: 0.6416480468284376 Seed: 13717\n",
      "Testing: 0.651753096662343 Training: 0.6438206718239328 Seed: 13719\n",
      "Testing: 0.6567222434444229 Training: 0.6425756249022329 Seed: 13720\n",
      "Testing: 0.6478194418160039 Training: 0.6447145003449366 Seed: 13721\n",
      "Testing: 0.6516155104272626 Training: 0.6437224350251372 Seed: 13725\n",
      "Testing: 0.6509896551249741 Training: 0.6439465617989072 Seed: 13726\n",
      "Testing: 0.6536939424954047 Training: 0.6433017587506188 Seed: 13727\n",
      "Testing: 0.6493045648038254 Training: 0.6443514382872856 Seed: 13728\n",
      "Testing: 0.6468719490628024 Training: 0.6449666812637028 Seed: 13730\n",
      "Testing: 0.6592359784294903 Training: 0.6419694301166815 Seed: 13731\n",
      "Testing: 0.6491193357952173 Training: 0.6444303649996997 Seed: 13735\n",
      "Testing: 0.6556835944393068 Training: 0.6426603707570728 Seed: 13737\n",
      "Testing: 0.6647182898730614 Training: 0.6404744661618048 Seed: 13739\n",
      "Testing: 0.6558725328391692 Training: 0.6428628759598727 Seed: 13740\n",
      "Testing: 0.6560388680512155 Training: 0.6425985077448759 Seed: 13746\n",
      "Testing: 0.6624480056902281 Training: 0.6410866948985832 Seed: 13747\n",
      "Testing: 0.6471967729464785 Training: 0.6449584945427528 Seed: 13751\n",
      "Testing: 0.6535115454704474 Training: 0.6432662510364715 Seed: 13752\n",
      "Testing: 0.6510774986985474 Training: 0.643994959878927 Seed: 13765\n",
      "Testing: 0.6463901407210627 Training: 0.6451398267892348 Seed: 13767\n",
      "Testing: 0.6548124728301525 Training: 0.6430336738463698 Seed: 13769\n",
      "Testing: 0.6614758660559146 Training: 0.6413203514873171 Seed: 13770\n",
      "Testing: 0.6612642229479614 Training: 0.6411556852282355 Seed: 13771\n",
      "Testing: 0.6510500389900964 Training: 0.6439869556311838 Seed: 13773\n",
      "Testing: 0.6457669039395398 Training: 0.6452389247764532 Seed: 13775\n",
      "Testing: 0.6608853198313493 Training: 0.6413301500516468 Seed: 13778\n",
      "Testing: 0.6523304051804221 Training: 0.6433566810273156 Seed: 13779\n",
      "Testing: 0.6651196853423516 Training: 0.6401540219544144 Seed: 13780\n",
      "Testing: 0.653997134495405 Training: 0.643231150551242 Seed: 13782\n",
      "Testing: 0.6595812920783267 Training: 0.6418926049367528 Seed: 13783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6467695025639363 Training: 0.6450113191262881 Seed: 13784\n",
      "Testing: 0.6721731217318627 Training: 0.6386465315491043 Seed: 13785\n",
      "Testing: 0.6618014576773132 Training: 0.6412580149940582 Seed: 13787\n",
      "Testing: 0.6560702315896224 Training: 0.6426940026299504 Seed: 13792\n",
      "Testing: 0.6460953915152212 Training: 0.6452002219777433 Seed: 13794\n",
      "Testing: 0.6559900634158742 Training: 0.6422123911851528 Seed: 13795\n",
      "Testing: 0.6554691548539963 Training: 0.6428736478683081 Seed: 13796\n",
      "Testing: 0.6466042935777325 Training: 0.6451147456242621 Seed: 13799\n",
      "Testing: 0.6539991334452148 Training: 0.6430213166635386 Seed: 13800\n",
      "Testing: 0.6533482591872388 Training: 0.6433859447496275 Seed: 13802\n",
      "Testing: 0.6474384220377772 Training: 0.6448161877960751 Seed: 13803\n",
      "Testing: 0.6464820103027021 Training: 0.6451314885368552 Seed: 13805\n",
      "Testing: 0.6520535723193707 Training: 0.6436053362716536 Seed: 13806\n",
      "Testing: 0.6635566842977458 Training: 0.6406314273139203 Seed: 13807\n",
      "Testing: 0.65140809362796 Training: 0.6436493408580899 Seed: 13809\n",
      "Testing: 0.6497688487192143 Training: 0.6442948010226582 Seed: 13813\n",
      "Testing: 0.6480335561912337 Training: 0.644714158281701 Seed: 13814\n",
      "Testing: 0.6481290949557628 Training: 0.6446457587568238 Seed: 13815\n",
      "Testing: 0.6539459234717032 Training: 0.6431768526297879 Seed: 13817\n",
      "Testing: 0.6519360657278753 Training: 0.6437452524279946 Seed: 13820\n",
      "Testing: 0.653523425964496 Training: 0.6432876239056597 Seed: 13821\n",
      "Testing: 0.6590005688193634 Training: 0.6415702518405704 Seed: 13823\n",
      "Testing: 0.6466670577712594 Training: 0.6450844865172713 Seed: 13829\n",
      "Testing: 0.6503335381552338 Training: 0.6441802673638449 Seed: 13832\n",
      "Testing: 0.6563510481514286 Training: 0.6424818003808666 Seed: 13835\n",
      "Testing: 0.6462632813181427 Training: 0.6452120774957185 Seed: 13836\n",
      "Testing: 0.6496909962487025 Training: 0.644411502063799 Seed: 13840\n",
      "Testing: 0.6521719176935608 Training: 0.6437723498957002 Seed: 13841\n",
      "Testing: 0.6500179252205769 Training: 0.6441069559029952 Seed: 13844\n",
      "Testing: 0.6473605814710462 Training: 0.6447822008447843 Seed: 13847\n",
      "Testing: 0.6546236342478486 Training: 0.6429829451190294 Seed: 13848\n",
      "Testing: 0.661973251657019 Training: 0.6410024237202914 Seed: 13850\n",
      "Testing: 0.6478575566264834 Training: 0.6447266505548195 Seed: 13851\n",
      "Testing: 0.6471226249779343 Training: 0.6448256009075939 Seed: 13857\n",
      "Testing: 0.6490460149805095 Training: 0.6444012646852375 Seed: 13858\n",
      "Testing: 0.6562664494735898 Training: 0.6426965276960739 Seed: 13862\n",
      "Testing: 0.6562888968039293 Training: 0.6425395199388511 Seed: 13863\n",
      "Testing: 0.6581336971146905 Training: 0.6422340934930038 Seed: 13864\n",
      "Testing: 0.646472512095764 Training: 0.6450188582178018 Seed: 13868\n",
      "Testing: 0.6483480580975016 Training: 0.64454067742434 Seed: 13870\n",
      "Testing: 0.6458456218577986 Training: 0.6452800899853672 Seed: 13872\n",
      "Testing: 0.6513461014431855 Training: 0.6439502204079635 Seed: 13873\n",
      "Testing: 0.6507006196360614 Training: 0.644063038929672 Seed: 13876\n",
      "Testing: 0.6509845362853778 Training: 0.6439524701140559 Seed: 13877\n",
      "Testing: 0.6616474682537914 Training: 0.641251473635859 Seed: 13879\n",
      "Testing: 0.6625410275669559 Training: 0.6410452162488576 Seed: 13880\n",
      "Testing: 0.6454532396467495 Training: 0.6454044252912681 Seed: 13882\n",
      "Testing: 0.6549615021472381 Training: 0.6429583798302995 Seed: 13884\n",
      "Testing: 0.6644709096751352 Training: 0.640447259239636 Seed: 13885\n",
      "Testing: 0.6511614375631714 Training: 0.6440028120629349 Seed: 13886\n",
      "Testing: 0.6599376455618835 Training: 0.6416855019422851 Seed: 13887\n",
      "Testing: 0.6466784650941639 Training: 0.6451033080025816 Seed: 13891\n",
      "Testing: 0.6530148019275716 Training: 0.6434748839138036 Seed: 13892\n",
      "Testing: 0.6544158333328389 Training: 0.6431337001456665 Seed: 13893\n",
      "Testing: 0.6525305335733269 Training: 0.6436854704447391 Seed: 13894\n",
      "Testing: 0.6660297769295858 Training: 0.6402164736437272 Seed: 13896\n",
      "Testing: 0.6726540916703582 Training: 0.6383639647887092 Seed: 13901\n",
      "Testing: 0.6530838196159015 Training: 0.6433796594475458 Seed: 13903\n",
      "Testing: 0.6525064443230993 Training: 0.6436344770140754 Seed: 13910\n",
      "Testing: 0.6553851932226666 Training: 0.642894837775904 Seed: 13912\n",
      "Testing: 0.6602520222952966 Training: 0.6418611962994372 Seed: 13913\n",
      "Testing: 0.6665757769354491 Training: 0.6398088591236922 Seed: 13914\n",
      "Testing: 0.6578971940113083 Training: 0.641946808232031 Seed: 13915\n",
      "Testing: 0.6492976791890863 Training: 0.6443305255997118 Seed: 13917\n",
      "Testing: 0.6494875399750536 Training: 0.6443518167946243 Seed: 13919\n",
      "Testing: 0.6619503021905602 Training: 0.6413112956456601 Seed: 13924\n",
      "Testing: 0.645642236307924 Training: 0.6452816312086369 Seed: 13925\n",
      "Testing: 0.6511563191266797 Training: 0.6438963458493351 Seed: 13926\n",
      "Testing: 0.6472450781638388 Training: 0.644974993121322 Seed: 13927\n",
      "Testing: 0.6641933418587483 Training: 0.6404891296385402 Seed: 13929\n",
      "Testing: 0.6547516728401777 Training: 0.6429351458855042 Seed: 13932\n",
      "Testing: 0.6533809410173497 Training: 0.643345531637106 Seed: 13933\n",
      "Testing: 0.6596572193949841 Training: 0.641847304142484 Seed: 13934\n",
      "Testing: 0.6507464386832712 Training: 0.6440485777510565 Seed: 13935\n",
      "Testing: 0.6519146640421596 Training: 0.6435292743175651 Seed: 13936\n",
      "Testing: 0.6471219342470814 Training: 0.6446946972682248 Seed: 13938\n",
      "Testing: 0.6654570464200724 Training: 0.640313932614383 Seed: 13939\n",
      "Testing: 0.6462242708163585 Training: 0.6452208733788428 Seed: 13942\n",
      "Testing: 0.6475356718016577 Training: 0.6446524534167717 Seed: 13944\n",
      "Testing: 0.6539874077293795 Training: 0.6431160390253764 Seed: 13945\n",
      "Testing: 0.6689516612190595 Training: 0.6394585848332333 Seed: 13950\n",
      "Testing: 0.6515848755585782 Training: 0.6436330792666305 Seed: 13955\n",
      "Testing: 0.6513502975888081 Training: 0.6439636959368745 Seed: 13956\n",
      "Testing: 0.6529830680189754 Training: 0.643579517835063 Seed: 13959\n",
      "Testing: 0.6515243282482066 Training: 0.6437510554630873 Seed: 13962\n",
      "Testing: 0.6489140146676501 Training: 0.6443426031204786 Seed: 13964\n",
      "Testing: 0.6583028995929986 Training: 0.6421564013081132 Seed: 13968\n",
      "Testing: 0.6481307162703484 Training: 0.6444168137647772 Seed: 13969\n",
      "Testing: 0.6552103374771999 Training: 0.6429945193540594 Seed: 13971\n",
      "Testing: 0.6594817714871288 Training: 0.6417801490056012 Seed: 13972\n",
      "Testing: 0.6501929074251649 Training: 0.6440911180958825 Seed: 13976\n",
      "Testing: 0.6547843260246498 Training: 0.643069730219834 Seed: 13978\n",
      "Testing: 0.6487755822220469 Training: 0.6444842387162925 Seed: 13980\n",
      "Testing: 0.6623653186314369 Training: 0.6411531359142392 Seed: 13981\n",
      "Testing: 0.6532670253723338 Training: 0.643379951790158 Seed: 13982\n",
      "Testing: 0.6462186076798964 Training: 0.6451696820362647 Seed: 13983\n",
      "Testing: 0.6591314174591748 Training: 0.6419208159004812 Seed: 13985\n",
      "Testing: 0.6525683978764547 Training: 0.6435275928986244 Seed: 13986\n",
      "Testing: 0.6512626779523487 Training: 0.6438467657403351 Seed: 13991\n",
      "Testing: 0.6564274670221082 Training: 0.6423905480560217 Seed: 13993\n",
      "Testing: 0.6494769787085999 Training: 0.6444301021767944 Seed: 13997\n",
      "Testing: 0.6531142868757578 Training: 0.6434446668256443 Seed: 13998\n",
      "Testing: 0.6589116253273444 Training: 0.641942077130651 Seed: 13999\n",
      "Testing: 0.6570462634501512 Training: 0.6423861869201526 Seed: 14001\n",
      "Testing: 0.6512384594538558 Training: 0.6439240864019337 Seed: 14003\n",
      "Testing: 0.6505723205186178 Training: 0.6439159607745718 Seed: 14004\n",
      "Testing: 0.6566347637598126 Training: 0.6424351819243506 Seed: 14006\n",
      "Testing: 0.6519084369295288 Training: 0.6437133930893613 Seed: 14008\n",
      "Testing: 0.6563013190237088 Training: 0.6426613130781396 Seed: 14009\n",
      "Testing: 0.6491588200314348 Training: 0.6444726628946273 Seed: 14012\n",
      "Testing: 0.6501327440851496 Training: 0.644216896310654 Seed: 14019\n",
      "Testing: 0.6544055536909857 Training: 0.6431656264474199 Seed: 14020\n",
      "Testing: 0.6546744452834605 Training: 0.6430129634399022 Seed: 14021\n",
      "Testing: 0.6542363576058594 Training: 0.6432410094118384 Seed: 14022\n",
      "Testing: 0.6533355132815256 Training: 0.6434031324876739 Seed: 14025\n",
      "Testing: 0.6597089462758353 Training: 0.6417070848465753 Seed: 14029\n",
      "Testing: 0.6583428609654512 Training: 0.6421335801806289 Seed: 14030\n",
      "Testing: 0.6553236089371735 Training: 0.6429305495268652 Seed: 14032\n",
      "Testing: 0.649908227729342 Training: 0.6443599496727409 Seed: 14033\n",
      "Testing: 0.6544745963872394 Training: 0.6431790376852284 Seed: 14035\n",
      "Testing: 0.6579480694349891 Training: 0.6423257737196661 Seed: 14037\n",
      "Testing: 0.6474682765038198 Training: 0.6448867762723545 Seed: 14039\n",
      "Testing: 0.666721543703312 Training: 0.6399613651776916 Seed: 14040\n",
      "Testing: 0.6599796370585571 Training: 0.6417124947750839 Seed: 14041\n",
      "Testing: 0.6506055568894566 Training: 0.644059084610086 Seed: 14043\n",
      "Testing: 0.6497291411450937 Training: 0.6440778084331098 Seed: 14044\n",
      "Testing: 0.6493520025792514 Training: 0.6444239410448079 Seed: 14049\n",
      "Testing: 0.6624966403695632 Training: 0.6410385931702384 Seed: 14050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6460349185058596 Training: 0.6451488656001442 Seed: 14053\n",
      "Testing: 0.6533882439498231 Training: 0.6433165146012296 Seed: 14056\n",
      "Testing: 0.6462973350813357 Training: 0.6452500167125254 Seed: 14057\n",
      "Testing: 0.6568434791214155 Training: 0.6427131770471328 Seed: 14058\n",
      "Testing: 0.6516525613987811 Training: 0.6438134171095913 Seed: 14059\n",
      "Testing: 0.6498722482803787 Training: 0.6443126193582458 Seed: 14060\n",
      "Testing: 0.6459212514305065 Training: 0.6449729367869633 Seed: 14063\n",
      "Testing: 0.6523807335331698 Training: 0.643557458078859 Seed: 14066\n",
      "Testing: 0.6544465454430414 Training: 0.643094646161917 Seed: 14067\n",
      "Testing: 0.6573745722631436 Training: 0.642549171201094 Seed: 14069\n",
      "Testing: 0.6622037740311403 Training: 0.6412598198306256 Seed: 14070\n",
      "Testing: 0.6511179013425327 Training: 0.6440335467767211 Seed: 14078\n",
      "Testing: 0.6596024133219522 Training: 0.641678021895556 Seed: 14081\n",
      "Testing: 0.646025251997709 Training: 0.6451286759499602 Seed: 14087\n",
      "Testing: 0.6616300548036069 Training: 0.6414863311685159 Seed: 14089\n",
      "Testing: 0.6487613312543663 Training: 0.6445475680036507 Seed: 14091\n",
      "Testing: 0.6632244749787489 Training: 0.6408515015621743 Seed: 14092\n",
      "Testing: 0.6479222715694374 Training: 0.644576469551529 Seed: 14093\n",
      "Testing: 0.652820917573198 Training: 0.6434811548087913 Seed: 14097\n",
      "Testing: 0.6488924951940274 Training: 0.6444822645671494 Seed: 14098\n",
      "Testing: 0.6564541645025631 Training: 0.6425021893180853 Seed: 14100\n",
      "Testing: 0.6505623867257936 Training: 0.6440312238532687 Seed: 14104\n",
      "Testing: 0.6467423721025977 Training: 0.6449525010928718 Seed: 14108\n",
      "Testing: 0.6512838555586026 Training: 0.6438099182986188 Seed: 14109\n",
      "Testing: 0.6491463403045543 Training: 0.6445009316057864 Seed: 14110\n",
      "Testing: 0.6568960838988716 Training: 0.6424306746697465 Seed: 14111\n",
      "Testing: 0.6543453394659166 Training: 0.6430952276886218 Seed: 14113\n",
      "Testing: 0.6726318155508705 Training: 0.6385201478839233 Seed: 14114\n",
      "Testing: 0.6576910002060135 Training: 0.6422642752250358 Seed: 14116\n",
      "Testing: 0.6516645382683427 Training: 0.6438057225408641 Seed: 14117\n",
      "Testing: 0.6684529638689396 Training: 0.6394776952376947 Seed: 14118\n",
      "Testing: 0.6474487309115596 Training: 0.644856044546992 Seed: 14119\n",
      "Testing: 0.6535782391313539 Training: 0.6432049023632518 Seed: 14120\n",
      "Testing: 0.6589729484765745 Training: 0.6420575682285159 Seed: 14122\n",
      "Testing: 0.65138798251045 Training: 0.6439498487445502 Seed: 14127\n",
      "Testing: 0.659667608776612 Training: 0.6418821709144766 Seed: 14128\n",
      "Testing: 0.6523060039860671 Training: 0.6436140810597312 Seed: 14130\n",
      "Testing: 0.670406329347868 Training: 0.6390748173430361 Seed: 14131\n",
      "Testing: 0.6522472391576708 Training: 0.6436158137626097 Seed: 14134\n",
      "Testing: 0.6720608333037741 Training: 0.6387855003083143 Seed: 14135\n",
      "Testing: 0.6530807383026199 Training: 0.6435091513401787 Seed: 14137\n",
      "Testing: 0.6515010963050039 Training: 0.643830837000773 Seed: 14139\n",
      "Testing: 0.666326553538249 Training: 0.6401705277381375 Seed: 14141\n",
      "Testing: 0.6508391394699047 Training: 0.6439033285174284 Seed: 14142\n",
      "Testing: 0.6508071364763404 Training: 0.6438950289616092 Seed: 14143\n",
      "Testing: 0.6478281338193671 Training: 0.6447974196741557 Seed: 14144\n",
      "Testing: 0.6495834675810799 Training: 0.6443233095806301 Seed: 14146\n",
      "Testing: 0.6522139701289704 Training: 0.6436771364956713 Seed: 14147\n",
      "Testing: 0.666179127466802 Training: 0.640175478057279 Seed: 14148\n",
      "Testing: 0.648008075162218 Training: 0.6447643986122427 Seed: 14150\n",
      "Testing: 0.6455471914063826 Training: 0.645289354480472 Seed: 14151\n",
      "Testing: 0.6632160471957729 Training: 0.6405794967466811 Seed: 14152\n",
      "Testing: 0.6533202518770723 Training: 0.6432944026366 Seed: 14153\n",
      "Testing: 0.6556609734140988 Training: 0.6428137183937588 Seed: 14158\n",
      "Testing: 0.64950382490013 Training: 0.6436916923019851 Seed: 14159\n",
      "Testing: 0.6481557935561821 Training: 0.64473106138856 Seed: 14161\n",
      "Testing: 0.6527666315172584 Training: 0.6435255900162808 Seed: 14163\n",
      "Testing: 0.6491877709837126 Training: 0.6444853674816795 Seed: 14164\n",
      "Testing: 0.6464860543743396 Training: 0.6451453216755092 Seed: 14167\n",
      "Testing: 0.6490508821799467 Training: 0.6444947089656706 Seed: 14170\n",
      "Testing: 0.6477381467618711 Training: 0.6447046045108238 Seed: 14172\n",
      "Testing: 0.6607448066321321 Training: 0.6415718081648033 Seed: 14173\n",
      "Testing: 0.6539584778667471 Training: 0.643111018154834 Seed: 14174\n",
      "Testing: 0.6559163003680059 Training: 0.6426462184379496 Seed: 14178\n",
      "Testing: 0.6457785194694534 Training: 0.6453078298548789 Seed: 14179\n",
      "Testing: 0.647328391592342 Training: 0.6449306265295041 Seed: 14180\n",
      "Testing: 0.6453910904483271 Training: 0.6452993218428648 Seed: 14185\n",
      "Testing: 0.6519094555326326 Training: 0.6433836077331927 Seed: 14186\n",
      "Testing: 0.6455565379842875 Training: 0.645286858437264 Seed: 14187\n",
      "Testing: 0.6535181827101298 Training: 0.6433919744880048 Seed: 14188\n",
      "Testing: 0.6569591596738177 Training: 0.6425451736295725 Seed: 14192\n",
      "Testing: 0.6668228567654916 Training: 0.6399070408870005 Seed: 14194\n",
      "Testing: 0.6477795582955423 Training: 0.6446969026842887 Seed: 14195\n",
      "Testing: 0.6650782096081234 Training: 0.640426960580836 Seed: 14202\n",
      "Testing: 0.6496718800330952 Training: 0.6443172149303391 Seed: 14203\n",
      "Testing: 0.6574584117009945 Training: 0.6422496994906515 Seed: 14204\n",
      "Testing: 0.6486185456474094 Training: 0.6444860016149091 Seed: 14205\n",
      "Testing: 0.648880436180658 Training: 0.6445057912514872 Seed: 14208\n",
      "Testing: 0.6500784386819894 Training: 0.6438835019074256 Seed: 14212\n",
      "Testing: 0.6573621508662064 Training: 0.6420280637016846 Seed: 14213\n",
      "Testing: 0.6456416261040084 Training: 0.6452960572947113 Seed: 14216\n",
      "Testing: 0.6529604587230314 Training: 0.6435233472987388 Seed: 14218\n",
      "Testing: 0.664189826637904 Training: 0.6405742916500696 Seed: 14219\n",
      "Testing: 0.6472586270456986 Training: 0.644912261173272 Seed: 14220\n",
      "Testing: 0.6571455640946415 Training: 0.6422717135354002 Seed: 14221\n",
      "Testing: 0.6611807533272716 Training: 0.6413662336063437 Seed: 14223\n",
      "Testing: 0.6502587360098594 Training: 0.644103508047942 Seed: 14226\n",
      "Testing: 0.6638063029358878 Training: 0.6409377079769547 Seed: 14229\n",
      "Testing: 0.6504841478749511 Training: 0.6439756137875983 Seed: 14232\n",
      "Testing: 0.6578193034810931 Training: 0.6422591035977661 Seed: 14234\n",
      "Testing: 0.6716009687772606 Training: 0.6388830369669053 Seed: 14235\n",
      "Testing: 0.653677741990054 Training: 0.6427059636408179 Seed: 14236\n",
      "Testing: 0.646822659671695 Training: 0.6449746889918282 Seed: 14238\n",
      "Testing: 0.6488945425987771 Training: 0.6443830634174456 Seed: 14239\n",
      "Testing: 0.6681087460032147 Training: 0.6396926815639701 Seed: 14244\n",
      "Testing: 0.6453551133882686 Training: 0.6453451284173783 Seed: 14247\n",
      "Testing: 0.665197376126891 Training: 0.6403099956254228 Seed: 14248\n",
      "Testing: 0.6501798741756956 Training: 0.6441680957071325 Seed: 14250\n",
      "Testing: 0.6557384828021627 Training: 0.6425781933424288 Seed: 14251\n",
      "Testing: 0.6512913863489569 Training: 0.6434767438397243 Seed: 14256\n",
      "Testing: 0.6565331946605655 Training: 0.6425221499147937 Seed: 14258\n",
      "Testing: 0.6505540895783184 Training: 0.644128169132095 Seed: 14259\n",
      "Testing: 0.6453656764921846 Training: 0.6453462411364297 Seed: 14260\n",
      "Testing: 0.6489614057570765 Training: 0.6443750975753763 Seed: 14261\n",
      "Testing: 0.6504333339465597 Training: 0.6441496051059475 Seed: 14263\n",
      "Testing: 0.6572424794338549 Training: 0.6423998373939271 Seed: 14266\n",
      "Testing: 0.6508764354174343 Training: 0.6440544292460597 Seed: 14268\n",
      "Testing: 0.6541889639619846 Training: 0.642931061698774 Seed: 14270\n",
      "Testing: 0.6477050843006386 Training: 0.6448099896316021 Seed: 14271\n",
      "Testing: 0.6503321297012324 Training: 0.6440697190474225 Seed: 14273\n",
      "Testing: 0.6525629757672738 Training: 0.6435526392132593 Seed: 14274\n",
      "Testing: 0.6540177742348192 Training: 0.6431545298170251 Seed: 14275\n",
      "Testing: 0.6530133470136291 Training: 0.6435085697423564 Seed: 14279\n",
      "Testing: 0.6479418619024169 Training: 0.644695300697498 Seed: 14280\n",
      "Testing: 0.6455793393419778 Training: 0.6453460852028339 Seed: 14282\n",
      "Testing: 0.6531985269614737 Training: 0.6434703714794778 Seed: 14286\n",
      "Testing: 0.6508746425923455 Training: 0.6439609159448481 Seed: 14287\n",
      "Testing: 0.6539035865040994 Training: 0.6430585824078909 Seed: 14289\n",
      "Testing: 0.6697526199189944 Training: 0.6390027655344728 Seed: 14290\n",
      "Testing: 0.6641407935722518 Training: 0.6407826424932334 Seed: 14293\n",
      "Testing: 0.6497895650050831 Training: 0.6441850244830718 Seed: 14294\n",
      "Testing: 0.6481779901075825 Training: 0.6446607000589128 Seed: 14296\n",
      "Testing: 0.6459845871799811 Training: 0.6451808578723954 Seed: 14300\n",
      "Testing: 0.6535292274795415 Training: 0.6432310178268666 Seed: 14301\n",
      "Testing: 0.6506017117704369 Training: 0.6441357654711513 Seed: 14302\n",
      "Testing: 0.6569239453762403 Training: 0.6426242542011733 Seed: 14303\n",
      "Testing: 0.6474592438861373 Training: 0.6440327076089166 Seed: 14304\n",
      "Testing: 0.6552976180348418 Training: 0.6429014565560388 Seed: 14305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6539196999859808 Training: 0.6432474672630486 Seed: 14308\n",
      "Testing: 0.6511215839294335 Training: 0.6439177720045283 Seed: 14309\n",
      "Testing: 0.6497168773872619 Training: 0.6443581378933363 Seed: 14310\n",
      "Testing: 0.6462366235344628 Training: 0.6451577886122484 Seed: 14311\n",
      "Testing: 0.6460348518049484 Training: 0.6452068187746924 Seed: 14316\n",
      "Testing: 0.6535154981811601 Training: 0.6432172158107805 Seed: 14320\n",
      "Testing: 0.6534672432505471 Training: 0.6432315667569315 Seed: 14321\n",
      "Testing: 0.6640595337105352 Training: 0.6405051285066213 Seed: 14329\n",
      "Testing: 0.6558070801259226 Training: 0.6427252562566572 Seed: 14331\n",
      "Testing: 0.6454609281463977 Training: 0.6453735760213704 Seed: 14332\n",
      "Testing: 0.6710624692242293 Training: 0.6386877518017516 Seed: 14334\n",
      "Testing: 0.6550480752966052 Training: 0.6429584451018078 Seed: 14335\n",
      "Testing: 0.6482487494803952 Training: 0.6446485071811738 Seed: 14340\n",
      "Testing: 0.6537486238356448 Training: 0.6432554928139279 Seed: 14341\n",
      "Testing: 0.6472069442725695 Training: 0.6449562636130466 Seed: 14342\n",
      "Testing: 0.6555532494088412 Training: 0.642863956023887 Seed: 14345\n",
      "Testing: 0.6578466384262469 Training: 0.6422616838319335 Seed: 14346\n",
      "Testing: 0.6586283387590472 Training: 0.6419492283952069 Seed: 14348\n",
      "Testing: 0.6491526740119214 Training: 0.6443449833273676 Seed: 14349\n",
      "Testing: 0.6480128715088411 Training: 0.6444049957247459 Seed: 14350\n",
      "Testing: 0.6519427355467833 Training: 0.6435811477581088 Seed: 14352\n",
      "Testing: 0.6534061336914084 Training: 0.6431708162651858 Seed: 14353\n",
      "Testing: 0.6489803915314027 Training: 0.6439437197927058 Seed: 14354\n",
      "Testing: 0.6459212591519982 Training: 0.6452367446046103 Seed: 14356\n",
      "Testing: 0.6511397274806355 Training: 0.6439115423869689 Seed: 14357\n",
      "Testing: 0.6684574600760027 Training: 0.6394173641260401 Seed: 14359\n",
      "Testing: 0.6551720226126296 Training: 0.642832844986643 Seed: 14360\n",
      "Testing: 0.6478574270721638 Training: 0.6446414173730269 Seed: 14364\n",
      "Testing: 0.6534109439743319 Training: 0.6434011679588314 Seed: 14366\n",
      "Testing: 0.6533976692830503 Training: 0.6432398962534442 Seed: 14369\n",
      "Testing: 0.6658599736386582 Training: 0.6404056027245235 Seed: 14372\n",
      "Testing: 0.6481028787913945 Training: 0.6447334102511341 Seed: 14373\n",
      "Testing: 0.6583281597492116 Training: 0.6420568906545868 Seed: 14375\n",
      "Testing: 0.6533460958086276 Training: 0.643332542727417 Seed: 14380\n",
      "Testing: 0.6485774174325508 Training: 0.6446127042227439 Seed: 14382\n",
      "Testing: 0.6603642709237887 Training: 0.6416185836770285 Seed: 14383\n",
      "Testing: 0.6564582190676626 Training: 0.6425348556834602 Seed: 14385\n",
      "Testing: 0.6581028847341608 Training: 0.6419354566509636 Seed: 14386\n",
      "Testing: 0.6537135772346387 Training: 0.6433884091707467 Seed: 14387\n",
      "Testing: 0.6551029936748652 Training: 0.6428891764654056 Seed: 14391\n",
      "Testing: 0.6704276847718652 Training: 0.6388311587918138 Seed: 14392\n",
      "Testing: 0.6462876433846774 Training: 0.6451736598570496 Seed: 14394\n",
      "Testing: 0.6683574481724319 Training: 0.6395693203367556 Seed: 14395\n",
      "Testing: 0.6495261455803523 Training: 0.6443409679404146 Seed: 14397\n",
      "Testing: 0.6516570266608063 Training: 0.6437896170287624 Seed: 14399\n",
      "Testing: 0.652900473332387 Training: 0.643509921729303 Seed: 14400\n",
      "Testing: 0.6455215478807006 Training: 0.6453103417989772 Seed: 14404\n",
      "Testing: 0.6454481835592891 Training: 0.6454388178911827 Seed: 14405\n",
      "Testing: 0.6621326862582821 Training: 0.6410276862734028 Seed: 14406\n",
      "Testing: 0.6506644285285621 Training: 0.6441347042645704 Seed: 14407\n",
      "Testing: 0.659977145404655 Training: 0.6416523951165398 Seed: 14409\n",
      "Testing: 0.6515896675832606 Training: 0.6433391187022404 Seed: 14413\n",
      "Testing: 0.6593421202979461 Training: 0.6414271581679667 Seed: 14423\n",
      "Testing: 0.6577960521525145 Training: 0.6423502938939201 Seed: 14425\n",
      "Testing: 0.6530933944072943 Training: 0.6434076192525799 Seed: 14428\n",
      "Testing: 0.6500331221716547 Training: 0.6442074223788026 Seed: 14429\n",
      "Testing: 0.6474053363746496 Training: 0.644852583241702 Seed: 14430\n",
      "Testing: 0.657044092865661 Training: 0.6425004923154012 Seed: 14431\n",
      "Testing: 0.6535424058437039 Training: 0.6432305249131678 Seed: 14432\n",
      "Testing: 0.6455982467013075 Training: 0.6453534270068667 Seed: 14434\n",
      "Testing: 0.6486571535477658 Training: 0.6443296053843139 Seed: 14437\n",
      "Testing: 0.6562123027392842 Training: 0.64260821037223 Seed: 14438\n",
      "Testing: 0.6568106155723867 Training: 0.6425899928311519 Seed: 14439\n",
      "Testing: 0.651579968538851 Training: 0.643694362732275 Seed: 14440\n",
      "Testing: 0.6543477143097165 Training: 0.6431850465511945 Seed: 14443\n",
      "Testing: 0.6570364917970759 Training: 0.6424549962377736 Seed: 14445\n",
      "Testing: 0.6482293068831565 Training: 0.6446421034454668 Seed: 14447\n",
      "Testing: 0.6531881185340814 Training: 0.6433166591223334 Seed: 14449\n",
      "Testing: 0.6494392507192623 Training: 0.6444017216565768 Seed: 14450\n",
      "Testing: 0.6689448722005492 Training: 0.6392258297412725 Seed: 14451\n",
      "Testing: 0.659703989560819 Training: 0.6419158049928413 Seed: 14453\n",
      "Testing: 0.6609871823926945 Training: 0.6413533787889195 Seed: 14454\n",
      "Testing: 0.6453542612210679 Training: 0.6453107893548682 Seed: 14457\n",
      "Testing: 0.6512436108531877 Training: 0.6438814365295444 Seed: 14459\n",
      "Testing: 0.6504011189443292 Training: 0.6438716140794759 Seed: 14464\n",
      "Testing: 0.6515129199368503 Training: 0.6438122070764304 Seed: 14465\n",
      "Testing: 0.6472855680154271 Training: 0.6449218383982884 Seed: 14467\n",
      "Testing: 0.6501270477937174 Training: 0.6442756967754703 Seed: 14470\n",
      "Testing: 0.6483888636578958 Training: 0.6446983320359541 Seed: 14473\n",
      "Testing: 0.6589493436408725 Training: 0.6420124557539425 Seed: 14474\n",
      "Testing: 0.6565483166881323 Training: 0.6424364775257205 Seed: 14475\n",
      "Testing: 0.6527418331930326 Training: 0.6433250689124297 Seed: 14484\n",
      "Testing: 0.6460078380114997 Training: 0.6452399405220337 Seed: 14487\n",
      "Testing: 0.6466982973591682 Training: 0.6450653089030776 Seed: 14488\n",
      "Testing: 0.6620940229011449 Training: 0.6412714067183263 Seed: 14494\n",
      "Testing: 0.6492057943595612 Training: 0.6444388185675868 Seed: 14495\n",
      "Testing: 0.6585244506788341 Training: 0.6420971562672748 Seed: 14496\n",
      "Testing: 0.6587582178676281 Training: 0.6418446324206828 Seed: 14498\n",
      "Testing: 0.6524767368900556 Training: 0.6436730575202647 Seed: 14500\n",
      "Testing: 0.6537886681649989 Training: 0.6431877883565028 Seed: 14505\n",
      "Testing: 0.6505565689747866 Training: 0.6440350813867557 Seed: 14506\n",
      "Testing: 0.6591528013085928 Training: 0.6418239553937413 Seed: 14507\n",
      "Testing: 0.6532992993302745 Training: 0.6433500618853568 Seed: 14510\n",
      "Testing: 0.6461716757785936 Training: 0.6450166286714643 Seed: 14512\n",
      "Testing: 0.6459077985162021 Training: 0.6447946556387887 Seed: 14514\n",
      "Testing: 0.6475960105273425 Training: 0.6448741240491731 Seed: 14516\n",
      "Testing: 0.6514731356614142 Training: 0.6438573005934328 Seed: 14518\n",
      "Testing: 0.6506322313809332 Training: 0.6440716119646469 Seed: 14520\n",
      "Testing: 0.6486937962217514 Training: 0.6443883646349168 Seed: 14522\n",
      "Testing: 0.6456638474821865 Training: 0.6452185665013788 Seed: 14523\n",
      "Testing: 0.6491502087660708 Training: 0.6442466755175904 Seed: 14524\n",
      "Testing: 0.6522021461172616 Training: 0.6436775995571482 Seed: 14526\n",
      "Testing: 0.6547290011155851 Training: 0.6425727404538094 Seed: 14527\n",
      "Testing: 0.6643595805037076 Training: 0.6406744054858922 Seed: 14528\n",
      "Testing: 0.6519836504984214 Training: 0.6437115325975609 Seed: 14529\n",
      "Testing: 0.6501148276023787 Training: 0.6442025704429335 Seed: 14530\n",
      "Testing: 0.6471165885010733 Training: 0.644990283717682 Seed: 14533\n",
      "Testing: 0.649402995183082 Training: 0.6444412956275214 Seed: 14534\n",
      "Testing: 0.6579333485813583 Training: 0.6422683700960763 Seed: 14539\n",
      "Testing: 0.6499817777846436 Training: 0.64420171612372 Seed: 14540\n",
      "Testing: 0.6657938953899275 Training: 0.6402575801636055 Seed: 14541\n",
      "Testing: 0.6547861715744898 Training: 0.6431113728415042 Seed: 14546\n",
      "Testing: 0.6612498151104858 Training: 0.6412813647258457 Seed: 14548\n",
      "Testing: 0.659629768750026 Training: 0.641832452253513 Seed: 14550\n",
      "Testing: 0.650614486210793 Training: 0.644066448233931 Seed: 14552\n",
      "Testing: 0.6475253560720818 Training: 0.6448491992358205 Seed: 14555\n",
      "Testing: 0.6567266142568525 Training: 0.6424824563853179 Seed: 14561\n",
      "Testing: 0.6568401033557245 Training: 0.6425466851766602 Seed: 14562\n",
      "Testing: 0.6609776361506969 Training: 0.6412072333461699 Seed: 14564\n",
      "Testing: 0.6568208960998979 Training: 0.6425027543051107 Seed: 14567\n",
      "Testing: 0.6520329546626207 Training: 0.6437397504932905 Seed: 14569\n",
      "Testing: 0.6710013974716695 Training: 0.6385126353990902 Seed: 14572\n",
      "Testing: 0.6476126699878247 Training: 0.6447617389593913 Seed: 14573\n",
      "Testing: 0.6472479434624931 Training: 0.6449911331362082 Seed: 14574\n",
      "Testing: 0.6605595768675773 Training: 0.6413322773732006 Seed: 14576\n",
      "Testing: 0.6603887590014228 Training: 0.641588199369453 Seed: 14577\n",
      "Testing: 0.6596795456407533 Training: 0.6417540813813314 Seed: 14578\n",
      "Testing: 0.6487220424292935 Training: 0.6445186691037372 Seed: 14580\n",
      "Testing: 0.6683737890019971 Training: 0.6395669881332271 Seed: 14583\n",
      "Testing: 0.6491238924829693 Training: 0.6444138782670759 Seed: 14584\n",
      "Testing: 0.6495130206959292 Training: 0.6444007081617937 Seed: 14586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6582726668341412 Training: 0.6421989279793163 Seed: 14587\n",
      "Testing: 0.6569498072488883 Training: 0.6423884929761339 Seed: 14588\n",
      "Testing: 0.657229920202568 Training: 0.6422308901975735 Seed: 14592\n",
      "Testing: 0.6479053840988458 Training: 0.6446293730386533 Seed: 14595\n",
      "Testing: 0.6477409734126907 Training: 0.6447775272222833 Seed: 14596\n",
      "Testing: 0.6483999906480106 Training: 0.6445744649233169 Seed: 14597\n",
      "Testing: 0.6463178867085643 Training: 0.6451523001448582 Seed: 14598\n",
      "Testing: 0.6538094237566908 Training: 0.6431957291606787 Seed: 14600\n",
      "Testing: 0.6506503182651764 Training: 0.6440177120659621 Seed: 14602\n",
      "Testing: 0.6629641273867134 Training: 0.6408459155503656 Seed: 14603\n",
      "Testing: 0.6489839692802051 Training: 0.6444771369746944 Seed: 14604\n",
      "Testing: 0.6468278144968438 Training: 0.6449538907901097 Seed: 14605\n",
      "Testing: 0.6458305690925407 Training: 0.6452228954973933 Seed: 14606\n",
      "Testing: 0.6545307112365367 Training: 0.6428575808631597 Seed: 14607\n",
      "Testing: 0.6532262593579004 Training: 0.6434622069256882 Seed: 14608\n",
      "Testing: 0.6454468228310475 Training: 0.6453355756978345 Seed: 14609\n",
      "Testing: 0.6530072819779019 Training: 0.643375206027464 Seed: 14612\n",
      "Testing: 0.655403617251435 Training: 0.6428491828230157 Seed: 14614\n",
      "Testing: 0.652030750327938 Training: 0.643239515395598 Seed: 14617\n",
      "Testing: 0.6472620595631372 Training: 0.6449415044264885 Seed: 14621\n",
      "Testing: 0.6522376659540324 Training: 0.6437061875164054 Seed: 14623\n",
      "Testing: 0.6562664711030901 Training: 0.6425933785700636 Seed: 14624\n",
      "Testing: 0.64889835602794 Training: 0.6445211726210247 Seed: 14625\n",
      "Testing: 0.6643027152523887 Training: 0.6405999089529416 Seed: 14628\n",
      "Testing: 0.6466624906834921 Training: 0.6450115820154653 Seed: 14630\n",
      "Testing: 0.6484541220241391 Training: 0.6444195388158083 Seed: 14632\n",
      "Testing: 0.665819023146368 Training: 0.6398682765254256 Seed: 14634\n",
      "Testing: 0.6600350076055166 Training: 0.6415846742908653 Seed: 14635\n",
      "Testing: 0.6682970523592484 Training: 0.6394183301197761 Seed: 14636\n",
      "Testing: 0.6518541328720108 Training: 0.6436852421313883 Seed: 14637\n",
      "Testing: 0.6486016556469143 Training: 0.6444834314140863 Seed: 14638\n",
      "Testing: 0.6489597627253577 Training: 0.6444095787850366 Seed: 14640\n",
      "Testing: 0.6573309712775162 Training: 0.6424087094063922 Seed: 14641\n",
      "Testing: 0.6507348929564395 Training: 0.6440543482947885 Seed: 14644\n",
      "Testing: 0.6578758673980611 Training: 0.6421940089103229 Seed: 14647\n",
      "Testing: 0.6486436140580963 Training: 0.64444266158331 Seed: 14650\n",
      "Testing: 0.6565446883867947 Training: 0.6423927758351036 Seed: 14653\n",
      "Testing: 0.6533665913457821 Training: 0.6433729957638323 Seed: 14654\n",
      "Testing: 0.6603459378761231 Training: 0.6416567643236403 Seed: 14655\n",
      "Testing: 0.6515524550500349 Training: 0.6438119375213481 Seed: 14656\n",
      "Testing: 0.6479639406153648 Training: 0.6444457329561474 Seed: 14657\n",
      "Testing: 0.6458459660902429 Training: 0.6452694420132509 Seed: 14659\n",
      "Testing: 0.6462216529789209 Training: 0.6449668721427964 Seed: 14660\n",
      "Testing: 0.655128184731109 Training: 0.6427296222735279 Seed: 14663\n",
      "Testing: 0.6565427889272134 Training: 0.6426196296716424 Seed: 14666\n",
      "Testing: 0.6609199561523706 Training: 0.6414474670377273 Seed: 14669\n",
      "Testing: 0.6487360944181406 Training: 0.6444863368294942 Seed: 14673\n",
      "Testing: 0.6572142967629958 Training: 0.6415204341078431 Seed: 14675\n",
      "Testing: 0.6505479096553902 Training: 0.6439154318177485 Seed: 14676\n",
      "Testing: 0.6553651578997728 Training: 0.643010151536566 Seed: 14677\n",
      "Testing: 0.647322020590945 Training: 0.6448662503201823 Seed: 14678\n",
      "Testing: 0.6571567280106905 Training: 0.6423669866125598 Seed: 14679\n",
      "Testing: 0.6496332615478123 Training: 0.6442293350409687 Seed: 14681\n",
      "Testing: 0.6457810886959975 Training: 0.6452235544595071 Seed: 14682\n",
      "Testing: 0.6548357411715873 Training: 0.6430064104505335 Seed: 14683\n",
      "Testing: 0.6509132087597228 Training: 0.6439767521820166 Seed: 14685\n",
      "Testing: 0.671720779122452 Training: 0.6382989580816842 Seed: 14686\n",
      "Testing: 0.6477969482371595 Training: 0.6447944246125504 Seed: 14687\n",
      "Testing: 0.650116645765589 Training: 0.6442344706509197 Seed: 14689\n",
      "Testing: 0.6546034246928817 Training: 0.6430942337133123 Seed: 14691\n",
      "Testing: 0.6565277828849858 Training: 0.6424415873034366 Seed: 14692\n",
      "Testing: 0.6580836008651203 Training: 0.6421393056735 Seed: 14693\n",
      "Testing: 0.6500292411560321 Training: 0.6441907099894887 Seed: 14698\n",
      "Testing: 0.6465785966012019 Training: 0.6451161521048512 Seed: 14699\n",
      "Testing: 0.6470803745608155 Training: 0.6450162593269877 Seed: 14706\n",
      "Testing: 0.6501911172250712 Training: 0.6441914251887313 Seed: 14707\n",
      "Testing: 0.648641484121153 Training: 0.6445423333767013 Seed: 14708\n",
      "Testing: 0.6551433477883524 Training: 0.6430342447965212 Seed: 14710\n",
      "Testing: 0.6505314850488535 Training: 0.6440374324680197 Seed: 14715\n",
      "Testing: 0.6526055251920826 Training: 0.6434733421428724 Seed: 14716\n",
      "Testing: 0.6535403774655897 Training: 0.643426894402814 Seed: 14717\n",
      "Testing: 0.6553768079856931 Training: 0.6428592145950777 Seed: 14718\n",
      "Testing: 0.645880857417803 Training: 0.6452350641440872 Seed: 14719\n",
      "Testing: 0.6550680623357344 Training: 0.6421770570158568 Seed: 14720\n",
      "Testing: 0.6474351164887897 Training: 0.6443928336251158 Seed: 14724\n",
      "Testing: 0.646201974342399 Training: 0.6452505758850173 Seed: 14728\n",
      "Testing: 0.6469183728442485 Training: 0.6449908407594408 Seed: 14729\n",
      "Testing: 0.6629360860207997 Training: 0.6408920865304404 Seed: 14730\n",
      "Testing: 0.6599246710059337 Training: 0.6417887755819134 Seed: 14731\n",
      "Testing: 0.657172864895021 Training: 0.6424084524611716 Seed: 14735\n",
      "Testing: 0.6559845003688839 Training: 0.6428540945469337 Seed: 14738\n",
      "Testing: 0.6567284962072065 Training: 0.6423680640638892 Seed: 14740\n",
      "Testing: 0.6458546542585144 Training: 0.645226839865826 Seed: 14741\n",
      "Testing: 0.6533293002122376 Training: 0.6434537807851322 Seed: 14744\n",
      "Testing: 0.6507777370253299 Training: 0.6440517165704865 Seed: 14745\n",
      "Testing: 0.6518477409062651 Training: 0.6435970399097812 Seed: 14750\n",
      "Testing: 0.6558675129001588 Training: 0.6427283448728893 Seed: 14752\n",
      "Testing: 0.6550164740302807 Training: 0.642916617328472 Seed: 14754\n",
      "Testing: 0.6564968595718026 Training: 0.6426432820769259 Seed: 14758\n",
      "Testing: 0.6473751965171757 Training: 0.6446201779049748 Seed: 14760\n",
      "Testing: 0.6689580387093395 Training: 0.6392464408828462 Seed: 14763\n",
      "Testing: 0.6482687637876241 Training: 0.6447069030696957 Seed: 14767\n",
      "Testing: 0.651247826785198 Training: 0.6431837858210294 Seed: 14771\n",
      "Testing: 0.6494339587542177 Training: 0.6442571039656365 Seed: 14774\n",
      "Testing: 0.6462743136462592 Training: 0.6451043793802536 Seed: 14775\n",
      "Testing: 0.6572028532383454 Training: 0.6424134107970121 Seed: 14776\n",
      "Testing: 0.6487824470629996 Training: 0.6445319446620215 Seed: 14777\n",
      "Testing: 0.6518750616227184 Training: 0.6437236653225197 Seed: 14778\n",
      "Testing: 0.667596137080017 Training: 0.6397085285618849 Seed: 14779\n",
      "Testing: 0.6525073977794957 Training: 0.6436228161713389 Seed: 14780\n",
      "Testing: 0.6559776326939162 Training: 0.6426472104982313 Seed: 14781\n",
      "Testing: 0.6555038919419188 Training: 0.6427682362202308 Seed: 14782\n",
      "Testing: 0.6565394507224569 Training: 0.6426569837577438 Seed: 14784\n",
      "Testing: 0.6554806463479795 Training: 0.6426705255981355 Seed: 14791\n",
      "Testing: 0.6468830778042348 Training: 0.6450680683257988 Seed: 14793\n",
      "Testing: 0.6567588971983382 Training: 0.6425886520277763 Seed: 14798\n",
      "Testing: 0.6524680572540159 Training: 0.6436096613260351 Seed: 14799\n",
      "Testing: 0.6631997135688578 Training: 0.6408287583915734 Seed: 14806\n",
      "Testing: 0.6617412200508651 Training: 0.6413375950798964 Seed: 14808\n",
      "Testing: 0.6460936098996815 Training: 0.6452351546581685 Seed: 14811\n",
      "Testing: 0.6552450099733017 Training: 0.6428151185089115 Seed: 14814\n",
      "Testing: 0.6569676221718559 Training: 0.6423774013531117 Seed: 14815\n",
      "Testing: 0.6517548575262985 Training: 0.6437862782346725 Seed: 14816\n",
      "Testing: 0.6472000586862142 Training: 0.6448418551559995 Seed: 14817\n",
      "Testing: 0.6551944229053558 Training: 0.6430333800058954 Seed: 14819\n",
      "Testing: 0.6594221033244624 Training: 0.6414267499501458 Seed: 14820\n",
      "Testing: 0.6472221719167209 Training: 0.644919055465778 Seed: 14825\n",
      "Testing: 0.6512006139089668 Training: 0.6438450390016659 Seed: 14828\n",
      "Testing: 0.6551219976274614 Training: 0.642863489117444 Seed: 14835\n",
      "Testing: 0.6562677911495102 Training: 0.6425673170983258 Seed: 14837\n",
      "Testing: 0.66045599659003 Training: 0.6415175337183983 Seed: 14839\n",
      "Testing: 0.6511476724508062 Training: 0.6439211589322539 Seed: 14842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6592333199979419 Training: 0.6417852661902139 Seed: 14846\n",
      "Testing: 0.6463904679447017 Training: 0.6450770939584232 Seed: 14847\n",
      "Testing: 0.6516712860681613 Training: 0.6438955973691762 Seed: 14851\n",
      "Testing: 0.6527464245221721 Training: 0.643491465403131 Seed: 14853\n",
      "Testing: 0.6507338818400914 Training: 0.643990329915793 Seed: 14854\n",
      "Testing: 0.6463586394254092 Training: 0.6450542168773805 Seed: 14855\n",
      "Testing: 0.6460272847869295 Training: 0.6452156669014112 Seed: 14856\n",
      "Testing: 0.6485246315047299 Training: 0.6444686518181704 Seed: 14858\n",
      "Testing: 0.6469539399975431 Training: 0.6449711290644553 Seed: 14859\n",
      "Testing: 0.6499607728207155 Training: 0.6442313388461736 Seed: 14863\n",
      "Testing: 0.6556400839501171 Training: 0.642824957606698 Seed: 14865\n",
      "Testing: 0.6622254964428161 Training: 0.6410449975031607 Seed: 14867\n",
      "Testing: 0.6648962656703634 Training: 0.6403067071046893 Seed: 14870\n",
      "Testing: 0.6539418412319639 Training: 0.6431639419381122 Seed: 14871\n",
      "Testing: 0.6548332658600751 Training: 0.6428927399478966 Seed: 14873\n",
      "Testing: 0.6576837846026873 Training: 0.6420381540981961 Seed: 14874\n",
      "Testing: 0.659880129678938 Training: 0.6414848691964726 Seed: 14876\n",
      "Testing: 0.654820128975951 Training: 0.6429931749856458 Seed: 14878\n",
      "Testing: 0.6545003935593193 Training: 0.6430964413949288 Seed: 14879\n",
      "Testing: 0.656934596981501 Training: 0.6425344908167717 Seed: 14880\n",
      "Testing: 0.6559765301620859 Training: 0.6425684189748145 Seed: 14881\n",
      "Testing: 0.6626555111705819 Training: 0.6412411178435186 Seed: 14882\n",
      "Testing: 0.6541421815899618 Training: 0.6432501922293672 Seed: 14883\n",
      "Testing: 0.649996551299668 Training: 0.6442304911677001 Seed: 14885\n",
      "Testing: 0.6535636234045582 Training: 0.6433974866872644 Seed: 14887\n",
      "Testing: 0.6495177889474586 Training: 0.6443344643892797 Seed: 14888\n",
      "Testing: 0.646988338556119 Training: 0.6449753873283317 Seed: 14889\n",
      "Testing: 0.6457969193558265 Training: 0.6453355079355052 Seed: 14891\n",
      "Testing: 0.6598913319902904 Training: 0.6415033581654697 Seed: 14892\n",
      "Testing: 0.6474321615538454 Training: 0.6447857732070936 Seed: 14894\n",
      "Testing: 0.6574851902422008 Training: 0.6422783970714903 Seed: 14895\n",
      "Testing: 0.6516856262387615 Training: 0.6430516749966639 Seed: 14896\n",
      "Testing: 0.646729399222566 Training: 0.6449376246130007 Seed: 14901\n",
      "Testing: 0.659867216139027 Training: 0.6417793652905112 Seed: 14902\n",
      "Testing: 0.6548413396726247 Training: 0.6430215160247861 Seed: 14904\n",
      "Testing: 0.6516284662396781 Training: 0.6437981972107429 Seed: 14905\n",
      "Testing: 0.6466323322402836 Training: 0.6450926502443777 Seed: 14906\n",
      "Testing: 0.6464807015274209 Training: 0.6450794055433098 Seed: 14908\n",
      "Testing: 0.6456090185334296 Training: 0.6453308993402819 Seed: 14910\n",
      "Testing: 0.6531632344377749 Training: 0.6434376463368681 Seed: 14912\n",
      "Testing: 0.655714228602874 Training: 0.6425425190687728 Seed: 14914\n",
      "Testing: 0.6548654645232886 Training: 0.6429722984654995 Seed: 14916\n",
      "Testing: 0.6512022378945963 Training: 0.6438710431404046 Seed: 14917\n",
      "Testing: 0.6539574113807141 Training: 0.6433696403486836 Seed: 14918\n",
      "Testing: 0.6503540538698739 Training: 0.6441801367464803 Seed: 14920\n",
      "Testing: 0.6612726686521235 Training: 0.6414159131644421 Seed: 14925\n",
      "Testing: 0.6519611135790161 Training: 0.6435793985860341 Seed: 14929\n",
      "Testing: 0.655956072944557 Training: 0.6427081371631496 Seed: 14931\n",
      "Testing: 0.6524210877317496 Training: 0.6436183977827957 Seed: 14932\n",
      "Testing: 0.6534639810099789 Training: 0.6434188839812841 Seed: 14933\n",
      "Testing: 0.6535349272967877 Training: 0.6433211730164998 Seed: 14936\n",
      "Testing: 0.6533067266626974 Training: 0.6434426950332792 Seed: 14937\n",
      "Testing: 0.6458880149184526 Training: 0.645223860822947 Seed: 14942\n",
      "Testing: 0.6495639732950169 Training: 0.644253041556595 Seed: 14946\n",
      "Testing: 0.6482236745596723 Training: 0.6446893129575251 Seed: 14949\n",
      "Testing: 0.658971826105167 Training: 0.6417580407585468 Seed: 14953\n",
      "Testing: 0.6646983908689186 Training: 0.6404181444220989 Seed: 14956\n",
      "Testing: 0.6658183964537415 Training: 0.6402025635491639 Seed: 14957\n",
      "Testing: 0.654419327811651 Training: 0.6428976522725812 Seed: 14958\n",
      "Testing: 0.6548879053904184 Training: 0.6428912387767918 Seed: 14961\n",
      "Testing: 0.6510716133790598 Training: 0.6437721775582356 Seed: 14962\n",
      "Testing: 0.6529721804300586 Training: 0.6435286308420536 Seed: 14965\n",
      "Testing: 0.6517420171202567 Training: 0.6437896962232583 Seed: 14969\n",
      "Testing: 0.6560694711432931 Training: 0.6425260100244198 Seed: 14970\n",
      "Testing: 0.6486048850587007 Training: 0.6444932453949144 Seed: 14977\n",
      "Testing: 0.6592544726328787 Training: 0.6419202219314365 Seed: 14978\n",
      "Testing: 0.6473349648252006 Training: 0.6449338603563367 Seed: 14979\n",
      "Testing: 0.6543674372657613 Training: 0.6431671751233952 Seed: 14983\n",
      "Testing: 0.6571367948076585 Training: 0.6424046705470325 Seed: 14985\n",
      "Testing: 0.6526826348017413 Training: 0.6434719003515276 Seed: 14986\n",
      "Testing: 0.6498823000462409 Training: 0.644055519932403 Seed: 14987\n",
      "Testing: 0.6625884694225233 Training: 0.6409104959017553 Seed: 14988\n",
      "Testing: 0.6552466865830471 Training: 0.6428479443763535 Seed: 14990\n",
      "Testing: 0.6514786002301401 Training: 0.6437327928995937 Seed: 14993\n",
      "Testing: 0.6523176103111759 Training: 0.6436611747508049 Seed: 14994\n",
      "Testing: 0.6475300776326905 Training: 0.6448479697113282 Seed: 14997\n",
      "Testing: 0.6573207435554614 Training: 0.6422684611939753 Seed: 15000\n",
      "Testing: 0.6503807304382525 Training: 0.6440253781985067 Seed: 15001\n",
      "Testing: 0.6500402000309323 Training: 0.6440850558703485 Seed: 15004\n",
      "Testing: 0.6454959381902112 Training: 0.6453645697796963 Seed: 15005\n",
      "Testing: 0.6458500140293922 Training: 0.6452459992012501 Seed: 15006\n",
      "Testing: 0.6507761535097365 Training: 0.6440421871827401 Seed: 15008\n",
      "Testing: 0.6464295979061558 Training: 0.6450127373949373 Seed: 15009\n",
      "Testing: 0.6489019649955251 Training: 0.6445019864873978 Seed: 15010\n",
      "Testing: 0.6581608189522279 Training: 0.6419884103289089 Seed: 15013\n",
      "Testing: 0.6530435697215873 Training: 0.6434801791233679 Seed: 15017\n",
      "Testing: 0.6523807107727797 Training: 0.6436484830624458 Seed: 15021\n",
      "Testing: 0.6527123432835242 Training: 0.6434799973669507 Seed: 15023\n",
      "Testing: 0.6513056909173705 Training: 0.6434498859971685 Seed: 15026\n",
      "Testing: 0.646768816356734 Training: 0.6450037062062298 Seed: 15028\n",
      "Testing: 0.6640946478061915 Training: 0.6404088368688361 Seed: 15030\n",
      "Testing: 0.6677324014172473 Training: 0.6393963148627175 Seed: 15032\n",
      "Testing: 0.6584596212682597 Training: 0.6420073757628217 Seed: 15034\n",
      "Testing: 0.645621604840946 Training: 0.6453282014042669 Seed: 15037\n",
      "Testing: 0.657149454597934 Training: 0.6418259295714448 Seed: 15038\n",
      "Testing: 0.6451788093395876 Training: 0.6450246411231314 Seed: 15041\n",
      "Testing: 0.6493733573950955 Training: 0.6440429346738853 Seed: 15043\n",
      "Testing: 0.649331265311168 Training: 0.6443631050759173 Seed: 15046\n",
      "Testing: 0.6696247331294674 Training: 0.6391810299901433 Seed: 15047\n",
      "Testing: 0.6486189096552573 Training: 0.6445713117545002 Seed: 15051\n",
      "Testing: 0.6604167126874734 Training: 0.6416635530615034 Seed: 15052\n",
      "Testing: 0.6493387186389175 Training: 0.6442578818714757 Seed: 15053\n",
      "Testing: 0.6476535284947702 Training: 0.6447192527204721 Seed: 15055\n",
      "Testing: 0.6478433732918256 Training: 0.6448243845309696 Seed: 15058\n",
      "Testing: 0.6495917692673885 Training: 0.6443726331333652 Seed: 15059\n",
      "Testing: 0.6563685425431091 Training: 0.6425245235811424 Seed: 15060\n",
      "Testing: 0.6568740436624364 Training: 0.6426076337831923 Seed: 15061\n",
      "Testing: 0.6541752077804994 Training: 0.6426899084368 Seed: 15062\n",
      "Testing: 0.6473967680969079 Training: 0.644639043096549 Seed: 15063\n",
      "Testing: 0.6464952640473225 Training: 0.6448326729116314 Seed: 15069\n",
      "Testing: 0.6517183751518533 Training: 0.6437833416185947 Seed: 15073\n",
      "Testing: 0.6609030812369474 Training: 0.6414753616326344 Seed: 15074\n",
      "Testing: 0.6491621102102176 Training: 0.644271126984029 Seed: 15075\n",
      "Testing: 0.666140473288304 Training: 0.6399767730966381 Seed: 15078\n",
      "Testing: 0.6689879036886542 Training: 0.6391680299739668 Seed: 15079\n",
      "Testing: 0.6588813444686425 Training: 0.6417965655222491 Seed: 15080\n",
      "Testing: 0.6568375135010505 Training: 0.6423885640201485 Seed: 15086\n",
      "Testing: 0.6603218482611863 Training: 0.6415858555430868 Seed: 15087\n",
      "Testing: 0.6514153920826025 Training: 0.6439210268209976 Seed: 15090\n",
      "Testing: 0.6569586182733577 Training: 0.6422739845273662 Seed: 15091\n",
      "Testing: 0.6542017350646245 Training: 0.6431562197529573 Seed: 15092\n",
      "Testing: 0.6579678285146938 Training: 0.6422598859212305 Seed: 15093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6467942879340595 Training: 0.6447518598846104 Seed: 15096\n",
      "Testing: 0.6527828555572797 Training: 0.6434496366594427 Seed: 15097\n",
      "Testing: 0.6519590009489807 Training: 0.643720818935154 Seed: 15098\n",
      "Testing: 0.6457388005652301 Training: 0.6452945491690996 Seed: 15099\n",
      "Testing: 0.6467408147277122 Training: 0.6448817350464944 Seed: 15105\n",
      "Testing: 0.6458352168522757 Training: 0.6444660598783615 Seed: 15109\n",
      "Testing: 0.6530916265246656 Training: 0.6433793940008916 Seed: 15110\n",
      "Testing: 0.6471206714706599 Training: 0.644873788529631 Seed: 15111\n",
      "Testing: 0.663933148656716 Training: 0.640771888787081 Seed: 15113\n",
      "Testing: 0.6498515080352735 Training: 0.6442356432269704 Seed: 15116\n",
      "Testing: 0.6518833354107187 Training: 0.6438597691877587 Seed: 15117\n",
      "Testing: 0.6455406595187827 Training: 0.6453276505154613 Seed: 15118\n",
      "Testing: 0.6550467940822657 Training: 0.6427677776916769 Seed: 15119\n",
      "Testing: 0.6600543526466955 Training: 0.6418089093793229 Seed: 15122\n",
      "Testing: 0.6474211163367364 Training: 0.6449407864213859 Seed: 15124\n",
      "Testing: 0.6570778942468161 Training: 0.6424011161840772 Seed: 15126\n",
      "Testing: 0.6473202243843303 Training: 0.644931418319143 Seed: 15128\n",
      "Testing: 0.6525454960447211 Training: 0.6435021078387709 Seed: 15129\n",
      "Testing: 0.6521054396204002 Training: 0.6435454151794837 Seed: 15130\n",
      "Testing: 0.6488211731098714 Training: 0.643533783972104 Seed: 15131\n",
      "Testing: 0.6458258600499356 Training: 0.6453037923445624 Seed: 15132\n",
      "Testing: 0.6508336997696892 Training: 0.644037465445868 Seed: 15133\n",
      "Testing: 0.645408413203045 Training: 0.645378684746255 Seed: 15135\n",
      "Testing: 0.6455473024811418 Training: 0.6453413378943659 Seed: 15139\n",
      "Testing: 0.6520359316955233 Training: 0.6436869561046699 Seed: 15143\n",
      "Testing: 0.6456582835639094 Training: 0.6453501888826678 Seed: 15144\n",
      "Testing: 0.656442728620724 Training: 0.6426562178734271 Seed: 15145\n",
      "Testing: 0.6484164707120457 Training: 0.6445071469736439 Seed: 15146\n",
      "Testing: 0.6534962188666373 Training: 0.6433314623858793 Seed: 15147\n",
      "Testing: 0.64602098359625 Training: 0.6450122504839073 Seed: 15149\n",
      "Testing: 0.6467873277080607 Training: 0.6450941333421711 Seed: 15152\n",
      "Testing: 0.6598809171594164 Training: 0.6416755772802523 Seed: 15155\n",
      "Testing: 0.6455313639785741 Training: 0.6453139144403832 Seed: 15158\n",
      "Testing: 0.650780981726143 Training: 0.6439355683138883 Seed: 15159\n",
      "Testing: 0.6486950430326204 Training: 0.6445542825869373 Seed: 15162\n",
      "Testing: 0.6578119920110015 Training: 0.6420881293603913 Seed: 15164\n",
      "Testing: 0.6465913168883584 Training: 0.6451727753134217 Seed: 15165\n",
      "Testing: 0.6529011802063601 Training: 0.6434912297818315 Seed: 15167\n",
      "Testing: 0.6543085382322991 Training: 0.643177795681421 Seed: 15168\n",
      "Testing: 0.6474142534018972 Training: 0.6448556380220403 Seed: 15172\n",
      "Testing: 0.6455120154152099 Training: 0.6453186264350903 Seed: 15176\n",
      "Testing: 0.6594718976270335 Training: 0.6418787064517599 Seed: 15177\n",
      "Testing: 0.6575814963151996 Training: 0.6420463318298935 Seed: 15178\n",
      "Testing: 0.6534125424141068 Training: 0.6433822080943542 Seed: 15179\n",
      "Testing: 0.647572696630157 Training: 0.6444054570028386 Seed: 15181\n",
      "Testing: 0.657787112193747 Training: 0.642352769720403 Seed: 15182\n",
      "Testing: 0.6553955220960985 Training: 0.6429166753498337 Seed: 15183\n",
      "Testing: 0.6477721207422941 Training: 0.6448240096370493 Seed: 15184\n",
      "Testing: 0.6593277322455295 Training: 0.6419882909268111 Seed: 15185\n",
      "Testing: 0.6511210514730283 Training: 0.64390652712499 Seed: 15187\n",
      "Testing: 0.6609912841389496 Training: 0.6412562562677011 Seed: 15189\n",
      "Testing: 0.6569175157130167 Training: 0.6423332193148671 Seed: 15193\n",
      "Testing: 0.6565579899921326 Training: 0.642558178695176 Seed: 15202\n",
      "Testing: 0.6482876134609215 Training: 0.6446761819027522 Seed: 15203\n",
      "Testing: 0.6474397605974991 Training: 0.6449450541597546 Seed: 15207\n",
      "Testing: 0.6572662566799562 Training: 0.6422223816696224 Seed: 15208\n",
      "Testing: 0.6460447434166214 Training: 0.645039754714646 Seed: 15209\n",
      "Testing: 0.6599321348451682 Training: 0.6416307052444747 Seed: 15212\n",
      "Testing: 0.6507140325771585 Training: 0.6441272082433953 Seed: 15216\n",
      "Testing: 0.6567833072460039 Training: 0.6425962372058739 Seed: 15217\n",
      "Testing: 0.6627706043601553 Training: 0.6411237256783842 Seed: 15220\n",
      "Testing: 0.6508021382024771 Training: 0.6440153756246021 Seed: 15222\n",
      "Testing: 0.6648081234068677 Training: 0.640339591508883 Seed: 15224\n",
      "Testing: 0.6603903850867451 Training: 0.6416744219216675 Seed: 15225\n",
      "Testing: 0.6567864324538119 Training: 0.6425580511664213 Seed: 15229\n",
      "Testing: 0.6476292757732879 Training: 0.6448536749810728 Seed: 15230\n",
      "Testing: 0.6464106414202725 Training: 0.6451475254498794 Seed: 15231\n",
      "Testing: 0.661566020014573 Training: 0.641298706434325 Seed: 15232\n",
      "Testing: 0.645805559010352 Training: 0.6453222734648548 Seed: 15233\n",
      "Testing: 0.6482771251367773 Training: 0.644491910272277 Seed: 15235\n",
      "Testing: 0.6482873423132808 Training: 0.6446904870118407 Seed: 15236\n",
      "Testing: 0.6575151295971022 Training: 0.6424457872391367 Seed: 15244\n",
      "Testing: 0.6518803909307024 Training: 0.6436913968464721 Seed: 15245\n",
      "Testing: 0.646126404752111 Training: 0.6451679196986536 Seed: 15246\n",
      "Testing: 0.6498771257494557 Training: 0.6441811623124978 Seed: 15248\n",
      "Testing: 0.6519723662355305 Training: 0.6437718564738903 Seed: 15249\n",
      "Testing: 0.6575126198483945 Training: 0.6420586930430644 Seed: 15250\n",
      "Testing: 0.6484410282593304 Training: 0.644561803340069 Seed: 15255\n",
      "Testing: 0.6497574535173032 Training: 0.6442085444283455 Seed: 15256\n",
      "Testing: 0.6476181201803247 Training: 0.6449058576100113 Seed: 15258\n",
      "Testing: 0.6589701790988931 Training: 0.6418907714464568 Seed: 15259\n",
      "Testing: 0.6522282292387261 Training: 0.643648335436799 Seed: 15260\n",
      "Testing: 0.6501758688624029 Training: 0.6441751487116376 Seed: 15261\n",
      "Testing: 0.6492346603640656 Training: 0.644417840562293 Seed: 15264\n",
      "Testing: 0.6524172492032482 Training: 0.6436242703829476 Seed: 15269\n",
      "Testing: 0.6524740751382345 Training: 0.6434962351391417 Seed: 15270\n",
      "Testing: 0.6615524552095403 Training: 0.6413864904820586 Seed: 15271\n",
      "Testing: 0.6635845684991327 Training: 0.6407977012398687 Seed: 15272\n",
      "Testing: 0.6504671859957993 Training: 0.6439211137875256 Seed: 15273\n",
      "Testing: 0.6469816744635305 Training: 0.6447116723892884 Seed: 15277\n",
      "Testing: 0.6478464045755209 Training: 0.6448320827938188 Seed: 15278\n",
      "Testing: 0.6536471770096075 Training: 0.6433173913273529 Seed: 15280\n",
      "Testing: 0.6566627536629834 Training: 0.6425007220014332 Seed: 15282\n",
      "Testing: 0.6662355654503362 Training: 0.639970146634731 Seed: 15283\n",
      "Testing: 0.6617504902886779 Training: 0.6412473044449318 Seed: 15284\n",
      "Testing: 0.6593542370124171 Training: 0.6414694621569798 Seed: 15289\n",
      "Testing: 0.6498802236510818 Training: 0.6442494464865095 Seed: 15290\n",
      "Testing: 0.6467670647488272 Training: 0.6448729292952449 Seed: 15292\n",
      "Testing: 0.6455057648498475 Training: 0.6453803477150648 Seed: 15295\n",
      "Testing: 0.6583816087055973 Training: 0.6420845924848542 Seed: 15296\n",
      "Testing: 0.649366439626295 Training: 0.644247289819597 Seed: 15299\n",
      "Testing: 0.6524694197809555 Training: 0.6436251835144357 Seed: 15304\n",
      "Testing: 0.6499922549526095 Training: 0.6441799162552355 Seed: 15306\n",
      "Testing: 0.6476970726475312 Training: 0.6447645746495677 Seed: 15308\n",
      "Testing: 0.6581995751980179 Training: 0.6423123878807645 Seed: 15309\n",
      "Testing: 0.6615875852025199 Training: 0.6411584144490934 Seed: 15311\n",
      "Testing: 0.6468990135676669 Training: 0.6448833395473206 Seed: 15314\n",
      "Testing: 0.6572654429363713 Training: 0.6423948783806789 Seed: 15317\n",
      "Testing: 0.652577048201556 Training: 0.6436322525408359 Seed: 15318\n",
      "Testing: 0.6617616788481362 Training: 0.6411247972686048 Seed: 15319\n",
      "Testing: 0.6474283021740782 Training: 0.6448339445463414 Seed: 15321\n",
      "Testing: 0.6528142038900723 Training: 0.6435329698832662 Seed: 15322\n",
      "Testing: 0.650050644092887 Training: 0.6443027349232797 Seed: 15324\n",
      "Testing: 0.6510213966972642 Training: 0.6439910959174613 Seed: 15326\n",
      "Testing: 0.6541622328139226 Training: 0.6432365612326112 Seed: 15327\n",
      "Testing: 0.6522385642634037 Training: 0.6437273621649904 Seed: 15329\n",
      "Testing: 0.6562584334931559 Training: 0.6424304525844635 Seed: 15335\n",
      "Testing: 0.6455003012287331 Training: 0.6453418321344438 Seed: 15337\n",
      "Testing: 0.6570073311504486 Training: 0.6423761044285274 Seed: 15338\n",
      "Testing: 0.6701384199147692 Training: 0.6388354636731446 Seed: 15340\n",
      "Testing: 0.6490307816705062 Training: 0.6444924182830472 Seed: 15342\n",
      "Testing: 0.6523874983374142 Training: 0.6435223165207372 Seed: 15344\n",
      "Testing: 0.6529398945333093 Training: 0.6434188325437151 Seed: 15347\n",
      "Testing: 0.6527171772129319 Training: 0.6433571146622743 Seed: 15348\n",
      "Testing: 0.6515054146425794 Training: 0.6438304754146815 Seed: 15352\n",
      "Testing: 0.6501795177659146 Training: 0.644104729342775 Seed: 15353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6520530399930712 Training: 0.6436507875578172 Seed: 15356\n",
      "Testing: 0.6661959724026125 Training: 0.6401960623970389 Seed: 15359\n",
      "Testing: 0.6588212971919674 Training: 0.6420136750873927 Seed: 15364\n",
      "Testing: 0.6465258582766951 Training: 0.6450090576334324 Seed: 15365\n",
      "Testing: 0.6496228052932533 Training: 0.64418733193606 Seed: 15366\n",
      "Testing: 0.6478477755554819 Training: 0.6447556328485204 Seed: 15368\n",
      "Testing: 0.6454166644653136 Training: 0.6453694574584337 Seed: 15370\n",
      "Testing: 0.6461953773672515 Training: 0.6451230643308257 Seed: 15372\n",
      "Testing: 0.6486144177832147 Training: 0.6445359214646668 Seed: 15374\n",
      "Testing: 0.6456382215407842 Training: 0.6453882798982099 Seed: 15375\n",
      "Testing: 0.6538104542526639 Training: 0.6433569620599258 Seed: 15379\n",
      "Testing: 0.6511594608195946 Training: 0.6439794819852095 Seed: 15381\n",
      "Testing: 0.6471797927702647 Training: 0.644897336245344 Seed: 15382\n",
      "Testing: 0.6488930453459918 Training: 0.6445200358550597 Seed: 15387\n",
      "Testing: 0.6567722881048166 Training: 0.6425973216065387 Seed: 15389\n",
      "Testing: 0.6512051414027669 Training: 0.6437879643147306 Seed: 15390\n",
      "Testing: 0.6466826726885541 Training: 0.644962144620476 Seed: 15392\n",
      "Testing: 0.6485992440945474 Training: 0.6444823690494611 Seed: 15393\n",
      "Testing: 0.6639974727646729 Training: 0.6407633324498967 Seed: 15397\n",
      "Testing: 0.6576203781990592 Training: 0.6423199944889242 Seed: 15405\n",
      "Testing: 0.647617006368421 Training: 0.6448557323762932 Seed: 15407\n",
      "Testing: 0.6538989304481132 Training: 0.6433030378181668 Seed: 15409\n",
      "Testing: 0.6594368582094365 Training: 0.6419068448360632 Seed: 15411\n",
      "Testing: 0.6476767750625437 Training: 0.6447723536999405 Seed: 15412\n",
      "Testing: 0.6586480539198084 Training: 0.642025084594408 Seed: 15415\n",
      "Testing: 0.6519088978072345 Training: 0.6433600878464225 Seed: 15419\n",
      "Testing: 0.6479505975296491 Training: 0.6446342128012708 Seed: 15422\n",
      "Testing: 0.6597477687287465 Training: 0.64175196275848 Seed: 15423\n",
      "Testing: 0.6557331922501954 Training: 0.6428301093432836 Seed: 15424\n",
      "Testing: 0.6469286631098135 Training: 0.6449464853563468 Seed: 15425\n",
      "Testing: 0.6666293223110509 Training: 0.6400419228048712 Seed: 15426\n",
      "Testing: 0.6479441768954803 Training: 0.6447070687509326 Seed: 15428\n",
      "Testing: 0.671951160805526 Training: 0.6386776310911382 Seed: 15429\n",
      "Testing: 0.6584731298966017 Training: 0.6421057371172569 Seed: 15430\n",
      "Testing: 0.6457626262317439 Training: 0.6452701482023003 Seed: 15433\n",
      "Testing: 0.6562586372041253 Training: 0.6426407660413962 Seed: 15434\n",
      "Testing: 0.6510904567956646 Training: 0.6437642280222484 Seed: 15437\n",
      "Testing: 0.6527800391381005 Training: 0.6434484125436606 Seed: 15439\n",
      "Testing: 0.6577636529034074 Training: 0.6418399913117726 Seed: 15441\n",
      "Testing: 0.6548573668620505 Training: 0.6428284712357746 Seed: 15445\n",
      "Testing: 0.650611811961133 Training: 0.6441158643929132 Seed: 15447\n",
      "Testing: 0.6507401290540313 Training: 0.6440777100242991 Seed: 15448\n",
      "Testing: 0.6471378259431138 Training: 0.6449988706706636 Seed: 15449\n",
      "Testing: 0.6532729686897343 Training: 0.6433530793134898 Seed: 15451\n",
      "Testing: 0.6671012563889213 Training: 0.6400506677886751 Seed: 15452\n",
      "Testing: 0.6573358252370228 Training: 0.642315579615887 Seed: 15455\n",
      "Testing: 0.6576583072768812 Training: 0.6422361537393594 Seed: 15456\n",
      "Testing: 0.6554819002460384 Training: 0.6427707967816003 Seed: 15457\n",
      "Testing: 0.6488534844126175 Training: 0.6444374911869079 Seed: 15458\n",
      "Testing: 0.6481755790155082 Training: 0.6447460506042653 Seed: 15464\n",
      "Testing: 0.6661448193472204 Training: 0.6401686772294122 Seed: 15465\n",
      "Testing: 0.6574762446070943 Training: 0.6423956950586168 Seed: 15466\n",
      "Testing: 0.6487613715590563 Training: 0.6445250613085782 Seed: 15467\n",
      "Testing: 0.6482197190734894 Training: 0.644672373671879 Seed: 15469\n",
      "Testing: 0.6513972746643544 Training: 0.6439160267028307 Seed: 15470\n",
      "Testing: 0.6608933123906334 Training: 0.6414160885103979 Seed: 15471\n",
      "Testing: 0.6455237352040887 Training: 0.6453924250928813 Seed: 15472\n",
      "Testing: 0.6587964379808542 Training: 0.642098195683758 Seed: 15473\n",
      "Testing: 0.6483509057339274 Training: 0.6445713250679108 Seed: 15474\n",
      "Testing: 0.6543929064675388 Training: 0.6427947895546016 Seed: 15477\n",
      "Testing: 0.6518038933836674 Training: 0.6436671934217647 Seed: 15478\n",
      "Testing: 0.6455739648231112 Training: 0.645337174968141 Seed: 15479\n",
      "Testing: 0.6524316623254105 Training: 0.6435886353591268 Seed: 15480\n",
      "Testing: 0.645625528981825 Training: 0.6451410013607437 Seed: 15481\n",
      "Testing: 0.6639800037716377 Training: 0.6405355029371114 Seed: 15482\n",
      "Testing: 0.649768383184286 Training: 0.6443067450511997 Seed: 15484\n",
      "Testing: 0.6679312428514462 Training: 0.6398760473793117 Seed: 15486\n",
      "Testing: 0.6610340842136794 Training: 0.6412084603330048 Seed: 15488\n",
      "Testing: 0.656397593720812 Training: 0.6426015789878681 Seed: 15489\n",
      "Testing: 0.6594555492647876 Training: 0.6419171312186546 Seed: 15490\n",
      "Testing: 0.6640222649488607 Training: 0.6405001605653837 Seed: 15495\n",
      "Testing: 0.6457071450256224 Training: 0.6453019255652381 Seed: 15498\n",
      "Testing: 0.6548137798378713 Training: 0.6427620095040143 Seed: 15499\n",
      "Testing: 0.6651184412318439 Training: 0.6404003758071612 Seed: 15500\n",
      "Testing: 0.6477276509040675 Training: 0.644592049516284 Seed: 15503\n",
      "Testing: 0.6567503366825 Training: 0.6425904896574266 Seed: 15507\n",
      "Testing: 0.647990399727997 Training: 0.6447392492268451 Seed: 15508\n",
      "Testing: 0.6474032174423875 Training: 0.6448024346795278 Seed: 15511\n",
      "Testing: 0.654987888074734 Training: 0.6429687998702124 Seed: 15514\n",
      "Testing: 0.6588570299546191 Training: 0.6417893719706314 Seed: 15515\n",
      "Testing: 0.6484569018038294 Training: 0.6444751076432618 Seed: 15516\n",
      "Testing: 0.6472564621979491 Training: 0.6449806972385002 Seed: 15517\n",
      "Testing: 0.6547174957652836 Training: 0.6431396820108597 Seed: 15519\n",
      "Testing: 0.6472343743677704 Training: 0.6449705922007504 Seed: 15520\n",
      "Testing: 0.6587491530370239 Training: 0.6421272970286642 Seed: 15522\n",
      "Testing: 0.6498358671330842 Training: 0.6443353028169798 Seed: 15523\n",
      "Testing: 0.6583751205918638 Training: 0.6419971665127416 Seed: 15524\n",
      "Testing: 0.653291038298486 Training: 0.6434126804915529 Seed: 15525\n",
      "Testing: 0.6551018694103371 Training: 0.6423319255561009 Seed: 15527\n",
      "Testing: 0.6589412010969389 Training: 0.6419984090392239 Seed: 15528\n",
      "Testing: 0.6496064983789183 Training: 0.6443195141144408 Seed: 15530\n",
      "Testing: 0.6515078786790063 Training: 0.6438677939025629 Seed: 15537\n",
      "Testing: 0.6498795569295165 Training: 0.6442557890883199 Seed: 15540\n",
      "Testing: 0.6486747681758223 Training: 0.6444705390004941 Seed: 15541\n",
      "Testing: 0.6484388952802095 Training: 0.6445463882773146 Seed: 15542\n",
      "Testing: 0.6609638501189157 Training: 0.6415347728527786 Seed: 15543\n",
      "Testing: 0.6537314475478588 Training: 0.6431649352299863 Seed: 15544\n",
      "Testing: 0.6510062889056455 Training: 0.6439371102928352 Seed: 15548\n",
      "Testing: 0.6560672144257127 Training: 0.6423986956619552 Seed: 15549\n",
      "Testing: 0.6485909551745954 Training: 0.644641204503523 Seed: 15551\n",
      "Testing: 0.6465565239135587 Training: 0.6451172567989086 Seed: 15554\n",
      "Testing: 0.6591268925477884 Training: 0.6417184163760622 Seed: 15557\n",
      "Testing: 0.6491289528765718 Training: 0.6444570155015077 Seed: 15558\n",
      "Testing: 0.648351643331034 Training: 0.6445525102915781 Seed: 15559\n",
      "Testing: 0.6692127386715726 Training: 0.63947933280064 Seed: 15560\n",
      "Testing: 0.6486230649226509 Training: 0.644623387886293 Seed: 15563\n",
      "Testing: 0.645459321167809 Training: 0.645294573108917 Seed: 15564\n",
      "Testing: 0.6549028366567919 Training: 0.6429745692699789 Seed: 15565\n",
      "Testing: 0.6476385886213321 Training: 0.6447655776793393 Seed: 15566\n",
      "Testing: 0.6567447303718136 Training: 0.6425380732029788 Seed: 15567\n",
      "Testing: 0.6588524377345994 Training: 0.6419799712798882 Seed: 15568\n",
      "Testing: 0.6584959249974177 Training: 0.6420720019957754 Seed: 15569\n",
      "Testing: 0.6524811863257534 Training: 0.6435952448821769 Seed: 15571\n",
      "Testing: 0.6484216359964534 Training: 0.6446683886851633 Seed: 15573\n",
      "Testing: 0.6482358532500596 Training: 0.6445391593052232 Seed: 15575\n",
      "Testing: 0.6632886618935518 Training: 0.6409107963871774 Seed: 15577\n",
      "Testing: 0.6533365925815758 Training: 0.6432615779001071 Seed: 15579\n",
      "Testing: 0.6584806950690368 Training: 0.6422951163529778 Seed: 15580\n",
      "Testing: 0.6499754993986779 Training: 0.6442766240234752 Seed: 15583\n",
      "Testing: 0.6540824126486452 Training: 0.6430324578994946 Seed: 15584\n",
      "Testing: 0.6595877502783309 Training: 0.6417438917620337 Seed: 15585\n",
      "Testing: 0.646802979925613 Training: 0.6450476296130334 Seed: 15587\n",
      "Testing: 0.6591793744221153 Training: 0.6418303975709494 Seed: 15588\n",
      "Testing: 0.6484378133562319 Training: 0.6445059421566766 Seed: 15589\n",
      "Testing: 0.6462045125030504 Training: 0.6450760421784878 Seed: 15591\n",
      "Testing: 0.6522570154993281 Training: 0.6436745615417747 Seed: 15592\n",
      "Testing: 0.6543906199426038 Training: 0.6431904989717439 Seed: 15593\n",
      "Testing: 0.6545941103308517 Training: 0.6431277190970112 Seed: 15594\n",
      "Testing: 0.6532113402847001 Training: 0.6434358783128039 Seed: 15598\n",
      "Testing: 0.6551649149278328 Training: 0.6429386474709826 Seed: 15599\n",
      "Testing: 0.6466345761596509 Training: 0.6449259242030811 Seed: 15601\n",
      "Testing: 0.677188122379286 Training: 0.6375093108988563 Seed: 15602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6498566458299575 Training: 0.6442776850696579 Seed: 15603\n",
      "Testing: 0.6521540420715011 Training: 0.6437617434599386 Seed: 15605\n",
      "Testing: 0.6456331454565997 Training: 0.6452089649145571 Seed: 15606\n",
      "Testing: 0.6561594032960552 Training: 0.64264543776999 Seed: 15607\n",
      "Testing: 0.6516343288703599 Training: 0.6434121171518503 Seed: 15609\n",
      "Testing: 0.6636361498124028 Training: 0.6404885017916379 Seed: 15610\n",
      "Testing: 0.6466755670023172 Training: 0.6450702920593326 Seed: 15611\n",
      "Testing: 0.6497783817945986 Training: 0.6443705866849657 Seed: 15612\n",
      "Testing: 0.6571142152459198 Training: 0.6424646681009404 Seed: 15613\n",
      "Testing: 0.6515336463623523 Training: 0.643847383793226 Seed: 15614\n",
      "Testing: 0.6633686108610051 Training: 0.6408875597294033 Seed: 15619\n",
      "Testing: 0.6527237052816299 Training: 0.6436400852157115 Seed: 15623\n",
      "Testing: 0.6505042353528983 Training: 0.6440273540125938 Seed: 15625\n",
      "Testing: 0.6626108437058973 Training: 0.641256607624109 Seed: 15626\n",
      "Testing: 0.6543762675103066 Training: 0.6432259143325492 Seed: 15627\n",
      "Testing: 0.6464523869255812 Training: 0.6451886030447174 Seed: 15629\n",
      "Testing: 0.6499085824176193 Training: 0.6442927765889874 Seed: 15631\n",
      "Testing: 0.6517443896337046 Training: 0.6436895000076395 Seed: 15633\n",
      "Testing: 0.6625686734420323 Training: 0.6409762487177236 Seed: 15638\n",
      "Testing: 0.6478793239509991 Training: 0.6448037906670282 Seed: 15641\n",
      "Testing: 0.650653869064389 Training: 0.6440523154525848 Seed: 15643\n",
      "Testing: 0.6579296792387327 Training: 0.6422211709901159 Seed: 15648\n",
      "Testing: 0.6478429618720384 Training: 0.6447375760271931 Seed: 15649\n",
      "Testing: 0.6677765265688168 Training: 0.6398207277349882 Seed: 15652\n",
      "Testing: 0.6459830004141629 Training: 0.6452481834681121 Seed: 15653\n",
      "Testing: 0.657811615668538 Training: 0.6421355560894113 Seed: 15655\n",
      "Testing: 0.6549448360030453 Training: 0.6430644803312113 Seed: 15657\n",
      "Testing: 0.661793982344904 Training: 0.6411439384403548 Seed: 15659\n",
      "Testing: 0.6546220959435336 Training: 0.6429174814810624 Seed: 15665\n",
      "Testing: 0.6539436418176805 Training: 0.6431475176660182 Seed: 15670\n",
      "Testing: 0.6520392235847321 Training: 0.6436565836210121 Seed: 15672\n",
      "Testing: 0.676567406458853 Training: 0.6373114406410575 Seed: 15674\n",
      "Testing: 0.6459627122190582 Training: 0.6452842471652577 Seed: 15677\n",
      "Testing: 0.6563213641239317 Training: 0.642684150164525 Seed: 15681\n",
      "Testing: 0.6507146057740261 Training: 0.6440279873256669 Seed: 15682\n",
      "Testing: 0.6458479162569587 Training: 0.6452374523463631 Seed: 15683\n",
      "Testing: 0.6635723245890144 Training: 0.6408127610321467 Seed: 15684\n",
      "Testing: 0.6555334578215755 Training: 0.6427323981172254 Seed: 15692\n",
      "Testing: 0.6522173046037777 Training: 0.6436709043985595 Seed: 15693\n",
      "Testing: 0.6481307933868219 Training: 0.6446397983212654 Seed: 15694\n",
      "Testing: 0.650801265384521 Training: 0.6438883421686381 Seed: 15696\n",
      "Testing: 0.6458158704618248 Training: 0.6452998030108368 Seed: 15699\n",
      "Testing: 0.6466565486237019 Training: 0.6448895231753544 Seed: 15700\n",
      "Testing: 0.6654846261183853 Training: 0.6402371566645351 Seed: 15701\n",
      "Testing: 0.6562118914975915 Training: 0.642735991926032 Seed: 15704\n",
      "Testing: 0.6531792360706281 Training: 0.6432617572979131 Seed: 15709\n",
      "Testing: 0.6453412520908907 Training: 0.6452780902134222 Seed: 15711\n",
      "Testing: 0.6499971557648878 Training: 0.6441507471328166 Seed: 15713\n",
      "Testing: 0.6520019758908893 Training: 0.6437472017232653 Seed: 15714\n",
      "Testing: 0.6523976959167571 Training: 0.64361812553789 Seed: 15720\n",
      "Testing: 0.6499713457068036 Training: 0.6442154726347193 Seed: 15721\n",
      "Testing: 0.6456843076769457 Training: 0.645251925584095 Seed: 15723\n",
      "Testing: 0.6666142486975453 Training: 0.6397944622040429 Seed: 15724\n",
      "Testing: 0.6631976231429914 Training: 0.6406032526378251 Seed: 15725\n",
      "Testing: 0.6477942616424891 Training: 0.6447683280548595 Seed: 15726\n",
      "Testing: 0.6610826585356049 Training: 0.641571396451747 Seed: 15727\n",
      "Testing: 0.6518687456633802 Training: 0.6437618736082686 Seed: 15728\n",
      "Testing: 0.6559433896543807 Training: 0.6427920121342854 Seed: 15729\n",
      "Testing: 0.6476043317706704 Training: 0.644878427168601 Seed: 15733\n",
      "Testing: 0.6688307883368558 Training: 0.6393134560021647 Seed: 15734\n",
      "Testing: 0.6458425483120548 Training: 0.6452076859926331 Seed: 15735\n",
      "Testing: 0.6469526780590603 Training: 0.6450240005574253 Seed: 15737\n",
      "Testing: 0.6584774145833441 Training: 0.6421427111586493 Seed: 15739\n",
      "Testing: 0.6494124467467101 Training: 0.6442563125488716 Seed: 15740\n",
      "Testing: 0.6574327648765501 Training: 0.6425139683564339 Seed: 15744\n",
      "Testing: 0.6537940328911204 Training: 0.6433117123746588 Seed: 15745\n",
      "Testing: 0.6557710491363926 Training: 0.642729763459024 Seed: 15751\n",
      "Testing: 0.6538337383661785 Training: 0.6431898617423446 Seed: 15752\n",
      "Testing: 0.647172651780228 Training: 0.6448167874830919 Seed: 15753\n",
      "Testing: 0.6501647091280228 Training: 0.6442869122242463 Seed: 15756\n",
      "Testing: 0.6490428159727677 Training: 0.6444551085730532 Seed: 15757\n",
      "Testing: 0.6515808952298201 Training: 0.6437574784353433 Seed: 15758\n",
      "Testing: 0.6461001747134109 Training: 0.6450974502380413 Seed: 15762\n",
      "Testing: 0.650339351347438 Training: 0.6439900278894459 Seed: 15763\n",
      "Testing: 0.6576731719525297 Training: 0.642334086832363 Seed: 15765\n",
      "Testing: 0.6492332480666861 Training: 0.6442740640899942 Seed: 15766\n",
      "Testing: 0.6606830477503809 Training: 0.6417466821134129 Seed: 15767\n",
      "Testing: 0.6549181238163412 Training: 0.6429200756495232 Seed: 15768\n",
      "Testing: 0.6462526333838834 Training: 0.6451059640276322 Seed: 15769\n",
      "Testing: 0.6565486993961458 Training: 0.6424397599985334 Seed: 15773\n",
      "Testing: 0.6572124043073007 Training: 0.6423611886019875 Seed: 15777\n",
      "Testing: 0.6461515844946503 Training: 0.6451422764315206 Seed: 15778\n",
      "Testing: 0.6491242342529545 Training: 0.6444818331837949 Seed: 15779\n",
      "Testing: 0.6498021144260286 Training: 0.6441545286572586 Seed: 15782\n",
      "Testing: 0.6622432580767461 Training: 0.6409595201812706 Seed: 15783\n",
      "Testing: 0.6633215129696493 Training: 0.6406766621748052 Seed: 15784\n",
      "Testing: 0.6622881391489663 Training: 0.6411917194078454 Seed: 15787\n",
      "Testing: 0.6509195911494974 Training: 0.6440203074676918 Seed: 15789\n",
      "Testing: 0.6501138595032019 Training: 0.6441364693753493 Seed: 15793\n",
      "Testing: 0.6510165437599968 Training: 0.6439057949693223 Seed: 15800\n",
      "Testing: 0.6583007323775389 Training: 0.6421973990739449 Seed: 15801\n",
      "Testing: 0.6462874762799686 Training: 0.6450331167126683 Seed: 15805\n",
      "Testing: 0.6472266981753314 Training: 0.6447911051329519 Seed: 15806\n",
      "Testing: 0.6581403791793677 Training: 0.6421358889319811 Seed: 15811\n",
      "Testing: 0.6459557686268411 Training: 0.645288590824783 Seed: 15815\n",
      "Testing: 0.6636589992234639 Training: 0.6409393051046535 Seed: 15816\n",
      "Testing: 0.6569665187902138 Training: 0.642393864252577 Seed: 15817\n",
      "Testing: 0.6572205241198597 Training: 0.6426110724964136 Seed: 15818\n",
      "Testing: 0.6520802872242215 Training: 0.6436778962658942 Seed: 15824\n",
      "Testing: 0.6673399921858161 Training: 0.6399732978400108 Seed: 15829\n",
      "Testing: 0.6487403132511016 Training: 0.6445213985985181 Seed: 15831\n",
      "Testing: 0.648468762213954 Training: 0.6446062323614276 Seed: 15833\n",
      "Testing: 0.6599938234866725 Training: 0.64166451240316 Seed: 15834\n",
      "Testing: 0.6494548026511157 Training: 0.6444343979588382 Seed: 15835\n",
      "Testing: 0.6456919658070746 Training: 0.6453325851425824 Seed: 15837\n",
      "Testing: 0.6619711770181164 Training: 0.6413561081378621 Seed: 15841\n",
      "Testing: 0.6469178609821409 Training: 0.6448232744389308 Seed: 15843\n",
      "Testing: 0.6686129563478957 Training: 0.6397284149060585 Seed: 15845\n",
      "Testing: 0.6497185900496634 Training: 0.6442555317866128 Seed: 15846\n",
      "Testing: 0.6675450164954958 Training: 0.6398508753031158 Seed: 15848\n",
      "Testing: 0.6462132851605142 Training: 0.6451931330951921 Seed: 15849\n",
      "Testing: 0.6514979301702888 Training: 0.6439459052312103 Seed: 15850\n",
      "Testing: 0.6614490061635245 Training: 0.6409127576201519 Seed: 15856\n",
      "Testing: 0.6553856300439789 Training: 0.6429253115158005 Seed: 15858\n",
      "Testing: 0.6661797002958003 Training: 0.6399206229080161 Seed: 15864\n",
      "Testing: 0.6570503121776214 Training: 0.6422378651938099 Seed: 15866\n",
      "Testing: 0.662214075504391 Training: 0.6413164622762811 Seed: 15869\n",
      "Testing: 0.6508483941817683 Training: 0.643981799515518 Seed: 15870\n",
      "Testing: 0.6479973716119201 Training: 0.6447698380725801 Seed: 15871\n",
      "Testing: 0.6621295691232008 Training: 0.6412055463413573 Seed: 15877\n",
      "Testing: 0.6558521747171513 Training: 0.642712102246735 Seed: 15878\n",
      "Testing: 0.6531414547095328 Training: 0.6434373834567062 Seed: 15879\n",
      "Testing: 0.6485781596473568 Training: 0.6445640396913304 Seed: 15882\n",
      "Testing: 0.6471638284641368 Training: 0.6448979742227963 Seed: 15887\n",
      "Testing: 0.6505238402338975 Training: 0.6440934718232688 Seed: 15890\n",
      "Testing: 0.6469430795544575 Training: 0.644942746507894 Seed: 15894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6478479382668583 Training: 0.6447444375993827 Seed: 15898\n",
      "Testing: 0.6529309881980279 Training: 0.6434176381100933 Seed: 15901\n",
      "Testing: 0.6532272245170472 Training: 0.643378075281354 Seed: 15902\n",
      "Testing: 0.6505629118905453 Training: 0.6441298206959577 Seed: 15905\n",
      "Testing: 0.6618921267852087 Training: 0.6412261436088468 Seed: 15907\n",
      "Testing: 0.6488676995750857 Training: 0.6445257801283142 Seed: 15908\n",
      "Testing: 0.6500916489699231 Training: 0.6441044683831476 Seed: 15911\n",
      "Testing: 0.6592167967438506 Training: 0.6418128473227189 Seed: 15912\n",
      "Testing: 0.6479334540442486 Training: 0.6447584531495267 Seed: 15913\n",
      "Testing: 0.6476020620351044 Training: 0.6447090070656964 Seed: 15914\n",
      "Testing: 0.6488585275568164 Training: 0.6445144741722341 Seed: 15915\n",
      "Testing: 0.6530994508919582 Training: 0.6435193362001774 Seed: 15918\n",
      "Testing: 0.6468297465995738 Training: 0.6450005519013736 Seed: 15919\n",
      "Testing: 0.6650711274800072 Training: 0.6405436887252562 Seed: 15920\n",
      "Testing: 0.6453851504917978 Training: 0.645363226352217 Seed: 15921\n",
      "Testing: 0.6453992272019684 Training: 0.6453921690947183 Seed: 15923\n",
      "Testing: 0.6466367544306639 Training: 0.6450158045981828 Seed: 15924\n",
      "Testing: 0.6576601552754326 Training: 0.642259307913861 Seed: 15927\n",
      "Testing: 0.6464673873518256 Training: 0.6451477171124487 Seed: 15930\n",
      "Testing: 0.6583266540949088 Training: 0.6422078168329332 Seed: 15932\n",
      "Testing: 0.6563054077142044 Training: 0.6425649555337789 Seed: 15933\n",
      "Testing: 0.645908045491901 Training: 0.6451123207265311 Seed: 15934\n",
      "Testing: 0.6597024182710896 Training: 0.6417232111215394 Seed: 15939\n",
      "Testing: 0.662383268436147 Training: 0.6410067780242432 Seed: 15943\n",
      "Testing: 0.6660504963378193 Training: 0.6402369251063961 Seed: 15945\n",
      "Testing: 0.6515085836048815 Training: 0.6439550100755862 Seed: 15947\n",
      "Testing: 0.6590214791192222 Training: 0.6420054256874602 Seed: 15948\n",
      "Testing: 0.6623477466541631 Training: 0.6412596130769043 Seed: 15949\n",
      "Testing: 0.648856344290274 Training: 0.6442202222838611 Seed: 15951\n",
      "Testing: 0.6477914053611878 Training: 0.6447154393697536 Seed: 15952\n",
      "Testing: 0.656716055690764 Training: 0.6424874963914686 Seed: 15953\n",
      "Testing: 0.6500062621330019 Training: 0.6442273774338534 Seed: 15957\n",
      "Testing: 0.648219270280614 Training: 0.6447005190334747 Seed: 15959\n",
      "Testing: 0.6551372285290302 Training: 0.6427008902109684 Seed: 15966\n",
      "Testing: 0.668431836704553 Training: 0.6395834704515926 Seed: 15967\n",
      "Testing: 0.645953596140119 Training: 0.6452917878082477 Seed: 15968\n",
      "Testing: 0.6512074981218888 Training: 0.64401641728095 Seed: 15971\n",
      "Testing: 0.6531120565076245 Training: 0.643424663814863 Seed: 15974\n",
      "Testing: 0.6479272485913963 Training: 0.6447589961537094 Seed: 15976\n",
      "Testing: 0.6648835047274415 Training: 0.6401276177680706 Seed: 15977\n",
      "Testing: 0.6480613202365172 Training: 0.6447160330986528 Seed: 15979\n",
      "Testing: 0.6605263776872713 Training: 0.6413598421173601 Seed: 15980\n",
      "Testing: 0.6649652608292104 Training: 0.6403494515313826 Seed: 15981\n",
      "Testing: 0.6499845171601794 Training: 0.6441581423069167 Seed: 15982\n",
      "Testing: 0.6538182388941133 Training: 0.6432817590175699 Seed: 15983\n",
      "Testing: 0.6611424873343497 Training: 0.6414890232756019 Seed: 15985\n",
      "Testing: 0.654008728099635 Training: 0.643135111492736 Seed: 15986\n",
      "Testing: 0.660240241847003 Training: 0.6415138301773942 Seed: 15990\n",
      "Testing: 0.6467278029535637 Training: 0.6449933086151521 Seed: 15993\n",
      "Testing: 0.6498711025455313 Training: 0.6443028906770961 Seed: 15994\n",
      "Testing: 0.6522617363874073 Training: 0.6436765218324466 Seed: 15998\n",
      "Testing: 0.6491603164433268 Training: 0.6444618532774551 Seed: 16000\n",
      "Testing: 0.6556031213277805 Training: 0.6427516041656723 Seed: 16001\n",
      "Testing: 0.6457980713145854 Training: 0.6451401555353536 Seed: 16002\n",
      "Testing: 0.6508020906929889 Training: 0.6439247962874897 Seed: 16007\n",
      "Testing: 0.6548645610011482 Training: 0.6429443865319057 Seed: 16008\n",
      "Testing: 0.65654696842537 Training: 0.6424348723445793 Seed: 16011\n",
      "Testing: 0.6554703111770592 Training: 0.6428948511921346 Seed: 16012\n",
      "Testing: 0.6517266151921107 Training: 0.6435784330450164 Seed: 16014\n",
      "Testing: 0.646904502057775 Training: 0.6450249874498064 Seed: 16017\n",
      "Testing: 0.6515243216255103 Training: 0.6438727407356305 Seed: 16018\n",
      "Testing: 0.6502180729296992 Training: 0.6441962739395236 Seed: 16020\n",
      "Testing: 0.6498660018666645 Training: 0.6442372610420705 Seed: 16023\n",
      "Testing: 0.6473608692961162 Training: 0.6448523630271217 Seed: 16025\n",
      "Testing: 0.6528451563874501 Training: 0.6435645318778845 Seed: 16027\n",
      "Testing: 0.6567915134455165 Training: 0.6426368514503151 Seed: 16028\n",
      "Testing: 0.6476647570486864 Training: 0.6447708393814773 Seed: 16031\n",
      "Testing: 0.6530764048015835 Training: 0.6433948200175837 Seed: 16034\n",
      "Testing: 0.6464703799709139 Training: 0.6449067663384207 Seed: 16036\n",
      "Testing: 0.6558082401099278 Training: 0.6426687637485498 Seed: 16039\n",
      "Testing: 0.6474741832119726 Training: 0.6446765825638588 Seed: 16041\n",
      "Testing: 0.658299673978693 Training: 0.6419309852283019 Seed: 16043\n",
      "Testing: 0.66038846730836 Training: 0.6415811782908125 Seed: 16046\n",
      "Testing: 0.6515221612365985 Training: 0.6437559700028201 Seed: 16047\n",
      "Testing: 0.663087062024482 Training: 0.6410334228114991 Seed: 16048\n",
      "Testing: 0.6558086868062756 Training: 0.6428347764273015 Seed: 16051\n",
      "Testing: 0.6498150322125327 Training: 0.6442932903636849 Seed: 16056\n",
      "Testing: 0.665347833917355 Training: 0.6400888725778656 Seed: 16058\n",
      "Testing: 0.6523005201336293 Training: 0.643558388689066 Seed: 16060\n",
      "Testing: 0.6563822400059566 Training: 0.6425570014354046 Seed: 16062\n",
      "Testing: 0.6557072348140999 Training: 0.6427580607052957 Seed: 16065\n",
      "Testing: 0.6549861153782472 Training: 0.6428912620617742 Seed: 16066\n",
      "Testing: 0.6485072486760233 Training: 0.6446116297851097 Seed: 16069\n",
      "Testing: 0.6569588515432649 Training: 0.6424579091115381 Seed: 16073\n",
      "Testing: 0.6475487750245404 Training: 0.6449006902189894 Seed: 16075\n",
      "Testing: 0.672096656649342 Training: 0.63862255662311 Seed: 16076\n",
      "Testing: 0.6458800267989335 Training: 0.6452784267143267 Seed: 16077\n",
      "Testing: 0.6593560670201634 Training: 0.6420017843607486 Seed: 16080\n",
      "Testing: 0.6534267860275181 Training: 0.6431232958262869 Seed: 16082\n",
      "Testing: 0.6531040008294419 Training: 0.643390657094593 Seed: 16084\n",
      "Testing: 0.6550478711269232 Training: 0.6429508906230028 Seed: 16090\n",
      "Testing: 0.6530061898754846 Training: 0.6435171115706199 Seed: 16095\n",
      "Testing: 0.6544907925643814 Training: 0.6429063115276066 Seed: 16097\n",
      "Testing: 0.6511344733504659 Training: 0.6437454022245537 Seed: 16100\n",
      "Testing: 0.6498965198325647 Training: 0.6440243503929587 Seed: 16102\n",
      "Testing: 0.6508839334923731 Training: 0.6439466161822902 Seed: 16105\n",
      "Testing: 0.6477124155477807 Training: 0.6444825584264335 Seed: 16106\n",
      "Testing: 0.6500362242312445 Training: 0.6442792183521254 Seed: 16109\n",
      "Testing: 0.6483196403141316 Training: 0.6446256158583779 Seed: 16111\n",
      "Testing: 0.6544451975835797 Training: 0.6431086982806149 Seed: 16113\n",
      "Testing: 0.6508107943523377 Training: 0.6440539695864258 Seed: 16117\n",
      "Testing: 0.6641944610124831 Training: 0.6404939164286447 Seed: 16118\n",
      "Testing: 0.6536958335223487 Training: 0.6431335405358961 Seed: 16119\n",
      "Testing: 0.659686028378215 Training: 0.6418171515276971 Seed: 16120\n",
      "Testing: 0.6564640769657608 Training: 0.6426194202727948 Seed: 16122\n",
      "Testing: 0.6499273108385784 Training: 0.6440612677987815 Seed: 16123\n",
      "Testing: 0.6580791696369066 Training: 0.6420073819289368 Seed: 16124\n",
      "Testing: 0.6504239647038876 Training: 0.6441290793052961 Seed: 16126\n",
      "Testing: 0.6531556414609534 Training: 0.6433877208368336 Seed: 16131\n",
      "Testing: 0.6587086506375475 Training: 0.6420842977457972 Seed: 16133\n",
      "Testing: 0.6562901293516357 Training: 0.6426287669834573 Seed: 16135\n",
      "Testing: 0.6536486949145147 Training: 0.6432734519294332 Seed: 16139\n",
      "Testing: 0.6456637379979636 Training: 0.644802668769111 Seed: 16140\n",
      "Testing: 0.6458855830619344 Training: 0.6452024333889789 Seed: 16144\n",
      "Testing: 0.6537657863902688 Training: 0.6431446057659027 Seed: 16145\n",
      "Testing: 0.6501712783121918 Training: 0.6441763668347342 Seed: 16146\n",
      "Testing: 0.653721245301532 Training: 0.6432640088937228 Seed: 16148\n",
      "Testing: 0.6456715092687028 Training: 0.6452825914715566 Seed: 16150\n",
      "Testing: 0.6530614190952975 Training: 0.6434224418260522 Seed: 16151\n",
      "Testing: 0.6476187852789462 Training: 0.6448236481341589 Seed: 16152\n",
      "Testing: 0.6455545433225874 Training: 0.6453505432748423 Seed: 16153\n",
      "Testing: 0.6470902590685724 Training: 0.6450083992969032 Seed: 16154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6513644146594898 Training: 0.6438395290684503 Seed: 16161\n",
      "Testing: 0.651392674279084 Training: 0.6438967589568529 Seed: 16163\n",
      "Testing: 0.6541315784238314 Training: 0.6432046387841275 Seed: 16165\n",
      "Testing: 0.6515179444438469 Training: 0.6437859435968103 Seed: 16166\n",
      "Testing: 0.6480812796886253 Training: 0.6446557391564265 Seed: 16168\n",
      "Testing: 0.6543549567819822 Training: 0.6424602397270585 Seed: 16174\n",
      "Testing: 0.6554217547681482 Training: 0.6427820789602998 Seed: 16176\n",
      "Testing: 0.651292406144964 Training: 0.6439590048799053 Seed: 16177\n",
      "Testing: 0.6619551541581726 Training: 0.6412014819616101 Seed: 16181\n",
      "Testing: 0.657726190536082 Training: 0.6423597481224836 Seed: 16182\n",
      "Testing: 0.6520622461567088 Training: 0.6437210928210734 Seed: 16184\n",
      "Testing: 0.648013468200474 Training: 0.6447198268143569 Seed: 16185\n",
      "Testing: 0.652413848098825 Training: 0.6435937660294928 Seed: 16186\n",
      "Testing: 0.6474678669477217 Training: 0.6447888955490151 Seed: 16187\n",
      "Testing: 0.6589652093597544 Training: 0.6421259394624261 Seed: 16189\n",
      "Testing: 0.6520716119005443 Training: 0.6436816864769304 Seed: 16191\n",
      "Testing: 0.6534834656612342 Training: 0.6432246840659143 Seed: 16192\n",
      "Testing: 0.6602305072362207 Training: 0.6417106180265411 Seed: 16193\n",
      "Testing: 0.6570622155575769 Training: 0.6423435765624562 Seed: 16194\n",
      "Testing: 0.6508394497949005 Training: 0.6439448557750636 Seed: 16195\n",
      "Testing: 0.6487034474845251 Training: 0.6445315705013203 Seed: 16196\n",
      "Testing: 0.6596373474548496 Training: 0.6418119870809762 Seed: 16198\n",
      "Testing: 0.6518712020715021 Training: 0.6436978472978907 Seed: 16199\n",
      "Testing: 0.6511596173731686 Training: 0.643898335724968 Seed: 16201\n",
      "Testing: 0.6499877178176936 Training: 0.6443148367835112 Seed: 16203\n",
      "Testing: 0.6463134397713823 Training: 0.6451910060823492 Seed: 16204\n",
      "Testing: 0.6495523382448177 Training: 0.6443122027973758 Seed: 16205\n",
      "Testing: 0.6483315478455887 Training: 0.6446712065190637 Seed: 16213\n",
      "Testing: 0.6546497860699091 Training: 0.6431091056181787 Seed: 16217\n",
      "Testing: 0.6574932551903825 Training: 0.6420683736935614 Seed: 16220\n",
      "Testing: 0.6477773769864876 Training: 0.6446927974385916 Seed: 16221\n",
      "Testing: 0.6576978656279934 Training: 0.6422109989783432 Seed: 16225\n",
      "Testing: 0.6496237839298897 Training: 0.6442625429442073 Seed: 16226\n",
      "Testing: 0.6476735747363078 Training: 0.6448076197253333 Seed: 16227\n",
      "Testing: 0.6560879273929368 Training: 0.6426163124778788 Seed: 16229\n",
      "Testing: 0.647530069184482 Training: 0.6448812480747056 Seed: 16232\n",
      "Testing: 0.6593956590516082 Training: 0.6418733651706174 Seed: 16234\n",
      "Testing: 0.6475933156617959 Training: 0.6448247426520329 Seed: 16236\n",
      "Testing: 0.6455684330389652 Training: 0.6452855457315231 Seed: 16237\n",
      "Testing: 0.649661907230409 Training: 0.6442692010432648 Seed: 16239\n",
      "Testing: 0.6454391300736054 Training: 0.6453673704948457 Seed: 16240\n",
      "Testing: 0.651918887859552 Training: 0.643775057531755 Seed: 16241\n",
      "Testing: 0.6511846436415498 Training: 0.6439892575730738 Seed: 16242\n",
      "Testing: 0.6623960963664274 Training: 0.6408957289942177 Seed: 16244\n",
      "Testing: 0.6569160317313871 Training: 0.6423816373947349 Seed: 16246\n",
      "Testing: 0.6530460232655552 Training: 0.6433938672534225 Seed: 16250\n",
      "Testing: 0.6518489658738762 Training: 0.6436521824052204 Seed: 16251\n",
      "Testing: 0.6566129727397286 Training: 0.6425036763836632 Seed: 16253\n",
      "Testing: 0.66549534171166 Training: 0.6403853006004938 Seed: 16254\n",
      "Testing: 0.6606533128546912 Training: 0.6414759532557471 Seed: 16255\n",
      "Testing: 0.6621002161856463 Training: 0.6410531751000513 Seed: 16256\n",
      "Testing: 0.6732758096584349 Training: 0.6380551183918154 Seed: 16257\n",
      "Testing: 0.6626052274651447 Training: 0.6407620202775945 Seed: 16258\n",
      "Testing: 0.6482950210135148 Training: 0.6446701513211563 Seed: 16262\n",
      "Testing: 0.645853465896375 Training: 0.645256735001658 Seed: 16264\n",
      "Testing: 0.6611359829749273 Training: 0.6414106518955824 Seed: 16267\n",
      "Testing: 0.648656611526863 Training: 0.6445437959354916 Seed: 16271\n",
      "Testing: 0.6521125145416362 Training: 0.6436245791614483 Seed: 16273\n",
      "Testing: 0.6586859761273837 Training: 0.6419716637219718 Seed: 16274\n",
      "Testing: 0.6580544702280768 Training: 0.6419956822569033 Seed: 16280\n",
      "Testing: 0.645362872874203 Training: 0.6453341950764024 Seed: 16289\n",
      "Testing: 0.6525379775623407 Training: 0.6434505383534987 Seed: 16291\n",
      "Testing: 0.6501192103152644 Training: 0.6441750225400925 Seed: 16293\n",
      "Testing: 0.645794879849966 Training: 0.6452244357865626 Seed: 16295\n",
      "Testing: 0.6552146122447091 Training: 0.6429089745174807 Seed: 16297\n",
      "Testing: 0.6544983666863728 Training: 0.6431430260560153 Seed: 16298\n",
      "Testing: 0.6580667916312474 Training: 0.6421123207333137 Seed: 16299\n",
      "Testing: 0.6594939909881308 Training: 0.6418984896421535 Seed: 16303\n",
      "Testing: 0.6643345401470127 Training: 0.6407344468848126 Seed: 16306\n",
      "Testing: 0.64893245329457 Training: 0.6445059085018271 Seed: 16307\n",
      "Testing: 0.6517823149394245 Training: 0.6438115007684835 Seed: 16308\n",
      "Testing: 0.6483945832473172 Training: 0.6445527333741438 Seed: 16309\n",
      "Testing: 0.6475766335718688 Training: 0.6447676485881513 Seed: 16311\n",
      "Testing: 0.6591832194119919 Training: 0.6419609071482735 Seed: 16312\n",
      "Testing: 0.6664959598607265 Training: 0.6400381063610214 Seed: 16313\n",
      "Testing: 0.6480400248056238 Training: 0.6447314933722329 Seed: 16314\n",
      "Testing: 0.6481894112740073 Training: 0.6446736488071219 Seed: 16315\n",
      "Testing: 0.6511024969397927 Training: 0.6439971912837279 Seed: 16317\n",
      "Testing: 0.651234738893509 Training: 0.6438780657162622 Seed: 16322\n",
      "Testing: 0.6698922652684166 Training: 0.6390351515189974 Seed: 16324\n",
      "Testing: 0.6478696712912829 Training: 0.6447080763165406 Seed: 16325\n",
      "Testing: 0.6566652901110909 Training: 0.6426667809612622 Seed: 16327\n",
      "Testing: 0.6573623494191247 Training: 0.642527063249833 Seed: 16328\n",
      "Testing: 0.6503476117254443 Training: 0.6442373916263295 Seed: 16329\n",
      "Testing: 0.6477014292913059 Training: 0.6447753434627989 Seed: 16330\n",
      "Testing: 0.653911366296447 Training: 0.6433329368507991 Seed: 16332\n",
      "Testing: 0.6558961885932933 Training: 0.6427476857757035 Seed: 16338\n",
      "Testing: 0.6456356878314697 Training: 0.6445309250455882 Seed: 16339\n",
      "Testing: 0.6456239269446846 Training: 0.6452560063279631 Seed: 16341\n",
      "Testing: 0.6545293566341195 Training: 0.6430523364288147 Seed: 16342\n",
      "Testing: 0.6563993215238944 Training: 0.6426711174056832 Seed: 16345\n",
      "Testing: 0.6570657039599976 Training: 0.642487974272323 Seed: 16347\n",
      "Testing: 0.6599100312943085 Training: 0.6418491499778822 Seed: 16349\n",
      "Testing: 0.6495961229058673 Training: 0.6442860998430278 Seed: 16353\n",
      "Testing: 0.6525008133577719 Training: 0.6434795193004303 Seed: 16354\n",
      "Testing: 0.6506663279292471 Training: 0.6440819602525993 Seed: 16355\n",
      "Testing: 0.6453055072330063 Training: 0.6449930330185161 Seed: 16356\n",
      "Testing: 0.6506738443069544 Training: 0.6437874448923437 Seed: 16357\n",
      "Testing: 0.6510802523276279 Training: 0.6438818908897684 Seed: 16362\n",
      "Testing: 0.6489622031234176 Training: 0.6444890214471888 Seed: 16365\n",
      "Testing: 0.646081370463887 Training: 0.6452251335465848 Seed: 16366\n",
      "Testing: 0.6478676588173867 Training: 0.6447541903980145 Seed: 16367\n",
      "Testing: 0.656554468705339 Training: 0.6425380392600537 Seed: 16371\n",
      "Testing: 0.6499763889645561 Training: 0.6441001612997017 Seed: 16372\n",
      "Testing: 0.6489525638294975 Training: 0.6438260047076831 Seed: 16373\n",
      "Testing: 0.6541395850506911 Training: 0.6431605562673115 Seed: 16377\n",
      "Testing: 0.6593017827215972 Training: 0.6417957826091738 Seed: 16378\n",
      "Testing: 0.6525974274576551 Training: 0.6435488025725663 Seed: 16379\n",
      "Testing: 0.6479813149685801 Training: 0.6446609287895918 Seed: 16380\n",
      "Testing: 0.6522264095988836 Training: 0.6436641825698561 Seed: 16383\n",
      "Testing: 0.651854129424279 Training: 0.6437203988414524 Seed: 16393\n",
      "Testing: 0.656895702549575 Training: 0.6425360541323816 Seed: 16398\n",
      "Testing: 0.6523081410061028 Training: 0.6436369534563544 Seed: 16400\n",
      "Testing: 0.6478910642983675 Training: 0.6447616066250781 Seed: 16407\n",
      "Testing: 0.6691939330825047 Training: 0.6392461731878247 Seed: 16408\n",
      "Testing: 0.6520227323350551 Training: 0.6436710952053868 Seed: 16409\n",
      "Testing: 0.6569197776779387 Training: 0.6422400395428172 Seed: 16411\n",
      "Testing: 0.6607341392147307 Training: 0.6413425073358894 Seed: 16412\n",
      "Testing: 0.6460549990540155 Training: 0.6451618536970448 Seed: 16415\n",
      "Testing: 0.6499415573177193 Training: 0.6442529238973727 Seed: 16416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6586240861196548 Training: 0.6420553905436539 Seed: 16420\n",
      "Testing: 0.6480647179016934 Training: 0.6447367947190638 Seed: 16421\n",
      "Testing: 0.6510803953054028 Training: 0.643956483985034 Seed: 16423\n",
      "Testing: 0.6599177017335015 Training: 0.6413669418623869 Seed: 16424\n",
      "Testing: 0.6652087662797893 Training: 0.6404536867452016 Seed: 16426\n",
      "Testing: 0.654875169132618 Training: 0.6429705646421744 Seed: 16427\n",
      "Testing: 0.6472780642318181 Training: 0.6447940333843141 Seed: 16428\n",
      "Testing: 0.6558369262704593 Training: 0.6425878132457554 Seed: 16430\n",
      "Testing: 0.6485107309079291 Training: 0.6443829124580588 Seed: 16432\n",
      "Testing: 0.6536979257321128 Training: 0.643310865240202 Seed: 16433\n",
      "Testing: 0.6557875129876938 Training: 0.6428552878059731 Seed: 16440\n",
      "Testing: 0.6553354638879768 Training: 0.6427473702790912 Seed: 16442\n",
      "Testing: 0.6502100060584298 Training: 0.6441643194677017 Seed: 16446\n",
      "Testing: 0.64660453693448 Training: 0.6451083399714441 Seed: 16447\n",
      "Testing: 0.6607077265172727 Training: 0.6415134396443593 Seed: 16449\n",
      "Testing: 0.645706092459584 Training: 0.6453829836551193 Seed: 16450\n",
      "Testing: 0.6541776727480023 Training: 0.643151579090366 Seed: 16451\n",
      "Testing: 0.6648942568207377 Training: 0.6405251755665409 Seed: 16452\n",
      "Testing: 0.6534837994528072 Training: 0.6432564053189487 Seed: 16453\n",
      "Testing: 0.6594762551620926 Training: 0.6415709583431717 Seed: 16454\n",
      "Testing: 0.6488509581190621 Training: 0.6445340099666726 Seed: 16456\n",
      "Testing: 0.6460687411276484 Training: 0.6452612927812567 Seed: 16460\n",
      "Testing: 0.6641702475760665 Training: 0.6402568324240719 Seed: 16461\n",
      "Testing: 0.6462740211442182 Training: 0.6451996582591123 Seed: 16462\n",
      "Testing: 0.6479586833766934 Training: 0.6448357400760061 Seed: 16465\n",
      "Testing: 0.6475644537922542 Training: 0.6447553496234076 Seed: 16469\n",
      "Testing: 0.6489692420138065 Training: 0.6445221219903866 Seed: 16470\n",
      "Testing: 0.6608998502463591 Training: 0.6415314240486104 Seed: 16472\n",
      "Testing: 0.6568021687913195 Training: 0.6425196548106592 Seed: 16474\n",
      "Testing: 0.6506780896501878 Training: 0.6441493503419169 Seed: 16480\n",
      "Testing: 0.6511626169860035 Training: 0.6437960890880989 Seed: 16483\n",
      "Testing: 0.6532126372407663 Training: 0.6431852763099322 Seed: 16485\n",
      "Testing: 0.6575352825511532 Training: 0.6421946544886787 Seed: 16489\n",
      "Testing: 0.6498290177862986 Training: 0.6442032103617722 Seed: 16490\n",
      "Testing: 0.6569905624228536 Training: 0.6424729961330464 Seed: 16500\n",
      "Testing: 0.6525460759084073 Training: 0.643545658066086 Seed: 16502\n",
      "Testing: 0.6463736452055059 Training: 0.6450537377308877 Seed: 16503\n",
      "Testing: 0.6553709834789481 Training: 0.6429665698568439 Seed: 16505\n",
      "Testing: 0.6478959869976475 Training: 0.6447267107520809 Seed: 16507\n",
      "Testing: 0.6561252356785479 Training: 0.6426768851300367 Seed: 16509\n",
      "Testing: 0.6487889913424919 Training: 0.6444425991628884 Seed: 16510\n",
      "Testing: 0.6462059444703478 Training: 0.6451245905689319 Seed: 16512\n",
      "Testing: 0.6455067988794789 Training: 0.6453225953575245 Seed: 16514\n",
      "Testing: 0.6542489091280134 Training: 0.6431953564448633 Seed: 16516\n",
      "Testing: 0.6467578822592727 Training: 0.6448571738603776 Seed: 16518\n",
      "Testing: 0.6511974616385445 Training: 0.6439748765281891 Seed: 16519\n",
      "Testing: 0.6490147745996644 Training: 0.6445263067444759 Seed: 16520\n",
      "Testing: 0.6535351992349681 Training: 0.6433615498060954 Seed: 16521\n",
      "Testing: 0.6650870270853221 Training: 0.6404655728644055 Seed: 16523\n",
      "Testing: 0.6456194085075446 Training: 0.6451106000830211 Seed: 16524\n",
      "Testing: 0.6529461041518115 Training: 0.6433741965465664 Seed: 16527\n",
      "Testing: 0.6521971402764009 Training: 0.6437520812325325 Seed: 16528\n",
      "Testing: 0.6453545676660586 Training: 0.6453042867039667 Seed: 16531\n",
      "Testing: 0.6552538060097524 Training: 0.6428344709609395 Seed: 16534\n",
      "Testing: 0.657093323881241 Training: 0.6424688784295899 Seed: 16536\n",
      "Testing: 0.6492488438128043 Training: 0.6443985203371677 Seed: 16537\n",
      "Testing: 0.6509170217158183 Training: 0.6439619635501797 Seed: 16538\n",
      "Testing: 0.6647548517751274 Training: 0.6403751167760685 Seed: 16540\n",
      "Testing: 0.6490448713946404 Training: 0.6444974565880297 Seed: 16547\n",
      "Testing: 0.668462634731081 Training: 0.6394527090484539 Seed: 16553\n",
      "Testing: 0.6489943885003767 Training: 0.6443713560033397 Seed: 16556\n",
      "Testing: 0.6621382486652639 Training: 0.6409829336775712 Seed: 16560\n",
      "Testing: 0.653620352930152 Training: 0.6432731999354098 Seed: 16563\n",
      "Testing: 0.6721733169645765 Training: 0.6389347427848222 Seed: 16566\n",
      "Testing: 0.6500637855076297 Training: 0.6441570058330589 Seed: 16568\n",
      "Testing: 0.6454522918795429 Training: 0.6451420965056845 Seed: 16569\n",
      "Testing: 0.6473242937948159 Training: 0.6448136151043316 Seed: 16574\n",
      "Testing: 0.6576653646428858 Training: 0.6423773806799392 Seed: 16577\n",
      "Testing: 0.6487109559442585 Training: 0.6446243368460499 Seed: 16578\n",
      "Testing: 0.6643420865334676 Training: 0.6407118143094774 Seed: 16581\n",
      "Testing: 0.6475657010840807 Training: 0.6445930871353647 Seed: 16584\n",
      "Testing: 0.6463544612382655 Training: 0.6450879039482076 Seed: 16585\n",
      "Testing: 0.6577544936762059 Training: 0.6422489436113328 Seed: 16587\n",
      "Testing: 0.6477679557612006 Training: 0.6446942406977654 Seed: 16588\n",
      "Testing: 0.6468425074444869 Training: 0.6449878772804518 Seed: 16589\n",
      "Testing: 0.6545970496990781 Training: 0.6429443800740594 Seed: 16590\n",
      "Testing: 0.6518357892479073 Training: 0.6437151902215614 Seed: 16591\n",
      "Testing: 0.652949374625878 Training: 0.6435491599953389 Seed: 16592\n",
      "Testing: 0.6559699101229248 Training: 0.6426409058943185 Seed: 16595\n",
      "Testing: 0.6548863986050424 Training: 0.6430926212914658 Seed: 16597\n",
      "Testing: 0.6454343211302531 Training: 0.6453215631015723 Seed: 16600\n",
      "Testing: 0.6655072149287559 Training: 0.6401705777729662 Seed: 16602\n",
      "Testing: 0.6689624975561618 Training: 0.6396806615569632 Seed: 16607\n",
      "Testing: 0.6460037674458659 Training: 0.6450948977774258 Seed: 16608\n",
      "Testing: 0.6516578060794711 Training: 0.6437107270451365 Seed: 16609\n",
      "Testing: 0.6586257397270551 Training: 0.6420797352558046 Seed: 16610\n",
      "Testing: 0.6630321710206181 Training: 0.6410331208617752 Seed: 16611\n",
      "Testing: 0.6646698991143205 Training: 0.640670377004941 Seed: 16612\n",
      "Testing: 0.6462046179719692 Training: 0.6451644896019356 Seed: 16614\n",
      "Testing: 0.6560367249715359 Training: 0.6426023346473735 Seed: 16616\n",
      "Testing: 0.6599658017698906 Training: 0.6417833188908357 Seed: 16619\n",
      "Testing: 0.6550109747809286 Training: 0.6429753542184053 Seed: 16621\n",
      "Testing: 0.6570216148774335 Training: 0.6423657038606616 Seed: 16623\n",
      "Testing: 0.654528336546229 Training: 0.6431166799072361 Seed: 16624\n",
      "Testing: 0.6517062223680226 Training: 0.6437792007574208 Seed: 16626\n",
      "Testing: 0.6490023254241154 Training: 0.6445205163348098 Seed: 16628\n",
      "Testing: 0.6542214075001377 Training: 0.6431340527443526 Seed: 16629\n",
      "Testing: 0.6571124076537561 Training: 0.6423381792941351 Seed: 16630\n",
      "Testing: 0.6485375321536742 Training: 0.6445924619986417 Seed: 16631\n",
      "Testing: 0.6472865906198824 Training: 0.6449063253653985 Seed: 16633\n",
      "Testing: 0.6510994432665588 Training: 0.6434555225586565 Seed: 16636\n",
      "Testing: 0.6585927542994069 Training: 0.6422093190628664 Seed: 16637\n",
      "Testing: 0.6506510418607463 Training: 0.6440180610480845 Seed: 16640\n",
      "Testing: 0.6564723551503993 Training: 0.6426218033402018 Seed: 16642\n",
      "Testing: 0.6460836227951171 Training: 0.6447110660378375 Seed: 16643\n",
      "Testing: 0.6532422845840397 Training: 0.6433652598136494 Seed: 16645\n",
      "Testing: 0.6551956133739734 Training: 0.6428992115107517 Seed: 16646\n",
      "Testing: 0.6466491666226873 Training: 0.6451313740927902 Seed: 16647\n",
      "Testing: 0.64984122978006 Training: 0.6441469067562362 Seed: 16649\n",
      "Testing: 0.6489046195878638 Training: 0.6445222515560212 Seed: 16651\n",
      "Testing: 0.6632103736607702 Training: 0.6409996068437013 Seed: 16652\n",
      "Testing: 0.6476348016440676 Training: 0.6448733774018255 Seed: 16653\n",
      "Testing: 0.645943509132092 Training: 0.6451011169585867 Seed: 16654\n",
      "Testing: 0.664952349588456 Training: 0.6402886398693131 Seed: 16655\n",
      "Testing: 0.6557962321243389 Training: 0.6427027051508407 Seed: 16657\n",
      "Testing: 0.6581105477001871 Training: 0.6420145430759798 Seed: 16659\n",
      "Testing: 0.654513168564097 Training: 0.6431857815404389 Seed: 16662\n",
      "Testing: 0.6509051048337136 Training: 0.643929542282084 Seed: 16663\n",
      "Testing: 0.648285083464065 Training: 0.6447208270285699 Seed: 16665\n",
      "Testing: 0.6547550827791531 Training: 0.642740832947452 Seed: 16666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6455623475334942 Training: 0.6452881099780187 Seed: 16668\n",
      "Testing: 0.6500590688700674 Training: 0.6441259525090501 Seed: 16674\n",
      "Testing: 0.6467311012748775 Training: 0.6450220543363568 Seed: 16675\n",
      "Testing: 0.6467515921362639 Training: 0.6451359758596658 Seed: 16676\n",
      "Testing: 0.652306463549618 Training: 0.6435678599825847 Seed: 16678\n",
      "Testing: 0.646844668195482 Training: 0.6445732271900776 Seed: 16679\n",
      "Testing: 0.6567361581161183 Training: 0.642556455754854 Seed: 16680\n",
      "Testing: 0.6500467531143366 Training: 0.6442335594762987 Seed: 16684\n",
      "Testing: 0.6678438955474137 Training: 0.6397262917069114 Seed: 16685\n",
      "Testing: 0.6505905578201066 Training: 0.6440632626099627 Seed: 16686\n",
      "Testing: 0.6575716743456196 Training: 0.6421258060293911 Seed: 16688\n",
      "Testing: 0.6520448770133362 Training: 0.6437260657439093 Seed: 16689\n",
      "Testing: 0.6522904792712316 Training: 0.6435773287972055 Seed: 16693\n",
      "Testing: 0.6530996508288528 Training: 0.6434962214458693 Seed: 16696\n",
      "Testing: 0.6559724839353221 Training: 0.6427246006787976 Seed: 16697\n",
      "Testing: 0.6476800578673225 Training: 0.6447550470181405 Seed: 16698\n",
      "Testing: 0.6571822139265231 Training: 0.6424256353430019 Seed: 16701\n",
      "Testing: 0.6458184748597157 Training: 0.6453288936012516 Seed: 16702\n",
      "Testing: 0.6512791895524557 Training: 0.6438884957638331 Seed: 16703\n",
      "Testing: 0.654761804068158 Training: 0.6430553481399818 Seed: 16705\n",
      "Testing: 0.6573406799029701 Training: 0.6424006536286195 Seed: 16709\n",
      "Testing: 0.6479074457834892 Training: 0.6446217096538002 Seed: 16711\n",
      "Testing: 0.6507423601679341 Training: 0.6439506882380409 Seed: 16712\n",
      "Testing: 0.655889284188854 Training: 0.6427306895571537 Seed: 16716\n",
      "Testing: 0.6572308078991912 Training: 0.6424774131046078 Seed: 16717\n",
      "Testing: 0.6516034890401168 Training: 0.6438566225656985 Seed: 16719\n",
      "Testing: 0.6520841352076511 Training: 0.6436861749150099 Seed: 16721\n",
      "Testing: 0.6585839268359351 Training: 0.6419574227566379 Seed: 16723\n",
      "Testing: 0.6500417608645194 Training: 0.6440966218195805 Seed: 16724\n",
      "Testing: 0.6528856522216853 Training: 0.6434389627559929 Seed: 16728\n",
      "Testing: 0.6490105720079107 Training: 0.6441924042630482 Seed: 16729\n",
      "Testing: 0.6575841087022255 Training: 0.6422142448662089 Seed: 16731\n",
      "Testing: 0.6552085422506779 Training: 0.6429612034017838 Seed: 16733\n",
      "Testing: 0.65088221049054 Training: 0.6439994634593378 Seed: 16736\n",
      "Testing: 0.6625330860312193 Training: 0.6411385632073313 Seed: 16739\n",
      "Testing: 0.6536220602154037 Training: 0.6433079225413849 Seed: 16740\n",
      "Testing: 0.6529125846674928 Training: 0.643555332510641 Seed: 16741\n",
      "Testing: 0.6480318107561234 Training: 0.6447532590145425 Seed: 16746\n",
      "Testing: 0.6456449179942425 Training: 0.6453766258041268 Seed: 16747\n",
      "Testing: 0.6510988684099337 Training: 0.6439346395191159 Seed: 16750\n",
      "Testing: 0.6491812021012024 Training: 0.6444235491269673 Seed: 16752\n",
      "Testing: 0.6472498399082138 Training: 0.6450014945317822 Seed: 16754\n",
      "Testing: 0.6724414588203418 Training: 0.6387952366722977 Seed: 16756\n",
      "Testing: 0.6476193283902323 Training: 0.6447012189983705 Seed: 16757\n",
      "Testing: 0.6486942525177017 Training: 0.644042256313302 Seed: 16761\n",
      "Testing: 0.6455583376293377 Training: 0.6453023955533577 Seed: 16762\n",
      "Testing: 0.6593450020194164 Training: 0.641746031051515 Seed: 16764\n",
      "Testing: 0.6473115779704212 Training: 0.6448334056907292 Seed: 16766\n",
      "Testing: 0.6482774713736903 Training: 0.6445138403951036 Seed: 16769\n",
      "Testing: 0.6463327419635523 Training: 0.6450552034774634 Seed: 16771\n",
      "Testing: 0.6495653960999809 Training: 0.6442706324770153 Seed: 16774\n",
      "Testing: 0.6495592586908531 Training: 0.6439606583844465 Seed: 16777\n",
      "Testing: 0.6584139695453693 Training: 0.642242415028006 Seed: 16779\n",
      "Testing: 0.6458672644247847 Training: 0.6451735331030092 Seed: 16780\n",
      "Testing: 0.6457005926862063 Training: 0.6452449284282346 Seed: 16781\n",
      "Testing: 0.6498982638075658 Training: 0.6441131088174994 Seed: 16782\n",
      "Testing: 0.6501631687536628 Training: 0.6441577933931377 Seed: 16783\n",
      "Testing: 0.6493098391931413 Training: 0.6443538619365061 Seed: 16785\n",
      "Testing: 0.6564753535762146 Training: 0.6426464109517419 Seed: 16791\n",
      "Testing: 0.6470484152702974 Training: 0.6449933859138189 Seed: 16794\n",
      "Testing: 0.653481558500396 Training: 0.6432527901482965 Seed: 16797\n",
      "Testing: 0.6470148036831905 Training: 0.6449841413093976 Seed: 16798\n",
      "Testing: 0.648385173698942 Training: 0.6445179906365462 Seed: 16799\n",
      "Testing: 0.6527694458738872 Training: 0.6435023586555406 Seed: 16800\n",
      "Testing: 0.6555917008348112 Training: 0.6427908880604689 Seed: 16802\n",
      "Testing: 0.6662123984177332 Training: 0.6400471145352087 Seed: 16806\n",
      "Testing: 0.6485335305933165 Training: 0.6439863747534529 Seed: 16808\n",
      "Testing: 0.6507851626730654 Training: 0.6439560379596265 Seed: 16809\n",
      "Testing: 0.6553854849436331 Training: 0.6427841558658822 Seed: 16812\n",
      "Testing: 0.6578367744720697 Training: 0.6422880591411164 Seed: 16814\n",
      "Testing: 0.6577696608855825 Training: 0.6422050408648059 Seed: 16818\n",
      "Testing: 0.6641395449876051 Training: 0.6406120107939104 Seed: 16820\n",
      "Testing: 0.6483253998956313 Training: 0.6446780828937896 Seed: 16825\n",
      "Testing: 0.6475091759937126 Training: 0.6447920507088043 Seed: 16827\n",
      "Testing: 0.6495503181427735 Training: 0.6444203322390029 Seed: 16834\n",
      "Testing: 0.6594677349665297 Training: 0.6418572111626917 Seed: 16836\n",
      "Testing: 0.651053142730458 Training: 0.6439829929120657 Seed: 16838\n",
      "Testing: 0.6505591391547814 Training: 0.6441148736034145 Seed: 16843\n",
      "Testing: 0.6471676341595708 Training: 0.6448639692393301 Seed: 16848\n",
      "Testing: 0.6487387693298766 Training: 0.6444015349662904 Seed: 16852\n",
      "Testing: 0.6569695727470353 Training: 0.6422370763807654 Seed: 16853\n",
      "Testing: 0.6672176324127921 Training: 0.6396962361279639 Seed: 16854\n",
      "Testing: 0.6456425869123416 Training: 0.6450581894993249 Seed: 16864\n",
      "Testing: 0.6460949813147498 Training: 0.6451619329675164 Seed: 16866\n",
      "Testing: 0.6564822719405168 Training: 0.6426277322084097 Seed: 16868\n",
      "Testing: 0.6481217760106455 Training: 0.6447442192598599 Seed: 16870\n",
      "Testing: 0.6501064500129969 Training: 0.6442390485704355 Seed: 16871\n",
      "Testing: 0.6482816359224889 Training: 0.6446333107565348 Seed: 16873\n",
      "Testing: 0.6524722121536788 Training: 0.6431023413061376 Seed: 16874\n",
      "Testing: 0.6543501868173066 Training: 0.6432037605548374 Seed: 16877\n",
      "Testing: 0.6514898589228515 Training: 0.6438738944172414 Seed: 16878\n",
      "Testing: 0.6474027393321107 Training: 0.6449053988475937 Seed: 16879\n",
      "Testing: 0.6520867556514746 Training: 0.6437346336668326 Seed: 16881\n",
      "Testing: 0.6508337066676543 Training: 0.6438544469554991 Seed: 16884\n",
      "Testing: 0.6467569636103008 Training: 0.6449159147072603 Seed: 16889\n",
      "Testing: 0.6470378865581854 Training: 0.644908511182607 Seed: 16890\n",
      "Testing: 0.6474617320004896 Training: 0.64469429233458 Seed: 16891\n",
      "Testing: 0.6484407367281604 Training: 0.6446435053649955 Seed: 16892\n",
      "Testing: 0.6542700697748277 Training: 0.643125292226443 Seed: 16895\n",
      "Testing: 0.6551655886846031 Training: 0.6427867842569785 Seed: 16896\n",
      "Testing: 0.648926702969996 Training: 0.6444819768575403 Seed: 16897\n",
      "Testing: 0.6676919648185184 Training: 0.6399102885306379 Seed: 16901\n",
      "Testing: 0.6494330625999117 Training: 0.6441037868598551 Seed: 16903\n",
      "Testing: 0.6459854479264071 Training: 0.6451080946962761 Seed: 16904\n",
      "Testing: 0.6573590961925484 Training: 0.6423542701380331 Seed: 16905\n",
      "Testing: 0.6555090080081807 Training: 0.6429263495037109 Seed: 16906\n",
      "Testing: 0.657854905325182 Training: 0.6421050248289528 Seed: 16907\n",
      "Testing: 0.6528532832177897 Training: 0.6435057638921622 Seed: 16908\n",
      "Testing: 0.6563043994101532 Training: 0.6425285270723767 Seed: 16909\n",
      "Testing: 0.6588820860985484 Training: 0.6420337687052414 Seed: 16914\n",
      "Testing: 0.6607387993238761 Training: 0.6415220074350034 Seed: 16915\n",
      "Testing: 0.653466931419468 Training: 0.6432332367533996 Seed: 16916\n",
      "Testing: 0.6527872922488082 Training: 0.6434780281917591 Seed: 16920\n",
      "Testing: 0.6555653761238751 Training: 0.6427037256125074 Seed: 16922\n",
      "Testing: 0.6651625500464888 Training: 0.6404648210137039 Seed: 16924\n",
      "Testing: 0.6597500801201732 Training: 0.6416342953471916 Seed: 16925\n",
      "Testing: 0.6469920950306703 Training: 0.6449483453299728 Seed: 16926\n",
      "Testing: 0.6460740328511351 Training: 0.6452775178839238 Seed: 16930\n",
      "Testing: 0.6470114461426548 Training: 0.6448734605545784 Seed: 16931\n",
      "Testing: 0.6476466541492212 Training: 0.6446831125054693 Seed: 16932\n",
      "Testing: 0.6565389049274852 Training: 0.6426213651713979 Seed: 16933\n",
      "Testing: 0.6504879277839645 Training: 0.6441449700437533 Seed: 16938\n",
      "Testing: 0.6527839282057266 Training: 0.6435243257102596 Seed: 16940\n",
      "Testing: 0.660076487612446 Training: 0.641827113176193 Seed: 16941\n",
      "Testing: 0.6532757118750384 Training: 0.6434086859485705 Seed: 16943\n",
      "Testing: 0.657215271153961 Training: 0.6424371836137204 Seed: 16944\n",
      "Testing: 0.651021585063955 Training: 0.6439249430087842 Seed: 16947\n",
      "Testing: 0.6601714563260503 Training: 0.6414401823921367 Seed: 16948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6533335501789205 Training: 0.6431183843709926 Seed: 16954\n",
      "Testing: 0.6456316086283869 Training: 0.6452724829099645 Seed: 16956\n",
      "Testing: 0.6583142027193872 Training: 0.6421187695533173 Seed: 16957\n",
      "Testing: 0.6461147803239469 Training: 0.6449480948549103 Seed: 16959\n",
      "Testing: 0.6489280930271937 Training: 0.644476266518565 Seed: 16960\n",
      "Testing: 0.6514694017084812 Training: 0.643803104403445 Seed: 16961\n",
      "Testing: 0.6538254222494952 Training: 0.6430806328440749 Seed: 16963\n",
      "Testing: 0.6454757950096528 Training: 0.6453114975963083 Seed: 16964\n",
      "Testing: 0.6477595670367102 Training: 0.6447688774691139 Seed: 16967\n",
      "Testing: 0.6512098264049878 Training: 0.6438269810134605 Seed: 16969\n",
      "Testing: 0.6530616347793143 Training: 0.6434397621244272 Seed: 16974\n",
      "Testing: 0.6531725843370441 Training: 0.6434819577762346 Seed: 16978\n",
      "Testing: 0.6497048839236139 Training: 0.6441074414664425 Seed: 16983\n",
      "Testing: 0.6474966712240178 Training: 0.6446357176663294 Seed: 16985\n",
      "Testing: 0.6609259658633381 Training: 0.641467529117776 Seed: 16986\n",
      "Testing: 0.6712293965749344 Training: 0.6390382158346437 Seed: 16988\n",
      "Testing: 0.6571728543904163 Training: 0.6424943069087413 Seed: 16989\n",
      "Testing: 0.6677006695792577 Training: 0.6397131091300094 Seed: 16990\n",
      "Testing: 0.6494886894213497 Training: 0.6442009079513122 Seed: 16991\n",
      "Testing: 0.6622229444482383 Training: 0.6408797351571675 Seed: 16993\n",
      "Testing: 0.6504582710026546 Training: 0.644161343647218 Seed: 16995\n",
      "Testing: 0.6568992599866468 Training: 0.642094981470195 Seed: 16997\n",
      "Testing: 0.6576404793425435 Training: 0.6421529963208126 Seed: 16999\n",
      "Testing: 0.6483704425355947 Training: 0.6446413375696317 Seed: 17003\n",
      "Testing: 0.6529400069274957 Training: 0.6434163359914493 Seed: 17005\n",
      "Testing: 0.6578180368644398 Training: 0.6419790100670666 Seed: 17008\n",
      "Testing: 0.6556338070178126 Training: 0.6428258105404172 Seed: 17009\n",
      "Testing: 0.6472915790906635 Training: 0.6448691627742931 Seed: 17010\n",
      "Testing: 0.6608464835490694 Training: 0.6415209139708966 Seed: 17011\n",
      "Testing: 0.6455857926517312 Training: 0.6452428376904438 Seed: 17012\n",
      "Testing: 0.6533471893297297 Training: 0.6434606956991246 Seed: 17013\n",
      "Testing: 0.6529130516417192 Training: 0.6433260514891629 Seed: 17015\n",
      "Testing: 0.6600523231336286 Training: 0.6416063856204511 Seed: 17017\n",
      "Testing: 0.6497551085455263 Training: 0.6443354650296647 Seed: 17021\n",
      "Testing: 0.650927493618695 Training: 0.6439876184475092 Seed: 17023\n",
      "Testing: 0.6485337237416854 Training: 0.6445380484838579 Seed: 17024\n",
      "Testing: 0.6621486106455196 Training: 0.6411233612803577 Seed: 17026\n",
      "Testing: 0.6521945794597397 Training: 0.6436650959759945 Seed: 17027\n",
      "Testing: 0.6522987004357513 Training: 0.6436461933875289 Seed: 17028\n",
      "Testing: 0.6564300782790566 Training: 0.642636419660089 Seed: 17029\n",
      "Testing: 0.6529023784319348 Training: 0.6433729397896304 Seed: 17030\n",
      "Testing: 0.6465690463952263 Training: 0.6451083149950034 Seed: 17031\n",
      "Testing: 0.661072731635953 Training: 0.641390895225665 Seed: 17033\n",
      "Testing: 0.6595893080544994 Training: 0.6414556420996782 Seed: 17035\n",
      "Testing: 0.6541311960539302 Training: 0.6431238699911439 Seed: 17037\n",
      "Testing: 0.6501074977000758 Training: 0.6436399361818825 Seed: 17038\n",
      "Testing: 0.6517512479101062 Training: 0.6438320898139539 Seed: 17039\n",
      "Testing: 0.6524555478123153 Training: 0.6436347533894912 Seed: 17041\n",
      "Testing: 0.6460840265318889 Training: 0.6449752787562176 Seed: 17043\n",
      "Testing: 0.6621026012067459 Training: 0.641049510035489 Seed: 17046\n",
      "Testing: 0.6550449073917304 Training: 0.6428929869149588 Seed: 17052\n",
      "Testing: 0.6509944432064678 Training: 0.6439992311935321 Seed: 17053\n",
      "Testing: 0.6698992537845166 Training: 0.6392190864430076 Seed: 17056\n",
      "Testing: 0.6533984084917115 Training: 0.6433627645008131 Seed: 17057\n",
      "Testing: 0.6462537635461727 Training: 0.6451640820910243 Seed: 17060\n",
      "Testing: 0.6581338220598143 Training: 0.6423253246034913 Seed: 17061\n",
      "Testing: 0.658681994197595 Training: 0.6420808224898282 Seed: 17062\n",
      "Testing: 0.6625289624244941 Training: 0.6408533637787639 Seed: 17063\n",
      "Testing: 0.6454045666461818 Training: 0.6451597997986915 Seed: 17064\n",
      "Testing: 0.6587526564431742 Training: 0.6421769818619643 Seed: 17065\n",
      "Testing: 0.6486418085696132 Training: 0.6444177317349068 Seed: 17066\n",
      "Testing: 0.6510643027497468 Training: 0.6438887335998529 Seed: 17067\n",
      "Testing: 0.6517404758434748 Training: 0.643719045844189 Seed: 17068\n",
      "Testing: 0.6559677300943009 Training: 0.6427617943033068 Seed: 17069\n",
      "Testing: 0.6503598354748518 Training: 0.6440296066862115 Seed: 17070\n",
      "Testing: 0.6528841310356255 Training: 0.6434967396359581 Seed: 17074\n",
      "Testing: 0.653504493936928 Training: 0.6435080696172344 Seed: 17079\n",
      "Testing: 0.6472747985515035 Training: 0.6449269093746541 Seed: 17081\n",
      "Testing: 0.6581681864335126 Training: 0.6419800340485714 Seed: 17083\n",
      "Testing: 0.6471395970665752 Training: 0.6449494528408167 Seed: 17088\n",
      "Testing: 0.6539002584065496 Training: 0.6431441324945452 Seed: 17090\n",
      "Testing: 0.663509109044545 Training: 0.6407416000568745 Seed: 17093\n",
      "Testing: 0.6466550906184684 Training: 0.6448454707116007 Seed: 17094\n",
      "Testing: 0.6482731184497176 Training: 0.6447024292666039 Seed: 17095\n",
      "Testing: 0.6535829905597395 Training: 0.6432128142129232 Seed: 17100\n",
      "Testing: 0.6462935339768248 Training: 0.6447327566615582 Seed: 17102\n",
      "Testing: 0.6560262712262985 Training: 0.6428453913995299 Seed: 17103\n",
      "Testing: 0.6550526897670805 Training: 0.6429365370508691 Seed: 17104\n",
      "Testing: 0.6574214271439365 Training: 0.6423606430794808 Seed: 17105\n",
      "Testing: 0.6489133704880894 Training: 0.6444731426515006 Seed: 17111\n",
      "Testing: 0.6518876003190814 Training: 0.643712057085615 Seed: 17113\n",
      "Testing: 0.6506132320713637 Training: 0.6441429238968823 Seed: 17116\n",
      "Testing: 0.6515961353895906 Training: 0.643714788141744 Seed: 17117\n",
      "Testing: 0.6456106777590054 Training: 0.645296868541243 Seed: 17118\n",
      "Testing: 0.6508750674895794 Training: 0.643898507650513 Seed: 17119\n",
      "Testing: 0.6663137783041033 Training: 0.6401602236877232 Seed: 17121\n",
      "Testing: 0.6481711917237303 Training: 0.644628972268999 Seed: 17122\n",
      "Testing: 0.6560270943078692 Training: 0.64271797560145 Seed: 17127\n",
      "Testing: 0.6454078223309395 Training: 0.6450767225501238 Seed: 17128\n",
      "Testing: 0.6477307247004354 Training: 0.6446195582981866 Seed: 17130\n",
      "Testing: 0.6496606842950834 Training: 0.6443424210866523 Seed: 17133\n",
      "Testing: 0.658591144682856 Training: 0.6420050500429101 Seed: 17134\n",
      "Testing: 0.6510487969969876 Training: 0.6438270211784516 Seed: 17135\n",
      "Testing: 0.6490439139622097 Training: 0.6442009919501266 Seed: 17137\n",
      "Testing: 0.6659088957005912 Training: 0.6399212833478396 Seed: 17140\n",
      "Testing: 0.6584091761440505 Training: 0.6421510495873369 Seed: 17142\n",
      "Testing: 0.6495490985256911 Training: 0.6433211333878535 Seed: 17144\n",
      "Testing: 0.667480137814692 Training: 0.6396201040075842 Seed: 17145\n",
      "Testing: 0.6658379122820413 Training: 0.6403961544077157 Seed: 17147\n",
      "Testing: 0.6565961673005407 Training: 0.6424980123533166 Seed: 17148\n",
      "Testing: 0.6498070352177631 Training: 0.6442700686864625 Seed: 17158\n",
      "Testing: 0.6516168962605607 Training: 0.6438066439640662 Seed: 17163\n",
      "Testing: 0.648594104561687 Training: 0.6445535489486458 Seed: 17167\n",
      "Testing: 0.654774740793508 Training: 0.6429609302484625 Seed: 17168\n",
      "Testing: 0.6586333818871524 Training: 0.6421478518032708 Seed: 17170\n",
      "Testing: 0.6542024290069662 Training: 0.6431129932420128 Seed: 17171\n",
      "Testing: 0.6523344920521361 Training: 0.6436956577820832 Seed: 17172\n",
      "Testing: 0.6526741068698412 Training: 0.6435134777490226 Seed: 17173\n",
      "Testing: 0.6500096895720634 Training: 0.6442179417815341 Seed: 17175\n",
      "Testing: 0.6469085739820419 Training: 0.645014955362421 Seed: 17177\n",
      "Testing: 0.6581549154723884 Training: 0.6422653747117124 Seed: 17178\n",
      "Testing: 0.6546504210814601 Training: 0.6431351342220079 Seed: 17179\n",
      "Testing: 0.6469402536476423 Training: 0.6449203251913367 Seed: 17184\n",
      "Testing: 0.6504527068973531 Training: 0.6440271217962494 Seed: 17187\n",
      "Testing: 0.6548587032245646 Training: 0.6429961590124907 Seed: 17190\n",
      "Testing: 0.6497370540786382 Training: 0.644264699819854 Seed: 17191\n",
      "Testing: 0.6557674452700739 Training: 0.6429336552464124 Seed: 17192\n",
      "Testing: 0.6478090227445791 Training: 0.6441717711663706 Seed: 17195\n",
      "Testing: 0.649761853591146 Training: 0.6442068749225492 Seed: 17196\n",
      "Testing: 0.648029223916824 Training: 0.6444862587012679 Seed: 17197\n",
      "Testing: 0.6500198320586804 Training: 0.6441891292146921 Seed: 17200\n",
      "Testing: 0.6479183775969695 Training: 0.6448292579924889 Seed: 17201\n",
      "Testing: 0.6535848662873661 Training: 0.6432709350960846 Seed: 17204\n",
      "Testing: 0.6556166634777977 Training: 0.6427538903587403 Seed: 17205\n",
      "Testing: 0.6459592542947746 Training: 0.6449821654234248 Seed: 17207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6509141592852954 Training: 0.6440299365804373 Seed: 17210\n",
      "Testing: 0.6718196095964598 Training: 0.6387228114370097 Seed: 17211\n",
      "Testing: 0.6541131302447458 Training: 0.6431004578964498 Seed: 17212\n",
      "Testing: 0.6513707776646152 Training: 0.6438964899670908 Seed: 17214\n",
      "Testing: 0.66021461338623 Training: 0.6415640844005809 Seed: 17215\n",
      "Testing: 0.6534243225052248 Training: 0.6433335269489919 Seed: 17216\n",
      "Testing: 0.6471884515056735 Training: 0.644919730418441 Seed: 17218\n",
      "Testing: 0.6580122442173139 Training: 0.6421978678911443 Seed: 17219\n",
      "Testing: 0.6651620739290698 Training: 0.6404438451083935 Seed: 17220\n",
      "Testing: 0.6470009623863493 Training: 0.6449213307680628 Seed: 17222\n",
      "Testing: 0.648294678376633 Training: 0.6445115335262277 Seed: 17224\n",
      "Testing: 0.6605265988090122 Training: 0.6415250330073208 Seed: 17226\n",
      "Testing: 0.6562212786536662 Training: 0.6426459510203769 Seed: 17227\n",
      "Testing: 0.6512241395087597 Training: 0.6439398237949376 Seed: 17230\n",
      "Testing: 0.6579074491259089 Training: 0.6422458919466318 Seed: 17231\n",
      "Testing: 0.648516141456667 Training: 0.644656604364191 Seed: 17233\n",
      "Testing: 0.6553529702903259 Training: 0.6428780857750873 Seed: 17237\n",
      "Testing: 0.6486961853754242 Training: 0.6445545990897091 Seed: 17238\n",
      "Testing: 0.6573610066596873 Training: 0.6421454914209317 Seed: 17243\n",
      "Testing: 0.6489373052830391 Training: 0.6444243384831774 Seed: 17244\n",
      "Testing: 0.6458636070343393 Training: 0.6450701389213187 Seed: 17246\n",
      "Testing: 0.6667217697820286 Training: 0.6399732859774094 Seed: 17247\n",
      "Testing: 0.6623402045547356 Training: 0.6411788147189716 Seed: 17250\n",
      "Testing: 0.6533416683553257 Training: 0.643443135379899 Seed: 17251\n",
      "Testing: 0.6475091273497617 Training: 0.6448216416495489 Seed: 17252\n",
      "Testing: 0.6506392448142273 Training: 0.6440323121266011 Seed: 17253\n",
      "Testing: 0.6479567538211138 Training: 0.6446935321715007 Seed: 17254\n",
      "Testing: 0.6614217550737309 Training: 0.6412814235709057 Seed: 17258\n",
      "Testing: 0.6535609936952684 Training: 0.6434062241259368 Seed: 17259\n",
      "Testing: 0.6474845298685175 Training: 0.6444073294259651 Seed: 17264\n",
      "Testing: 0.656273981580319 Training: 0.6426436456971387 Seed: 17265\n",
      "Testing: 0.6495605220943977 Training: 0.6442077372379305 Seed: 17267\n",
      "Testing: 0.64651335517897 Training: 0.6450802693738674 Seed: 17270\n",
      "Testing: 0.6669961644963336 Training: 0.6397769528561772 Seed: 17271\n",
      "Testing: 0.6485457026245353 Training: 0.644568659366295 Seed: 17277\n",
      "Testing: 0.6593312195289932 Training: 0.6417631631243914 Seed: 17287\n",
      "Testing: 0.6516350448757192 Training: 0.6437978509374249 Seed: 17288\n",
      "Testing: 0.6520185239160421 Training: 0.6437519808172218 Seed: 17289\n",
      "Testing: 0.6553278979576065 Training: 0.642652573772837 Seed: 17290\n",
      "Testing: 0.6485252664992964 Training: 0.6445377447559753 Seed: 17296\n",
      "Testing: 0.653773881564811 Training: 0.6432993265273222 Seed: 17297\n",
      "Testing: 0.666333276599861 Training: 0.6400531496346187 Seed: 17298\n",
      "Testing: 0.6695375991409613 Training: 0.6391431647555181 Seed: 17299\n",
      "Testing: 0.65638296492347 Training: 0.6427180942440034 Seed: 17300\n",
      "Testing: 0.6526620547836101 Training: 0.6433589078706516 Seed: 17303\n",
      "Testing: 0.6852927875336821 Training: 0.6356392859352955 Seed: 17304\n",
      "Testing: 0.6563552256981051 Training: 0.6426400733426071 Seed: 17305\n",
      "Testing: 0.6474472424890038 Training: 0.644892882770207 Seed: 17307\n",
      "Testing: 0.6510255831236413 Training: 0.6439714371737875 Seed: 17313\n",
      "Testing: 0.6532157584494847 Training: 0.6432205330671652 Seed: 17314\n",
      "Testing: 0.6495035745735033 Training: 0.6442770813194358 Seed: 17315\n",
      "Testing: 0.6473762471276822 Training: 0.6448154316000052 Seed: 17316\n",
      "Testing: 0.6520629791034919 Training: 0.643676904323913 Seed: 17317\n",
      "Testing: 0.6544156329954949 Training: 0.6431740544918954 Seed: 17318\n",
      "Testing: 0.6463786755293888 Training: 0.645146155448605 Seed: 17321\n",
      "Testing: 0.6470446858770127 Training: 0.6449669097995594 Seed: 17323\n",
      "Testing: 0.6491618607809924 Training: 0.6444591531408297 Seed: 17328\n",
      "Testing: 0.6624289152887061 Training: 0.6409765135135764 Seed: 17329\n",
      "Testing: 0.656033143829414 Training: 0.6427534815453316 Seed: 17334\n",
      "Testing: 0.6681537436014322 Training: 0.6397306387222262 Seed: 17337\n",
      "Testing: 0.65244007222534 Training: 0.6435445209005776 Seed: 17339\n",
      "Testing: 0.6486301496625064 Training: 0.6445832841181414 Seed: 17341\n",
      "Testing: 0.6463567501614801 Training: 0.6449583816345235 Seed: 17344\n",
      "Testing: 0.6517474821045318 Training: 0.6437894026377581 Seed: 17347\n",
      "Testing: 0.6548386583887453 Training: 0.642974900181593 Seed: 17348\n",
      "Testing: 0.6653518289187524 Training: 0.6398211169575074 Seed: 17349\n",
      "Testing: 0.6571349445155847 Training: 0.6423701740861067 Seed: 17350\n",
      "Testing: 0.6508973730605272 Training: 0.6439557314599471 Seed: 17351\n",
      "Testing: 0.6479872222067722 Training: 0.6446473451326038 Seed: 17353\n",
      "Testing: 0.659199999654942 Training: 0.6416464185167727 Seed: 17354\n",
      "Testing: 0.6559920705180193 Training: 0.6427309227410171 Seed: 17355\n",
      "Testing: 0.6533851539553233 Training: 0.6432396393850115 Seed: 17357\n",
      "Testing: 0.6562347505328373 Training: 0.6425853695830981 Seed: 17358\n",
      "Testing: 0.6500129591990079 Training: 0.6442403973638015 Seed: 17361\n",
      "Testing: 0.6567024106174959 Training: 0.6425773763041187 Seed: 17364\n",
      "Testing: 0.6487336106422421 Training: 0.6445396906439855 Seed: 17366\n",
      "Testing: 0.6461594032411442 Training: 0.6452399138912543 Seed: 17367\n",
      "Testing: 0.6512897885330537 Training: 0.6438125842936097 Seed: 17372\n",
      "Testing: 0.6669088341667695 Training: 0.6396518539567612 Seed: 17373\n",
      "Testing: 0.6568849813571653 Training: 0.6424864956292519 Seed: 17375\n",
      "Testing: 0.6536126556187402 Training: 0.6432761453411775 Seed: 17376\n",
      "Testing: 0.6624400491300778 Training: 0.6410098080423856 Seed: 17379\n",
      "Testing: 0.6465049603296881 Training: 0.644740806165955 Seed: 17380\n",
      "Testing: 0.6490972854793766 Training: 0.6444535533799018 Seed: 17381\n",
      "Testing: 0.6477120037101204 Training: 0.6447811054086973 Seed: 17382\n",
      "Testing: 0.6612481972294392 Training: 0.6413363452320326 Seed: 17384\n",
      "Testing: 0.6575846454593925 Training: 0.6421573199983948 Seed: 17391\n",
      "Testing: 0.6490726034068789 Training: 0.6444639214870301 Seed: 17396\n",
      "Testing: 0.6579200883510716 Training: 0.6421414180726666 Seed: 17399\n",
      "Testing: 0.6525518109859024 Training: 0.6436096683011985 Seed: 17400\n",
      "Testing: 0.6470489725969012 Training: 0.6449962786826698 Seed: 17401\n",
      "Testing: 0.6557413195181998 Training: 0.6426415029791461 Seed: 17405\n",
      "Testing: 0.6507927979382298 Training: 0.6441064440034563 Seed: 17407\n",
      "Testing: 0.6465819545192955 Training: 0.6449911538233399 Seed: 17409\n",
      "Testing: 0.6586751761444346 Training: 0.641971918586847 Seed: 17410\n",
      "Testing: 0.6526767789118256 Training: 0.6435640503867517 Seed: 17411\n",
      "Testing: 0.6508389038980948 Training: 0.6439576047897893 Seed: 17413\n",
      "Testing: 0.652049708220473 Training: 0.6437628614915627 Seed: 17414\n",
      "Testing: 0.6461708916008335 Training: 0.6451703154631366 Seed: 17419\n",
      "Testing: 0.6538635617916452 Training: 0.6432792918302215 Seed: 17420\n",
      "Testing: 0.6475979652930296 Training: 0.6448232906036242 Seed: 17421\n",
      "Testing: 0.6534832589371654 Training: 0.6433248896407393 Seed: 17422\n",
      "Testing: 0.6589613919787337 Training: 0.6419490381541895 Seed: 17428\n",
      "Testing: 0.6526606359395891 Training: 0.6435221002083467 Seed: 17430\n",
      "Testing: 0.6471890882126416 Training: 0.6449108017078105 Seed: 17435\n",
      "Testing: 0.6623197741794191 Training: 0.6410954803825042 Seed: 17436\n",
      "Testing: 0.6563094343993587 Training: 0.6426429034214217 Seed: 17437\n",
      "Testing: 0.6549272307377624 Training: 0.6428321121435858 Seed: 17442\n",
      "Testing: 0.6584208980597743 Training: 0.642130573044263 Seed: 17445\n",
      "Testing: 0.6538050264720634 Training: 0.6431340980494051 Seed: 17448\n",
      "Testing: 0.6618727620436621 Training: 0.6413320659057992 Seed: 17451\n",
      "Testing: 0.648818090646828 Training: 0.6445515802771621 Seed: 17452\n",
      "Testing: 0.645576613522034 Training: 0.6451172507025535 Seed: 17453\n",
      "Testing: 0.6617233965306157 Training: 0.6412143470837002 Seed: 17454\n",
      "Testing: 0.650229958015815 Training: 0.6442183720658188 Seed: 17455\n",
      "Testing: 0.6520515745390332 Training: 0.6435177232625289 Seed: 17460\n",
      "Testing: 0.6584815886974742 Training: 0.6417460297000517 Seed: 17461\n",
      "Testing: 0.6507689177399433 Training: 0.6440332548539146 Seed: 17462\n",
      "Testing: 0.6538385289050516 Training: 0.6432372526639206 Seed: 17465\n",
      "Testing: 0.645670951482392 Training: 0.6451970304958388 Seed: 17466\n",
      "Testing: 0.6472906132357482 Training: 0.6444838969115401 Seed: 17468\n",
      "Testing: 0.6479001086167393 Training: 0.644783734944874 Seed: 17469\n",
      "Testing: 0.6533825459536993 Training: 0.6433448555370036 Seed: 17471\n",
      "Testing: 0.6531204925071392 Training: 0.6434406273139381 Seed: 17474\n",
      "Testing: 0.648787741973851 Training: 0.6445391106675661 Seed: 17475\n",
      "Testing: 0.6526156348389607 Training: 0.6435955129510295 Seed: 17476\n",
      "Testing: 0.6575848463309368 Training: 0.6422655025908766 Seed: 17478\n",
      "Testing: 0.6459712692614623 Training: 0.6451877915250084 Seed: 17479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6562135646027576 Training: 0.6427046349450836 Seed: 17481\n",
      "Testing: 0.6499788338205905 Training: 0.6441806952092152 Seed: 17483\n",
      "Testing: 0.6519713841926148 Training: 0.6437780859252846 Seed: 17485\n",
      "Testing: 0.647739196143557 Training: 0.64484099742285 Seed: 17491\n",
      "Testing: 0.6503846722833957 Training: 0.6441404905986103 Seed: 17492\n",
      "Testing: 0.6595167021701661 Training: 0.641714363791964 Seed: 17493\n",
      "Testing: 0.6460255589627418 Training: 0.6451198419467878 Seed: 17495\n",
      "Testing: 0.6457522309411869 Training: 0.6452120362763715 Seed: 17499\n",
      "Testing: 0.6539081977980754 Training: 0.6432452806995077 Seed: 17500\n",
      "Testing: 0.6493206539163203 Training: 0.6441665175798634 Seed: 17502\n",
      "Testing: 0.6486768085304437 Training: 0.6444189138842991 Seed: 17503\n",
      "Testing: 0.6602553814917871 Training: 0.6416811786854227 Seed: 17506\n",
      "Testing: 0.647436469450573 Training: 0.6447613800738052 Seed: 17507\n",
      "Testing: 0.6563897221151036 Training: 0.6424084789926496 Seed: 17512\n",
      "Testing: 0.645577360841974 Training: 0.6453600785298897 Seed: 17513\n",
      "Testing: 0.6516246632239635 Training: 0.6437699044429614 Seed: 17514\n",
      "Testing: 0.6453587965607638 Training: 0.6453393306183811 Seed: 17515\n",
      "Testing: 0.6472645843084801 Training: 0.6447487786461377 Seed: 17516\n",
      "Testing: 0.6662095576087564 Training: 0.6399909174463367 Seed: 17517\n",
      "Testing: 0.6522102370541107 Training: 0.6436526867258578 Seed: 17521\n",
      "Testing: 0.6525317638105829 Training: 0.6436104015696086 Seed: 17523\n",
      "Testing: 0.6555623564645314 Training: 0.6428116718271356 Seed: 17524\n",
      "Testing: 0.6551679533094841 Training: 0.6430016200366138 Seed: 17527\n",
      "Testing: 0.6471801106241133 Training: 0.6449259673742845 Seed: 17530\n",
      "Testing: 0.6483652340920709 Training: 0.6446641297724713 Seed: 17532\n",
      "Testing: 0.6468994744748094 Training: 0.6449109651961772 Seed: 17536\n",
      "Testing: 0.660625010532423 Training: 0.641591189145307 Seed: 17538\n",
      "Testing: 0.6467101330203344 Training: 0.6450579673451705 Seed: 17541\n",
      "Testing: 0.6720791173388867 Training: 0.6381958165130774 Seed: 17542\n",
      "Testing: 0.6495094052239093 Training: 0.6443358145131205 Seed: 17543\n",
      "Testing: 0.6465855782440955 Training: 0.6450799904211633 Seed: 17544\n",
      "Testing: 0.6472680888896551 Training: 0.6448954428587159 Seed: 17545\n",
      "Testing: 0.6602476299500951 Training: 0.6414947874188134 Seed: 17547\n",
      "Testing: 0.6565256710845977 Training: 0.642542010556722 Seed: 17549\n",
      "Testing: 0.6552155438115237 Training: 0.6429872477792067 Seed: 17552\n",
      "Testing: 0.659991864482207 Training: 0.6416181557028073 Seed: 17557\n",
      "Testing: 0.6526155889078035 Training: 0.6433619281590978 Seed: 17558\n",
      "Testing: 0.6495540562778517 Training: 0.6443438066121195 Seed: 17559\n",
      "Testing: 0.6519427250132107 Training: 0.6436297867259422 Seed: 17560\n",
      "Testing: 0.6462365175562371 Training: 0.6451225073391658 Seed: 17561\n",
      "Testing: 0.6571526744069477 Training: 0.6423289350172352 Seed: 17563\n",
      "Testing: 0.6694811679971333 Training: 0.6392544241278483 Seed: 17569\n",
      "Testing: 0.6621897387035458 Training: 0.6407740086148449 Seed: 17571\n",
      "Testing: 0.6492941857445047 Training: 0.6443652143275559 Seed: 17572\n",
      "Testing: 0.6487704546541423 Training: 0.6444284778189803 Seed: 17573\n",
      "Testing: 0.6494541972466416 Training: 0.6444700683068919 Seed: 17576\n",
      "Testing: 0.6489063505706152 Training: 0.6444734801777354 Seed: 17577\n",
      "Testing: 0.6640539277657042 Training: 0.6407723991885181 Seed: 17579\n",
      "Testing: 0.6596769184122329 Training: 0.6418593663815718 Seed: 17581\n",
      "Testing: 0.6753239853896239 Training: 0.6376760917088335 Seed: 17583\n",
      "Testing: 0.6588799391299189 Training: 0.6420085487744098 Seed: 17584\n",
      "Testing: 0.653861365180379 Training: 0.6429549286985634 Seed: 17586\n",
      "Testing: 0.6507982393220834 Training: 0.6440846838828602 Seed: 17588\n",
      "Testing: 0.655906266102454 Training: 0.6427170423229877 Seed: 17589\n",
      "Testing: 0.6504212649577618 Training: 0.6440491165059798 Seed: 17590\n",
      "Testing: 0.6528515711070622 Training: 0.6435529570136282 Seed: 17592\n",
      "Testing: 0.6617107179175504 Training: 0.641103905021039 Seed: 17593\n",
      "Testing: 0.6516764671879519 Training: 0.6436217170693337 Seed: 17595\n",
      "Testing: 0.6602250526991105 Training: 0.6416500345133705 Seed: 17598\n",
      "Testing: 0.6476431569797816 Training: 0.644840782872705 Seed: 17601\n",
      "Testing: 0.6460289389554912 Training: 0.6452560814415429 Seed: 17602\n",
      "Testing: 0.6469120807625892 Training: 0.6450031454116099 Seed: 17605\n",
      "Testing: 0.6466023754505519 Training: 0.6449976931383459 Seed: 17607\n",
      "Testing: 0.6488300334072519 Training: 0.6444496371999341 Seed: 17609\n",
      "Testing: 0.6618701286543627 Training: 0.6409414552318273 Seed: 17610\n",
      "Testing: 0.6487732120073947 Training: 0.6445136404142834 Seed: 17611\n",
      "Testing: 0.6532961725514695 Training: 0.6433723026638309 Seed: 17615\n",
      "Testing: 0.6489160035981619 Training: 0.6444652682934746 Seed: 17616\n",
      "Testing: 0.657134758663796 Training: 0.642444335456435 Seed: 17617\n",
      "Testing: 0.6565686657010649 Training: 0.6426640369199679 Seed: 17618\n",
      "Testing: 0.6671410456697052 Training: 0.6400621300169089 Seed: 17621\n",
      "Testing: 0.6476607861277851 Training: 0.6448534058924881 Seed: 17622\n",
      "Testing: 0.6516159544971947 Training: 0.6438145048805661 Seed: 17623\n",
      "Testing: 0.6504505633001499 Training: 0.6441352514374805 Seed: 17624\n",
      "Testing: 0.647976681431953 Training: 0.6446909039696129 Seed: 17626\n",
      "Testing: 0.649045068976017 Training: 0.6443784471591135 Seed: 17629\n",
      "Testing: 0.6623021448213449 Training: 0.641367926308004 Seed: 17630\n",
      "Testing: 0.6543517080217299 Training: 0.6431934643883345 Seed: 17631\n",
      "Testing: 0.6461044493919696 Training: 0.6450802246259809 Seed: 17636\n",
      "Testing: 0.6536365870613701 Training: 0.6432405932312343 Seed: 17640\n",
      "Testing: 0.6515505833210157 Training: 0.6436908100547436 Seed: 17642\n",
      "Testing: 0.6483679192074874 Training: 0.644676554172102 Seed: 17643\n",
      "Testing: 0.6562964709846661 Training: 0.6426701847401546 Seed: 17644\n",
      "Testing: 0.645517862404243 Training: 0.6452757327249183 Seed: 17646\n",
      "Testing: 0.6531361057328516 Training: 0.6431571369613942 Seed: 17648\n",
      "Testing: 0.6590548473444012 Training: 0.6418299564682624 Seed: 17650\n",
      "Testing: 0.6526800357567576 Training: 0.6432988867056777 Seed: 17651\n",
      "Testing: 0.6470048631100751 Training: 0.6449544071581365 Seed: 17653\n",
      "Testing: 0.6528747957780494 Training: 0.6434813446062262 Seed: 17654\n",
      "Testing: 0.6501895634796746 Training: 0.6441427294623308 Seed: 17655\n",
      "Testing: 0.6729086992457829 Training: 0.6384964523292441 Seed: 17659\n",
      "Testing: 0.6523024428997659 Training: 0.6436897102179974 Seed: 17660\n",
      "Testing: 0.6510999863777263 Training: 0.6439535625728328 Seed: 17663\n",
      "Testing: 0.6607111564103036 Training: 0.6414697035929807 Seed: 17665\n",
      "Testing: 0.6514980566234823 Training: 0.6438353697637474 Seed: 17666\n",
      "Testing: 0.6528851801727862 Training: 0.6435767499816578 Seed: 17667\n",
      "Testing: 0.6474307618575371 Training: 0.6447722771898148 Seed: 17668\n",
      "Testing: 0.6457816700751716 Training: 0.6452577656487344 Seed: 17669\n",
      "Testing: 0.6600336642405565 Training: 0.6416709291665863 Seed: 17670\n",
      "Testing: 0.6500543844645533 Training: 0.6442161920844729 Seed: 17675\n",
      "Testing: 0.6524043107421398 Training: 0.6432854351127689 Seed: 17677\n",
      "Testing: 0.648031473655938 Training: 0.6447115723105954 Seed: 17680\n",
      "Testing: 0.648223262069906 Training: 0.6441338284397214 Seed: 17688\n",
      "Testing: 0.6455545515231706 Training: 0.6453102313440661 Seed: 17690\n",
      "Testing: 0.6468611160783708 Training: 0.644954089318537 Seed: 17691\n",
      "Testing: 0.6501687685187552 Training: 0.6442264934205773 Seed: 17696\n",
      "Testing: 0.6505278619450856 Training: 0.6441503937430575 Seed: 17699\n",
      "Testing: 0.6484174706642704 Training: 0.6445872629085933 Seed: 17700\n",
      "Testing: 0.6596817309808647 Training: 0.6417044799675686 Seed: 17701\n",
      "Testing: 0.6610705407770796 Training: 0.6416260698920373 Seed: 17703\n",
      "Testing: 0.6512264742552778 Training: 0.6439442237654112 Seed: 17704\n",
      "Testing: 0.6576582928496081 Training: 0.6420491226875835 Seed: 17705\n",
      "Testing: 0.6490591010747433 Training: 0.6444580279865876 Seed: 17706\n",
      "Testing: 0.6558116011258346 Training: 0.6426824602126873 Seed: 17710\n",
      "Testing: 0.6529679284951061 Training: 0.643413867020022 Seed: 17716\n",
      "Testing: 0.6510032278597576 Training: 0.6439032834527283 Seed: 17717\n",
      "Testing: 0.6664793199823681 Training: 0.639855705159609 Seed: 17719\n",
      "Testing: 0.6473381406996752 Training: 0.6449216109924362 Seed: 17720\n",
      "Testing: 0.6599649830453201 Training: 0.641580330603401 Seed: 17721\n",
      "Testing: 0.6554058119730555 Training: 0.6428752591700678 Seed: 17726\n",
      "Testing: 0.6536423601480122 Training: 0.6431650319822055 Seed: 17728\n",
      "Testing: 0.6524498349957264 Training: 0.6433818826794359 Seed: 17729\n",
      "Testing: 0.6652458496414166 Training: 0.6403773686683607 Seed: 17730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6530104155104344 Training: 0.6434468661261287 Seed: 17735\n",
      "Testing: 0.6518163109168502 Training: 0.6437833720514071 Seed: 17736\n",
      "Testing: 0.6478982141100779 Training: 0.6447911062862237 Seed: 17738\n",
      "Testing: 0.6481355843767442 Training: 0.644742020818336 Seed: 17741\n",
      "Testing: 0.6647171687096101 Training: 0.6404344650362177 Seed: 17742\n",
      "Testing: 0.6461047820298549 Training: 0.6451710757410807 Seed: 17743\n",
      "Testing: 0.6517875433085719 Training: 0.6438536195684905 Seed: 17745\n",
      "Testing: 0.6588404361250833 Training: 0.6417693560290629 Seed: 17746\n",
      "Testing: 0.6466542607874671 Training: 0.644650730502582 Seed: 17748\n",
      "Testing: 0.6675159309513767 Training: 0.6397053974837927 Seed: 17750\n",
      "Testing: 0.6585439208258277 Training: 0.6420734365699431 Seed: 17751\n",
      "Testing: 0.6508788519325044 Training: 0.6440658671904287 Seed: 17753\n",
      "Testing: 0.6607078586888925 Training: 0.6414937264321047 Seed: 17755\n",
      "Testing: 0.6514420196612236 Training: 0.6437581798250942 Seed: 17757\n",
      "Testing: 0.6462403471922153 Training: 0.6451944200443531 Seed: 17758\n",
      "Testing: 0.6503692692466916 Training: 0.6440575785029451 Seed: 17759\n",
      "Testing: 0.6544338079680634 Training: 0.6432447049717682 Seed: 17760\n",
      "Testing: 0.6590196275513083 Training: 0.6418999943551054 Seed: 17761\n",
      "Testing: 0.6525253206971154 Training: 0.643422720503166 Seed: 17764\n",
      "Testing: 0.6578413879177641 Training: 0.6423131978048544 Seed: 17770\n",
      "Testing: 0.6522889689835547 Training: 0.6436503543459988 Seed: 17773\n",
      "Testing: 0.6468160914349885 Training: 0.644919789937983 Seed: 17774\n",
      "Testing: 0.6502110068575908 Training: 0.6441981616097454 Seed: 17775\n",
      "Testing: 0.6480636216696192 Training: 0.6446773060724948 Seed: 17780\n",
      "Testing: 0.6695969183245333 Training: 0.639249774223672 Seed: 17781\n",
      "Testing: 0.6543179592324987 Training: 0.6429861387260845 Seed: 17782\n",
      "Testing: 0.6556831636433413 Training: 0.6426496351424396 Seed: 17784\n",
      "Testing: 0.6686718653299957 Training: 0.6393225323381904 Seed: 17787\n",
      "Testing: 0.647823792186879 Training: 0.6444845095732881 Seed: 17788\n",
      "Testing: 0.6572947477938598 Training: 0.6422833540213152 Seed: 17789\n",
      "Testing: 0.6630608743221329 Training: 0.6409554956618373 Seed: 17791\n",
      "Testing: 0.6553954933349979 Training: 0.6430098099622907 Seed: 17793\n",
      "Testing: 0.647874433123204 Training: 0.6446743225593936 Seed: 17794\n",
      "Testing: 0.6455550359015694 Training: 0.6451631338985292 Seed: 17796\n",
      "Testing: 0.6479714577401174 Training: 0.6446796768252268 Seed: 17798\n",
      "Testing: 0.6620302922533887 Training: 0.6413259044412791 Seed: 17799\n",
      "Testing: 0.6604205172579912 Training: 0.6414243953266712 Seed: 17801\n",
      "Testing: 0.6518660686660709 Training: 0.6436680782848907 Seed: 17802\n",
      "Testing: 0.6590195529553472 Training: 0.6419155164576376 Seed: 17804\n",
      "Testing: 0.656344050712335 Training: 0.6428144661989473 Seed: 17807\n",
      "Testing: 0.6471757303579622 Training: 0.6448742375519848 Seed: 17808\n",
      "Testing: 0.6457065916005847 Training: 0.6452626213563893 Seed: 17809\n",
      "Testing: 0.6511828579411341 Training: 0.6438732696096432 Seed: 17810\n",
      "Testing: 0.6633847829935482 Training: 0.6406799184834954 Seed: 17811\n",
      "Testing: 0.6473579656313905 Training: 0.6448269690493866 Seed: 17815\n",
      "Testing: 0.6510100313277046 Training: 0.6439496971587336 Seed: 17816\n",
      "Testing: 0.6510515656587393 Training: 0.6439628906367293 Seed: 17818\n",
      "Testing: 0.6518246202699388 Training: 0.6437633752599872 Seed: 17820\n",
      "Testing: 0.6636692614607278 Training: 0.6405995449560978 Seed: 17821\n",
      "Testing: 0.6473239082418946 Training: 0.6449289013429271 Seed: 17825\n",
      "Testing: 0.6578068646477884 Training: 0.6423359726102829 Seed: 17826\n",
      "Testing: 0.6520150712460405 Training: 0.6436768152785867 Seed: 17829\n",
      "Testing: 0.6554879205537498 Training: 0.6428266559075997 Seed: 17835\n",
      "Testing: 0.6460014439069156 Training: 0.6452469139956326 Seed: 17839\n",
      "Testing: 0.6611853887525548 Training: 0.641440096926857 Seed: 17840\n",
      "Testing: 0.6548801851817982 Training: 0.6430187671976932 Seed: 17841\n",
      "Testing: 0.6484383269303897 Training: 0.6445572422533032 Seed: 17847\n",
      "Testing: 0.6571341729733003 Training: 0.6424060312202174 Seed: 17853\n",
      "Testing: 0.6604127483552469 Training: 0.6417026985474974 Seed: 17854\n",
      "Testing: 0.6463914037071954 Training: 0.6451764566494718 Seed: 17855\n",
      "Testing: 0.6508101184065008 Training: 0.6439706977756785 Seed: 17860\n",
      "Testing: 0.6550230163461064 Training: 0.6428000176681852 Seed: 17865\n",
      "Testing: 0.6510243401743553 Training: 0.643934012403317 Seed: 17867\n",
      "Testing: 0.6539951058545186 Training: 0.6432108682382841 Seed: 17868\n",
      "Testing: 0.6626693042677885 Training: 0.6408930702562412 Seed: 17869\n",
      "Testing: 0.658158894505477 Training: 0.6420843715226379 Seed: 17870\n",
      "Testing: 0.6500022529687536 Training: 0.643828429722787 Seed: 17871\n",
      "Testing: 0.6464233953533898 Training: 0.6451231163019784 Seed: 17873\n",
      "Testing: 0.6552676208900677 Training: 0.6429252758156142 Seed: 17874\n",
      "Testing: 0.6474305277166943 Training: 0.6448634179705877 Seed: 17875\n",
      "Testing: 0.6455510020650269 Training: 0.6450778226719254 Seed: 17876\n",
      "Testing: 0.6543974979372084 Training: 0.6432154291079912 Seed: 17878\n",
      "Testing: 0.6534359900807478 Training: 0.6434110450163208 Seed: 17879\n",
      "Testing: 0.6501413829009489 Training: 0.6441521746091171 Seed: 17881\n",
      "Testing: 0.6579176816528378 Training: 0.6420265220560193 Seed: 17882\n",
      "Testing: 0.6554082966210557 Training: 0.6428726988510298 Seed: 17883\n",
      "Testing: 0.649211165188116 Training: 0.6445042674385308 Seed: 17884\n",
      "Testing: 0.6576609675147056 Training: 0.6420490873920284 Seed: 17885\n",
      "Testing: 0.6496207093764494 Training: 0.6443215086216196 Seed: 17886\n",
      "Testing: 0.6625525390543306 Training: 0.6409417701429222 Seed: 17887\n",
      "Testing: 0.661156383809865 Training: 0.6414678866650375 Seed: 17888\n",
      "Testing: 0.6665742842916019 Training: 0.640013521377484 Seed: 17889\n",
      "Testing: 0.648776155767086 Training: 0.6443375466928344 Seed: 17890\n",
      "Testing: 0.6511428398757801 Training: 0.6439329132596223 Seed: 17891\n",
      "Testing: 0.6674319383467278 Training: 0.6394750679838567 Seed: 17892\n",
      "Testing: 0.6685455823044792 Training: 0.6395569380492719 Seed: 17893\n",
      "Testing: 0.6594132750042124 Training: 0.6418348443289718 Seed: 17895\n",
      "Testing: 0.6582661136351386 Training: 0.6422120094763771 Seed: 17896\n",
      "Testing: 0.6461265568629463 Training: 0.6452388552093176 Seed: 17900\n",
      "Testing: 0.6459252853884839 Training: 0.6451186492631331 Seed: 17904\n",
      "Testing: 0.6625968855742036 Training: 0.641156522401039 Seed: 17907\n",
      "Testing: 0.6571743053059936 Training: 0.6424012178647095 Seed: 17908\n",
      "Testing: 0.6455070428955306 Training: 0.6451671089987535 Seed: 17909\n",
      "Testing: 0.646640752311421 Training: 0.6450119354281947 Seed: 17910\n",
      "Testing: 0.6579026796575889 Training: 0.6422329956537534 Seed: 17916\n",
      "Testing: 0.6480510491574121 Training: 0.6447304157357107 Seed: 17919\n",
      "Testing: 0.6554476388359497 Training: 0.6427894062807069 Seed: 17920\n",
      "Testing: 0.6584410627920205 Training: 0.6418925749527515 Seed: 17921\n",
      "Testing: 0.6647327583290528 Training: 0.6403395568717642 Seed: 17922\n",
      "Testing: 0.655451272956617 Training: 0.6426618806552093 Seed: 17924\n",
      "Testing: 0.6514179929933451 Training: 0.6438259430793769 Seed: 17927\n",
      "Testing: 0.6460497241235963 Training: 0.645229339871918 Seed: 17930\n",
      "Testing: 0.6480791474596971 Training: 0.6445498238540628 Seed: 17931\n",
      "Testing: 0.6482968019441585 Training: 0.6446636511760492 Seed: 17932\n",
      "Testing: 0.653693862670067 Training: 0.6432899644042219 Seed: 17933\n",
      "Testing: 0.6528618687116677 Training: 0.6433346120236816 Seed: 17938\n",
      "Testing: 0.6478006977175629 Training: 0.6447590459748809 Seed: 17942\n",
      "Testing: 0.6572918668771973 Training: 0.6425010786311455 Seed: 17953\n",
      "Testing: 0.6502319701447082 Training: 0.6441533837075488 Seed: 17954\n",
      "Testing: 0.6477661592388421 Training: 0.6448344388839873 Seed: 17958\n",
      "Testing: 0.6470832774281634 Training: 0.6448104854362848 Seed: 17959\n",
      "Testing: 0.6630038846960575 Training: 0.6410277832267129 Seed: 17960\n",
      "Testing: 0.6470227110981561 Training: 0.6448948490904801 Seed: 17964\n",
      "Testing: 0.6481384042981851 Training: 0.6446548992948719 Seed: 17968\n",
      "Testing: 0.6501333409011117 Training: 0.6442342024105291 Seed: 17969\n",
      "Testing: 0.646279761065672 Training: 0.6451794854676538 Seed: 17971\n",
      "Testing: 0.645639069147703 Training: 0.6452002300106394 Seed: 17973\n",
      "Testing: 0.6554117994238791 Training: 0.6428766496711533 Seed: 17974\n",
      "Testing: 0.6495933673782905 Training: 0.6441762347000348 Seed: 17977\n",
      "Testing: 0.6575744091341947 Training: 0.6424531670423393 Seed: 17981\n",
      "Testing: 0.6501582529974869 Training: 0.6442396932779335 Seed: 17992\n",
      "Testing: 0.6518432920193038 Training: 0.6437294551026413 Seed: 17994\n",
      "Testing: 0.6564378502832694 Training: 0.6426717701917392 Seed: 17995\n",
      "Testing: 0.6565980862229897 Training: 0.6425358867158644 Seed: 17999\n",
      "Testing: 0.6491829124973665 Training: 0.6444221142499175 Seed: 18001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6456449388390304 Training: 0.6452976623226704 Seed: 18003\n",
      "Testing: 0.6485414802224194 Training: 0.6445224555330992 Seed: 18004\n",
      "Testing: 0.6504556675926055 Training: 0.6441456399813518 Seed: 18008\n",
      "Testing: 0.6600263242740784 Training: 0.6417011439013587 Seed: 18009\n",
      "Testing: 0.6567135369865047 Training: 0.6424673013334816 Seed: 18011\n",
      "Testing: 0.6504153345119367 Training: 0.6441402663011799 Seed: 18012\n",
      "Testing: 0.6525404131273642 Training: 0.643550738484598 Seed: 18019\n",
      "Testing: 0.6538520076732759 Training: 0.6431870007274653 Seed: 18020\n",
      "Testing: 0.650601311291116 Training: 0.6440067174929875 Seed: 18024\n",
      "Testing: 0.6493962890208209 Training: 0.6443776838707519 Seed: 18025\n",
      "Testing: 0.645876222778332 Training: 0.6444361111209063 Seed: 18026\n",
      "Testing: 0.6466492723595433 Training: 0.6450650321333198 Seed: 18027\n",
      "Testing: 0.6469334599761973 Training: 0.6448512078594316 Seed: 18030\n",
      "Testing: 0.6525967041862066 Training: 0.6434066222457948 Seed: 18031\n",
      "Testing: 0.6458982011992622 Training: 0.6452056486667115 Seed: 18037\n",
      "Testing: 0.6599809043628442 Training: 0.641786044466908 Seed: 18040\n",
      "Testing: 0.6544502076418999 Training: 0.6428978570547578 Seed: 18042\n",
      "Testing: 0.6481305782337826 Training: 0.6446563689908008 Seed: 18048\n",
      "Testing: 0.6497524923997162 Training: 0.6441801298246878 Seed: 18051\n",
      "Testing: 0.6594499724845433 Training: 0.6418478145543847 Seed: 18052\n",
      "Testing: 0.6486242434922058 Training: 0.644557826065771 Seed: 18053\n",
      "Testing: 0.6492723950885435 Training: 0.644363799550812 Seed: 18054\n",
      "Testing: 0.6540027249281862 Training: 0.6432868396301246 Seed: 18055\n",
      "Testing: 0.6534810590248017 Training: 0.6433261679734532 Seed: 18058\n",
      "Testing: 0.6474176262395136 Training: 0.6448880965607293 Seed: 18063\n",
      "Testing: 0.659463005125372 Training: 0.6419511109699034 Seed: 18064\n",
      "Testing: 0.6497151985884472 Training: 0.6442358103512986 Seed: 18065\n",
      "Testing: 0.6479691228329895 Training: 0.6435840465823499 Seed: 18068\n",
      "Testing: 0.6507305363321074 Training: 0.6439895504628934 Seed: 18071\n",
      "Testing: 0.6526853603580349 Training: 0.6434612544526023 Seed: 18075\n",
      "Testing: 0.649400570730849 Training: 0.6443209784639011 Seed: 18076\n",
      "Testing: 0.6488844283932783 Training: 0.6443056640178255 Seed: 18078\n",
      "Testing: 0.6474605006523759 Training: 0.6448491099463852 Seed: 18079\n",
      "Testing: 0.648672687052498 Training: 0.6445679301770603 Seed: 18086\n",
      "Testing: 0.6691347686343025 Training: 0.6392460972203196 Seed: 18088\n",
      "Testing: 0.6530973597135549 Training: 0.643433828349461 Seed: 18090\n",
      "Testing: 0.6558994576038835 Training: 0.6428754314111509 Seed: 18092\n",
      "Testing: 0.6487377120115666 Training: 0.6445683407073999 Seed: 18093\n",
      "Testing: 0.6510126450849825 Training: 0.6438052416929769 Seed: 18094\n",
      "Testing: 0.6581742036544239 Training: 0.6422179226503685 Seed: 18096\n",
      "Testing: 0.6577706596167532 Training: 0.6422784713937928 Seed: 18097\n",
      "Testing: 0.6550814342521536 Training: 0.642643451211393 Seed: 18099\n",
      "Testing: 0.6511642925066178 Training: 0.6438228528683231 Seed: 18102\n",
      "Testing: 0.6554042977474425 Training: 0.6427846823430003 Seed: 18108\n",
      "Testing: 0.6717406929581549 Training: 0.6382970859773542 Seed: 18109\n",
      "Testing: 0.6499259908686329 Training: 0.6441393842933205 Seed: 18110\n",
      "Testing: 0.6526795618491902 Training: 0.6434835201047685 Seed: 18114\n",
      "Testing: 0.6488439853405561 Training: 0.6445480326788867 Seed: 18115\n",
      "Testing: 0.6664703126470017 Training: 0.6400958981696852 Seed: 18116\n",
      "Testing: 0.6559158921819214 Training: 0.6427126708405301 Seed: 18117\n",
      "Testing: 0.6560432336556276 Training: 0.6427215161547564 Seed: 18121\n",
      "Testing: 0.6546589265347835 Training: 0.6429531129399699 Seed: 18126\n",
      "Testing: 0.6494272147176473 Training: 0.6442800251221339 Seed: 18127\n",
      "Testing: 0.6513967545608875 Training: 0.6438001265124274 Seed: 18129\n",
      "Testing: 0.6498219376298962 Training: 0.6442680391450405 Seed: 18131\n",
      "Testing: 0.6594829475978735 Training: 0.6418138300039657 Seed: 18132\n",
      "Testing: 0.6563488287497163 Training: 0.6425813692292555 Seed: 18135\n",
      "Testing: 0.6534828765365749 Training: 0.643450037515844 Seed: 18137\n",
      "Testing: 0.6487797626733376 Training: 0.6445406978398734 Seed: 18140\n",
      "Testing: 0.6525375337843493 Training: 0.643545119414393 Seed: 18142\n",
      "Testing: 0.6480407419112411 Training: 0.6445996465516551 Seed: 18143\n",
      "Testing: 0.6547390008789848 Training: 0.6430571262233722 Seed: 18144\n",
      "Testing: 0.6454643692033957 Training: 0.6453974387978677 Seed: 18145\n",
      "Testing: 0.6593659787233204 Training: 0.6418824235469242 Seed: 18147\n",
      "Testing: 0.6595241490273891 Training: 0.6417384346691867 Seed: 18148\n",
      "Testing: 0.6616712084705041 Training: 0.6413214218352044 Seed: 18150\n",
      "Testing: 0.6492525918896944 Training: 0.6443863236224018 Seed: 18153\n",
      "Testing: 0.6557137308332799 Training: 0.6427600503308729 Seed: 18155\n",
      "Testing: 0.6505272325361467 Training: 0.6440786020278211 Seed: 18156\n",
      "Testing: 0.6510917525318203 Training: 0.6440256157490913 Seed: 18157\n",
      "Testing: 0.6511214706889864 Training: 0.6440475936902015 Seed: 18158\n",
      "Testing: 0.6475907139131329 Training: 0.6446031424649028 Seed: 18161\n",
      "Testing: 0.6509809089559684 Training: 0.6439998707754773 Seed: 18162\n",
      "Testing: 0.6484997373404109 Training: 0.6446041699910972 Seed: 18170\n",
      "Testing: 0.6576939260399629 Training: 0.6421500562223098 Seed: 18172\n",
      "Testing: 0.6484012436086318 Training: 0.6446329605483841 Seed: 18175\n",
      "Testing: 0.6652566088645975 Training: 0.6402045887017387 Seed: 18176\n",
      "Testing: 0.6516300499284075 Training: 0.6438295641273697 Seed: 18177\n",
      "Testing: 0.6480675768224217 Training: 0.644753353357681 Seed: 18178\n",
      "Testing: 0.6453953458576153 Training: 0.6453466874406575 Seed: 18179\n",
      "Testing: 0.6500516844516652 Training: 0.6442420058642229 Seed: 18180\n",
      "Testing: 0.6470261895879182 Training: 0.6449515455112547 Seed: 18181\n",
      "Testing: 0.6557524062933288 Training: 0.6426986419537077 Seed: 18182\n",
      "Testing: 0.6512416209166696 Training: 0.6436358467152583 Seed: 18183\n",
      "Testing: 0.6649924771832914 Training: 0.6404484468360541 Seed: 18184\n",
      "Testing: 0.656550298778714 Training: 0.6426082984157528 Seed: 18185\n",
      "Testing: 0.6497641792753848 Training: 0.6443041159482252 Seed: 18186\n",
      "Testing: 0.6518713148179249 Training: 0.6437259426872067 Seed: 18187\n",
      "Testing: 0.6518781139326897 Training: 0.643675689628652 Seed: 18189\n",
      "Testing: 0.6498373690781916 Training: 0.6441807161382972 Seed: 18201\n",
      "Testing: 0.6468405499238734 Training: 0.645040878222952 Seed: 18202\n",
      "Testing: 0.6471561910019646 Training: 0.6448137893589088 Seed: 18203\n",
      "Testing: 0.648204854702605 Training: 0.6446765110105169 Seed: 18208\n",
      "Testing: 0.6572574515277243 Training: 0.6423359845536323 Seed: 18209\n",
      "Testing: 0.6490498296146527 Training: 0.6444599056430549 Seed: 18210\n",
      "Testing: 0.6485666518900062 Training: 0.6445113189518686 Seed: 18212\n",
      "Testing: 0.6468665073049393 Training: 0.6449934259824627 Seed: 18215\n",
      "Testing: 0.6503921415508713 Training: 0.6441487550608305 Seed: 18217\n",
      "Testing: 0.6575571651560653 Training: 0.6423781429487359 Seed: 18218\n",
      "Testing: 0.6550172221749322 Training: 0.6429220948605305 Seed: 18219\n",
      "Testing: 0.6624712725856559 Training: 0.6409993266079016 Seed: 18220\n",
      "Testing: 0.65715675077196 Training: 0.6423168595681688 Seed: 18222\n",
      "Testing: 0.6470258032305515 Training: 0.6448323586334701 Seed: 18223\n",
      "Testing: 0.6516245748200921 Training: 0.6438351304730692 Seed: 18226\n",
      "Testing: 0.6525129514863979 Training: 0.6435530768956421 Seed: 18229\n",
      "Testing: 0.649052708770988 Training: 0.6444154420448185 Seed: 18231\n",
      "Testing: 0.6519773994874924 Training: 0.6437521601207563 Seed: 18232\n",
      "Testing: 0.660342408572446 Training: 0.6415503971968088 Seed: 18233\n",
      "Testing: 0.6513258395407049 Training: 0.6438577341732421 Seed: 18235\n",
      "Testing: 0.6495986733758163 Training: 0.6443683286851747 Seed: 18236\n",
      "Testing: 0.6504137889287603 Training: 0.6440980093049082 Seed: 18239\n",
      "Testing: 0.6477862899897291 Training: 0.644659122448913 Seed: 18244\n",
      "Testing: 0.6495136947186597 Training: 0.6443044169687109 Seed: 18246\n",
      "Testing: 0.6535932936511429 Training: 0.6434127840997617 Seed: 18247\n",
      "Testing: 0.6495349136519919 Training: 0.6443597744047627 Seed: 18248\n",
      "Testing: 0.6481029255271064 Training: 0.6446077190726351 Seed: 18249\n",
      "Testing: 0.6483539664546921 Training: 0.6446789444606204 Seed: 18251\n",
      "Testing: 0.6499296120424256 Training: 0.6441519131850935 Seed: 18252\n",
      "Testing: 0.6513235825389521 Training: 0.6438996906281818 Seed: 18253\n",
      "Testing: 0.6533212533278581 Training: 0.6434561319486399 Seed: 18255\n",
      "Testing: 0.6519538513433472 Training: 0.6437537839155532 Seed: 18256\n",
      "Testing: 0.6612800799317488 Training: 0.6414056390905208 Seed: 18258\n",
      "Testing: 0.6556010736882365 Training: 0.6428377609230516 Seed: 18262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.645374691913384 Training: 0.6453593871452897 Seed: 18264\n",
      "Testing: 0.652135513914439 Training: 0.6437046778712958 Seed: 18266\n",
      "Testing: 0.6483066385888465 Training: 0.6446342899738536 Seed: 18271\n",
      "Testing: 0.6533978249333184 Training: 0.6432620367348585 Seed: 18278\n",
      "Testing: 0.6464979653042289 Training: 0.6451530209693671 Seed: 18281\n",
      "Testing: 0.6519730795443832 Training: 0.6436728536338728 Seed: 18282\n",
      "Testing: 0.6459914908703388 Training: 0.6448943464007292 Seed: 18283\n",
      "Testing: 0.647477180239781 Training: 0.6447776550323728 Seed: 18286\n",
      "Testing: 0.6575527539425026 Training: 0.6422921315694611 Seed: 18287\n",
      "Testing: 0.6467974879414161 Training: 0.645065878722996 Seed: 18292\n",
      "Testing: 0.6613099690807621 Training: 0.6414452902147872 Seed: 18293\n",
      "Testing: 0.65031194364978 Training: 0.643984504868084 Seed: 18299\n",
      "Testing: 0.6470714286461653 Training: 0.6450322010173203 Seed: 18300\n",
      "Testing: 0.649061839423702 Training: 0.6444624459492653 Seed: 18301\n",
      "Testing: 0.6658245565494437 Training: 0.640268209717009 Seed: 18303\n",
      "Testing: 0.6617150905328293 Training: 0.641206789096719 Seed: 18304\n",
      "Testing: 0.6608358076566568 Training: 0.641483691081611 Seed: 18307\n",
      "Testing: 0.6513297346587078 Training: 0.6439573234552451 Seed: 18309\n",
      "Testing: 0.64818250857938 Training: 0.6446720358823737 Seed: 18310\n",
      "Testing: 0.6503240248296889 Training: 0.6441968875600305 Seed: 18311\n",
      "Testing: 0.6565401110119596 Training: 0.6425241227425013 Seed: 18312\n",
      "Testing: 0.6531727592841445 Training: 0.6434203101406548 Seed: 18313\n",
      "Testing: 0.6642748381042431 Training: 0.6407284606191943 Seed: 18317\n",
      "Testing: 0.6483977954146555 Training: 0.6445478995520559 Seed: 18318\n",
      "Testing: 0.6524330935347125 Training: 0.6436328704678104 Seed: 18320\n",
      "Testing: 0.6563277649132084 Training: 0.6426517958451907 Seed: 18324\n",
      "Testing: 0.6458691234544267 Training: 0.6452994788887068 Seed: 18327\n",
      "Testing: 0.6491405720114999 Training: 0.6442805099307294 Seed: 18328\n",
      "Testing: 0.6643413692428071 Training: 0.6405685596611551 Seed: 18330\n",
      "Testing: 0.6477828809636627 Training: 0.6448058833238723 Seed: 18333\n",
      "Testing: 0.6536328087805053 Training: 0.6430830118033761 Seed: 18335\n",
      "Testing: 0.6460722945545581 Training: 0.6451461037377171 Seed: 18336\n",
      "Testing: 0.6520317254435093 Training: 0.6437379771119267 Seed: 18339\n",
      "Testing: 0.651393244154673 Training: 0.6439533917144507 Seed: 18340\n",
      "Testing: 0.6544764314385381 Training: 0.6430292228804231 Seed: 18344\n",
      "Testing: 0.6472270950558532 Training: 0.6448817200911807 Seed: 18346\n",
      "Testing: 0.6738616829042002 Training: 0.6381610160346733 Seed: 18347\n",
      "Testing: 0.6545683482071446 Training: 0.6431424021556367 Seed: 18355\n",
      "Testing: 0.6523333016354738 Training: 0.6435989149660841 Seed: 18356\n",
      "Testing: 0.6512094894355381 Training: 0.6440266028474122 Seed: 18357\n",
      "Testing: 0.6547079863871699 Training: 0.6431073751027049 Seed: 18359\n",
      "Testing: 0.6535803316091785 Training: 0.6433740803497802 Seed: 18361\n",
      "Testing: 0.65254779605067 Training: 0.6436379674085151 Seed: 18362\n",
      "Testing: 0.6565269524092012 Training: 0.6425367117985806 Seed: 18363\n",
      "Testing: 0.6515805123225171 Training: 0.6435959822240771 Seed: 18364\n",
      "Testing: 0.6488905832545708 Training: 0.6445087944322985 Seed: 18366\n",
      "Testing: 0.6473457512137171 Training: 0.6449059970154596 Seed: 18368\n",
      "Testing: 0.6553725521806645 Training: 0.6429039346657277 Seed: 18370\n",
      "Testing: 0.6468142485108197 Training: 0.6449165431243762 Seed: 18372\n",
      "Testing: 0.6517087985229517 Training: 0.6438017532886303 Seed: 18373\n",
      "Testing: 0.6574748652366421 Training: 0.6423454175925206 Seed: 18374\n",
      "Testing: 0.6473266415809882 Training: 0.6448972624797218 Seed: 18375\n",
      "Testing: 0.6483799778711862 Training: 0.6446397335767443 Seed: 18376\n",
      "Testing: 0.6644791331038591 Training: 0.6406867930646378 Seed: 18377\n",
      "Testing: 0.6480234250262074 Training: 0.6446558046030636 Seed: 18386\n",
      "Testing: 0.6525530677135991 Training: 0.6434909722134605 Seed: 18387\n",
      "Testing: 0.6470907218665213 Training: 0.644817525795932 Seed: 18388\n",
      "Testing: 0.6529700025720272 Training: 0.6435123510815752 Seed: 18389\n",
      "Testing: 0.6681789331680092 Training: 0.6396885716141534 Seed: 18390\n",
      "Testing: 0.6512979436862087 Training: 0.6440118681504007 Seed: 18391\n",
      "Testing: 0.655470050778298 Training: 0.6426122694440422 Seed: 18392\n",
      "Testing: 0.6639313585121874 Training: 0.640320354875739 Seed: 18393\n",
      "Testing: 0.6489432502099314 Training: 0.6438354778097688 Seed: 18399\n",
      "Testing: 0.6514026420178735 Training: 0.6438846476904895 Seed: 18401\n",
      "Testing: 0.6502095391434866 Training: 0.6441222798372881 Seed: 18403\n",
      "Testing: 0.6586183044584749 Training: 0.6419785028134475 Seed: 18406\n",
      "Testing: 0.6539742407535724 Training: 0.6430421503308468 Seed: 18407\n",
      "Testing: 0.6476714265608986 Training: 0.6447720549201372 Seed: 18411\n",
      "Testing: 0.6570695273261201 Training: 0.6423041960638869 Seed: 18412\n",
      "Testing: 0.6533235316892566 Training: 0.6433683183463318 Seed: 18418\n",
      "Testing: 0.652673650630719 Training: 0.643606122271354 Seed: 18419\n",
      "Testing: 0.6528500076651993 Training: 0.6433922121519889 Seed: 18421\n",
      "Testing: 0.6515230911901201 Training: 0.643814263022158 Seed: 18422\n",
      "Testing: 0.6636418640233187 Training: 0.6407230524453003 Seed: 18423\n",
      "Testing: 0.6450123183968819 Training: 0.644891395392373 Seed: 18424\n",
      "Testing: 0.6551703490442442 Training: 0.6428854915304464 Seed: 18427\n",
      "Testing: 0.6584039924483653 Training: 0.642194478622062 Seed: 18430\n",
      "Testing: 0.6590164056868267 Training: 0.6419230866855627 Seed: 18432\n",
      "Testing: 0.6474980307175234 Training: 0.6448226791612143 Seed: 18434\n",
      "Testing: 0.6474427482044876 Training: 0.6447608298908944 Seed: 18435\n",
      "Testing: 0.6509445364968873 Training: 0.6440048429342783 Seed: 18436\n",
      "Testing: 0.6528974052259554 Training: 0.6433135414228496 Seed: 18438\n",
      "Testing: 0.649909378427094 Training: 0.6442945316647161 Seed: 18440\n",
      "Testing: 0.6465703378639029 Training: 0.6451209171643547 Seed: 18441\n",
      "Testing: 0.6472249664755245 Training: 0.6449350534651155 Seed: 18444\n",
      "Testing: 0.6501202534841792 Training: 0.644105095912765 Seed: 18445\n",
      "Testing: 0.6637498726537627 Training: 0.640578933011767 Seed: 18447\n",
      "Testing: 0.6506216524501699 Training: 0.6440335590522444 Seed: 18448\n",
      "Testing: 0.6474573545253779 Training: 0.6448573907972811 Seed: 18449\n",
      "Testing: 0.6553283704200936 Training: 0.6428597907443803 Seed: 18453\n",
      "Testing: 0.6499080346061877 Training: 0.6442679345016187 Seed: 18454\n",
      "Testing: 0.6511517940514604 Training: 0.6438790150484215 Seed: 18455\n",
      "Testing: 0.6511819408172537 Training: 0.6438184511465244 Seed: 18459\n",
      "Testing: 0.6688085794414389 Training: 0.6392716780691243 Seed: 18465\n",
      "Testing: 0.6559676076265835 Training: 0.6426067088757231 Seed: 18468\n",
      "Testing: 0.6528362756084838 Training: 0.6433901088822651 Seed: 18469\n",
      "Testing: 0.6510602102698511 Training: 0.6439566275246874 Seed: 18470\n",
      "Testing: 0.6633336529028752 Training: 0.6410296386635951 Seed: 18473\n",
      "Testing: 0.6575353868743451 Training: 0.6422921090109157 Seed: 18474\n",
      "Testing: 0.6637016308327969 Training: 0.6407687823187179 Seed: 18475\n",
      "Testing: 0.6457878164802451 Training: 0.6451487124405738 Seed: 18480\n",
      "Testing: 0.6466375710629588 Training: 0.6450594673936432 Seed: 18490\n",
      "Testing: 0.6541498514057639 Training: 0.643086728483507 Seed: 18491\n",
      "Testing: 0.651046259170064 Training: 0.6440724320917358 Seed: 18493\n",
      "Testing: 0.6454128805497347 Training: 0.6452458191548619 Seed: 18494\n",
      "Testing: 0.6501340540306112 Training: 0.6441455619515646 Seed: 18495\n",
      "Testing: 0.6625662410457455 Training: 0.6408855487643312 Seed: 18496\n",
      "Testing: 0.6678418240401667 Training: 0.6396694211496197 Seed: 18497\n",
      "Testing: 0.64701295741133 Training: 0.644846518507026 Seed: 18498\n",
      "Testing: 0.6497485138462413 Training: 0.6441365450445464 Seed: 18500\n",
      "Testing: 0.6467404818112116 Training: 0.6450155370777655 Seed: 18502\n",
      "Testing: 0.6502333588453559 Training: 0.64410845856675 Seed: 18503\n",
      "Testing: 0.6519717436442051 Training: 0.6438181112649629 Seed: 18506\n",
      "Testing: 0.6611177770903414 Training: 0.641370866752411 Seed: 18509\n",
      "Testing: 0.6692206175391101 Training: 0.6390710678564547 Seed: 18510\n",
      "Testing: 0.6520458048004361 Training: 0.6437444996373825 Seed: 18513\n",
      "Testing: 0.6502991223742512 Training: 0.6439486309797988 Seed: 18514\n",
      "Testing: 0.6605381690556283 Training: 0.6415511951588295 Seed: 18516\n",
      "Testing: 0.662229762783146 Training: 0.6410368150647057 Seed: 18517\n",
      "Testing: 0.6534738256882445 Training: 0.6434139609007961 Seed: 18520\n",
      "Testing: 0.6470901080592968 Training: 0.644966907555892 Seed: 18521\n",
      "Testing: 0.6563253563114068 Training: 0.6426736228125162 Seed: 18523\n",
      "Testing: 0.6588019985405972 Training: 0.6420550248044419 Seed: 18524\n",
      "Testing: 0.6467808431073591 Training: 0.6449312679467845 Seed: 18531\n",
      "Testing: 0.6468878966530772 Training: 0.6450854976138638 Seed: 18532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6502621712601693 Training: 0.6442142022196896 Seed: 18534\n",
      "Testing: 0.6528588253352662 Training: 0.6435925274250547 Seed: 18538\n",
      "Testing: 0.6498879210329704 Training: 0.6439306434709762 Seed: 18539\n",
      "Testing: 0.6650732646833999 Training: 0.6403814650405277 Seed: 18541\n",
      "Testing: 0.6591723766233746 Training: 0.6418534289329749 Seed: 18542\n",
      "Testing: 0.6453903382888762 Training: 0.6453523463836186 Seed: 18544\n",
      "Testing: 0.6526408976530278 Training: 0.6432783004366054 Seed: 18545\n",
      "Testing: 0.6494678250594983 Training: 0.6442938203969084 Seed: 18546\n",
      "Testing: 0.6649158806619443 Training: 0.6404982241730788 Seed: 18551\n",
      "Testing: 0.6463917767579922 Training: 0.6450084672694882 Seed: 18552\n",
      "Testing: 0.6562596117549575 Training: 0.6427119117441479 Seed: 18557\n",
      "Testing: 0.6504242392725551 Training: 0.6441157206399333 Seed: 18558\n",
      "Testing: 0.6496108542414694 Training: 0.6442239318419642 Seed: 18560\n",
      "Testing: 0.6651774098066306 Training: 0.6404160282132112 Seed: 18561\n",
      "Testing: 0.6540916050779934 Training: 0.6431455616222912 Seed: 18562\n",
      "Testing: 0.6539459540956607 Training: 0.6433368895014014 Seed: 18564\n",
      "Testing: 0.6468574756854863 Training: 0.6449569768768518 Seed: 18566\n",
      "Testing: 0.6508538035274402 Training: 0.643854713287652 Seed: 18568\n",
      "Testing: 0.6686599601143807 Training: 0.6394655390779271 Seed: 18571\n",
      "Testing: 0.6454627274257934 Training: 0.6453827017375174 Seed: 18574\n",
      "Testing: 0.6531973563280291 Training: 0.6434222715781771 Seed: 18575\n",
      "Testing: 0.6503084155855222 Training: 0.644057389591234 Seed: 18576\n",
      "Testing: 0.6463016728522496 Training: 0.6450937576844118 Seed: 18579\n",
      "Testing: 0.6486599858365069 Training: 0.6445015428049683 Seed: 18580\n",
      "Testing: 0.670022356678871 Training: 0.6393459609672568 Seed: 18581\n",
      "Testing: 0.6483085441057004 Training: 0.6445559575262243 Seed: 18582\n",
      "Testing: 0.6499817258153358 Training: 0.6442017350557041 Seed: 18583\n",
      "Testing: 0.6765030091761634 Training: 0.637813536708449 Seed: 18584\n",
      "Testing: 0.6682872295464057 Training: 0.6396752218612706 Seed: 18586\n",
      "Testing: 0.6512586496327172 Training: 0.6439726234665188 Seed: 18587\n",
      "Testing: 0.656660762177484 Training: 0.64257866861342 Seed: 18591\n",
      "Testing: 0.6482267724908783 Training: 0.6446782097285864 Seed: 18594\n",
      "Testing: 0.6484815039193124 Training: 0.6445507278992728 Seed: 18596\n",
      "Testing: 0.6541842624501941 Training: 0.6431448804376173 Seed: 18597\n",
      "Testing: 0.6696259909040133 Training: 0.6391456971404634 Seed: 18600\n",
      "Testing: 0.6596390113566897 Training: 0.6420101806775832 Seed: 18602\n",
      "Testing: 0.6559976142267412 Training: 0.6426816299818294 Seed: 18604\n",
      "Testing: 0.6477514749702242 Training: 0.644766347052981 Seed: 18605\n",
      "Testing: 0.6550446251182777 Training: 0.6429175407471754 Seed: 18607\n",
      "Testing: 0.6517721097738599 Training: 0.643515131941692 Seed: 18610\n",
      "Testing: 0.6663356998591665 Training: 0.6398817677062605 Seed: 18614\n",
      "Testing: 0.6518575844998163 Training: 0.6437144889352993 Seed: 18619\n",
      "Testing: 0.6469937077117979 Training: 0.6449498358833257 Seed: 18620\n",
      "Testing: 0.6566924556505964 Training: 0.6424585776627125 Seed: 18621\n",
      "Testing: 0.6455933577233894 Training: 0.645357526656418 Seed: 18622\n",
      "Testing: 0.6479191608223379 Training: 0.6447747426401778 Seed: 18623\n",
      "Testing: 0.666321354161963 Training: 0.6401910520896112 Seed: 18624\n",
      "Testing: 0.6459169986490653 Training: 0.6451384913320845 Seed: 18625\n",
      "Testing: 0.6479385897280613 Training: 0.6446136314753943 Seed: 18626\n",
      "Testing: 0.6469992570506121 Training: 0.6447879480828211 Seed: 18627\n",
      "Testing: 0.6493568636944692 Training: 0.6442665739894078 Seed: 18629\n",
      "Testing: 0.6501775165207169 Training: 0.6441718771981522 Seed: 18631\n",
      "Testing: 0.6583039219801894 Training: 0.642093384379139 Seed: 18632\n",
      "Testing: 0.6582722861333414 Training: 0.6422359637063773 Seed: 18634\n",
      "Testing: 0.6530422898104181 Training: 0.6435184265496882 Seed: 18635\n",
      "Testing: 0.6537595229369957 Training: 0.6432716607402477 Seed: 18637\n",
      "Testing: 0.6528210382145233 Training: 0.6435512035034587 Seed: 18638\n",
      "Testing: 0.6613379680299555 Training: 0.6414270818084754 Seed: 18640\n",
      "Testing: 0.6483549499590443 Training: 0.6446711653040633 Seed: 18641\n",
      "Testing: 0.648420626112212 Training: 0.6446760775410519 Seed: 18643\n",
      "Testing: 0.6584184006303697 Training: 0.6422790275129348 Seed: 18645\n",
      "Testing: 0.6520992725333271 Training: 0.6435938983938543 Seed: 18648\n",
      "Testing: 0.6491051001904357 Training: 0.6442182821761824 Seed: 18649\n",
      "Testing: 0.6501193172072537 Training: 0.644021535266526 Seed: 18650\n",
      "Testing: 0.6552499179926918 Training: 0.6430466419749077 Seed: 18651\n",
      "Testing: 0.6531994214632832 Training: 0.6430824466018392 Seed: 18653\n",
      "Testing: 0.6485505418297827 Training: 0.6445645028171132 Seed: 18656\n",
      "Testing: 0.6615713813241578 Training: 0.6412626241704212 Seed: 18657\n",
      "Testing: 0.6542054414649222 Training: 0.6431463631083398 Seed: 18660\n",
      "Testing: 0.6486104569611981 Training: 0.6445196853429263 Seed: 18664\n",
      "Testing: 0.6534337932577771 Training: 0.6434057930639857 Seed: 18666\n",
      "Testing: 0.6507014077671572 Training: 0.6440947263163298 Seed: 18667\n",
      "Testing: 0.649023263555242 Training: 0.6442405989271888 Seed: 18669\n",
      "Testing: 0.6564411320003327 Training: 0.6425587863479155 Seed: 18670\n",
      "Testing: 0.6498876724183712 Training: 0.643890294493459 Seed: 18672\n",
      "Testing: 0.6577253432717242 Training: 0.6423593560436043 Seed: 18673\n",
      "Testing: 0.6626903512562011 Training: 0.640907331084148 Seed: 18674\n",
      "Testing: 0.6456992226056242 Training: 0.6451380372474314 Seed: 18675\n",
      "Testing: 0.6611913610510687 Training: 0.6412550511505513 Seed: 18676\n",
      "Testing: 0.6506233684238419 Training: 0.6440221214519601 Seed: 18678\n",
      "Testing: 0.6588414616221829 Training: 0.6418836658421098 Seed: 18679\n",
      "Testing: 0.6471138281546724 Training: 0.64491516896737 Seed: 18683\n",
      "Testing: 0.6591953682542925 Training: 0.6418021537841734 Seed: 18687\n",
      "Testing: 0.6518101109258956 Training: 0.6438014142148623 Seed: 18689\n",
      "Testing: 0.6621745200356546 Training: 0.6410745415682821 Seed: 18692\n",
      "Testing: 0.6487249995331155 Training: 0.6444317734121905 Seed: 18694\n",
      "Testing: 0.6597918502515819 Training: 0.6417090324408158 Seed: 18695\n",
      "Testing: 0.6615417604058016 Training: 0.6410916662455153 Seed: 18700\n",
      "Testing: 0.6530361547066845 Training: 0.6434366743748909 Seed: 18701\n",
      "Testing: 0.6519342729636209 Training: 0.6432176718204663 Seed: 18703\n",
      "Testing: 0.6461831412888053 Training: 0.6451974808503903 Seed: 18706\n",
      "Testing: 0.6572156728453222 Training: 0.6422874459519836 Seed: 18708\n",
      "Testing: 0.65393946169476 Training: 0.6432202717964656 Seed: 18709\n",
      "Testing: 0.6467159810924543 Training: 0.6450608685775758 Seed: 18711\n",
      "Testing: 0.6484387406101852 Training: 0.644541674092132 Seed: 18713\n",
      "Testing: 0.6495822044107611 Training: 0.6442871354191947 Seed: 18715\n",
      "Testing: 0.6635300819912352 Training: 0.6408407678109095 Seed: 18716\n",
      "Testing: 0.651341379511015 Training: 0.6439291519209278 Seed: 18718\n",
      "Testing: 0.6640464729627288 Training: 0.6408383378873097 Seed: 18723\n",
      "Testing: 0.6552760858053205 Training: 0.6429411580295075 Seed: 18729\n",
      "Testing: 0.6485049498347469 Training: 0.6446337428957967 Seed: 18731\n",
      "Testing: 0.666309475882832 Training: 0.640105355014641 Seed: 18732\n",
      "Testing: 0.6479147731125607 Training: 0.6447868537764487 Seed: 18740\n",
      "Testing: 0.64762990181578 Training: 0.6448602818423426 Seed: 18745\n",
      "Testing: 0.6636499763019147 Training: 0.6407882055592418 Seed: 18750\n",
      "Testing: 0.6559887274802574 Training: 0.6425147754442195 Seed: 18752\n",
      "Testing: 0.6457597162347468 Training: 0.6452719953184111 Seed: 18753\n",
      "Testing: 0.6508626740134912 Training: 0.6440313173817752 Seed: 18756\n",
      "Testing: 0.6663187192609499 Training: 0.6397220388107807 Seed: 18761\n",
      "Testing: 0.6500154700705545 Training: 0.6442249239794483 Seed: 18767\n",
      "Testing: 0.6516852713808535 Training: 0.6436926468651252 Seed: 18768\n",
      "Testing: 0.6604538911816447 Training: 0.641699868732841 Seed: 18770\n",
      "Testing: 0.6528070054407558 Training: 0.64339781314592 Seed: 18771\n",
      "Testing: 0.6626161945776302 Training: 0.6408878225033137 Seed: 18773\n",
      "Testing: 0.6482021541376205 Training: 0.6446238585650351 Seed: 18774\n",
      "Testing: 0.649333845132737 Training: 0.6443655436811105 Seed: 18775\n",
      "Testing: 0.6529135191681197 Training: 0.6434899775166074 Seed: 18777\n",
      "Testing: 0.6460845265433885 Training: 0.6451907183179765 Seed: 18778\n",
      "Testing: 0.6468903760515776 Training: 0.6449358530047236 Seed: 18780\n",
      "Testing: 0.6563435268519665 Training: 0.6425939809271175 Seed: 18781\n",
      "Testing: 0.652940568018541 Training: 0.643425986563917 Seed: 18782\n",
      "Testing: 0.6546433947592095 Training: 0.6428632948520296 Seed: 18786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6548082315083781 Training: 0.6430318086537083 Seed: 18788\n",
      "Testing: 0.6596875669280102 Training: 0.6416762001857437 Seed: 18791\n",
      "Testing: 0.6648169252381886 Training: 0.6403760478167453 Seed: 18792\n",
      "Testing: 0.6588016350629591 Training: 0.6416021728503785 Seed: 18794\n",
      "Testing: 0.6453459839353162 Training: 0.6451174138983617 Seed: 18797\n",
      "Testing: 0.6708523390937173 Training: 0.6391760970283008 Seed: 18800\n",
      "Testing: 0.6528534301468409 Training: 0.6434489127393654 Seed: 18804\n",
      "Testing: 0.6559739780864499 Training: 0.6425395398998011 Seed: 18807\n",
      "Testing: 0.6502291780695253 Training: 0.6441459312256541 Seed: 18809\n",
      "Testing: 0.6527378794251418 Training: 0.6432184610970408 Seed: 18811\n",
      "Testing: 0.6513464173242691 Training: 0.6438502671985629 Seed: 18812\n",
      "Testing: 0.6523877839140815 Training: 0.6436694592079782 Seed: 18813\n",
      "Testing: 0.6628649385336679 Training: 0.6410541206063798 Seed: 18815\n",
      "Testing: 0.6529910458702164 Training: 0.643509482761653 Seed: 18819\n",
      "Testing: 0.6459882367081798 Training: 0.6451425314717696 Seed: 18820\n",
      "Testing: 0.6455168392186494 Training: 0.6453162905781109 Seed: 18822\n",
      "Testing: 0.6577524540776243 Training: 0.6424805839423595 Seed: 18823\n",
      "Testing: 0.6494807262005414 Training: 0.6442509551870499 Seed: 18825\n",
      "Testing: 0.6530600147385338 Training: 0.6433317238505742 Seed: 18826\n",
      "Testing: 0.6457034823860771 Training: 0.645342760995886 Seed: 18828\n",
      "Testing: 0.6477561112209442 Training: 0.6446413436486419 Seed: 18830\n",
      "Testing: 0.6609952762769398 Training: 0.6415277405285241 Seed: 18831\n",
      "Testing: 0.6528797323887804 Training: 0.6434921349095643 Seed: 18832\n",
      "Testing: 0.6478042122397101 Training: 0.64470606249898 Seed: 18835\n",
      "Testing: 0.6464663202473792 Training: 0.6450624628191403 Seed: 18838\n",
      "Testing: 0.6473201846957557 Training: 0.6447807969433096 Seed: 18839\n",
      "Testing: 0.6472424277074912 Training: 0.644924205736387 Seed: 18840\n",
      "Testing: 0.6588800685731717 Training: 0.6420155360473647 Seed: 18842\n",
      "Testing: 0.653382880100782 Training: 0.6433199032583111 Seed: 18845\n",
      "Testing: 0.6459331857002449 Training: 0.6451923606376233 Seed: 18847\n",
      "Testing: 0.6537714103051787 Training: 0.6433004409707457 Seed: 18848\n",
      "Testing: 0.6467611874139887 Training: 0.6449213874102906 Seed: 18852\n",
      "Testing: 0.6455504728882957 Training: 0.6453349080593459 Seed: 18854\n",
      "Testing: 0.6533036776621952 Training: 0.6434579726250996 Seed: 18857\n",
      "Testing: 0.6456111408517651 Training: 0.6453182044891046 Seed: 18858\n",
      "Testing: 0.6485837118474624 Training: 0.6444016948848053 Seed: 18860\n",
      "Testing: 0.6489340207259099 Training: 0.6445414386036679 Seed: 18862\n",
      "Testing: 0.650060019596154 Training: 0.6442311821520098 Seed: 18863\n",
      "Testing: 0.6488412626834417 Training: 0.6445238051696331 Seed: 18864\n",
      "Testing: 0.6461602988317393 Training: 0.6451410506844528 Seed: 18866\n",
      "Testing: 0.6468977957675436 Training: 0.6450361185375539 Seed: 18868\n",
      "Testing: 0.6608270022462293 Training: 0.6414265102416417 Seed: 18869\n",
      "Testing: 0.6494316404262457 Training: 0.6443218133455014 Seed: 18871\n",
      "Testing: 0.6504263455664318 Training: 0.644096714041369 Seed: 18877\n",
      "Testing: 0.6484946776492843 Training: 0.6443936826944135 Seed: 18878\n",
      "Testing: 0.6532900277926632 Training: 0.6433737556230148 Seed: 18882\n",
      "Testing: 0.6558028031709087 Training: 0.6425417014308497 Seed: 18883\n",
      "Testing: 0.6580665976276239 Training: 0.6422602121506235 Seed: 18888\n",
      "Testing: 0.654017926618379 Training: 0.6431925526615587 Seed: 18889\n",
      "Testing: 0.6500894105722337 Training: 0.6441532835041405 Seed: 18891\n",
      "Testing: 0.6500381857056226 Training: 0.6438677157163936 Seed: 18893\n",
      "Testing: 0.6637020373011845 Training: 0.6409080169140835 Seed: 18894\n",
      "Testing: 0.6498720235938719 Training: 0.6442254822139664 Seed: 18897\n",
      "Testing: 0.645541304590359 Training: 0.6453812552461156 Seed: 18898\n",
      "Testing: 0.6504139958039689 Training: 0.644150772194419 Seed: 18899\n",
      "Testing: 0.6648878415205279 Training: 0.6406335423175556 Seed: 18902\n",
      "Testing: 0.6588005374821961 Training: 0.6418973793982076 Seed: 18903\n",
      "Testing: 0.6545247128529652 Training: 0.6432244849558337 Seed: 18904\n",
      "Testing: 0.6485063344148653 Training: 0.644586644167908 Seed: 18906\n",
      "Testing: 0.6475966230693893 Training: 0.6449070769577171 Seed: 18911\n",
      "Testing: 0.647652911082889 Training: 0.6448013890905917 Seed: 18913\n",
      "Testing: 0.647302203460156 Training: 0.6449222821287547 Seed: 18916\n",
      "Testing: 0.65053651103855 Training: 0.6440594085338298 Seed: 18920\n",
      "Testing: 0.6520519136513885 Training: 0.643771028624873 Seed: 18923\n",
      "Testing: 0.6630618570071181 Training: 0.6408577412956886 Seed: 18924\n",
      "Testing: 0.6691898230896649 Training: 0.6397366430019584 Seed: 18926\n",
      "Testing: 0.6558453631439344 Training: 0.642586166984467 Seed: 18931\n",
      "Testing: 0.6490057415741883 Training: 0.6444006355040228 Seed: 18933\n",
      "Testing: 0.6538411971762004 Training: 0.6433034113208627 Seed: 18935\n",
      "Testing: 0.6498998207148812 Training: 0.6442285279041509 Seed: 18937\n",
      "Testing: 0.6496473183946786 Training: 0.6442770387723588 Seed: 18947\n",
      "Testing: 0.6605267556960666 Training: 0.6417657323800627 Seed: 18949\n",
      "Testing: 0.6681384903096743 Training: 0.6393428921256399 Seed: 18952\n",
      "Testing: 0.6470904367505474 Training: 0.6449290231168908 Seed: 18953\n",
      "Testing: 0.649279226535052 Training: 0.6444378815151283 Seed: 18954\n",
      "Testing: 0.6525757464775724 Training: 0.6436190674904277 Seed: 18955\n",
      "Testing: 0.6599391136482796 Training: 0.64165232173003 Seed: 18956\n",
      "Testing: 0.645887295165362 Training: 0.645292205329674 Seed: 18957\n",
      "Testing: 0.6492751758925945 Training: 0.6443656655738644 Seed: 18958\n",
      "Testing: 0.6463393988308956 Training: 0.6451340428741188 Seed: 18959\n",
      "Testing: 0.6565512362915098 Training: 0.6424215408390895 Seed: 18960\n",
      "Testing: 0.6470616471196157 Training: 0.6439708279799635 Seed: 18965\n",
      "Testing: 0.6511966168098149 Training: 0.6438177934471246 Seed: 18966\n",
      "Testing: 0.6508543546957997 Training: 0.6440112330039067 Seed: 18968\n",
      "Testing: 0.6480337819804693 Training: 0.6447170573564439 Seed: 18973\n",
      "Testing: 0.6569348065108942 Training: 0.6424522721449419 Seed: 18974\n",
      "Testing: 0.6559840545977356 Training: 0.6427045028439431 Seed: 18977\n",
      "Testing: 0.6586747741024331 Training: 0.6421188638178293 Seed: 18980\n",
      "Testing: 0.650703513771524 Training: 0.6440584000972244 Seed: 18981\n",
      "Testing: 0.645639340824522 Training: 0.6453097442073836 Seed: 18982\n",
      "Testing: 0.6586539647055134 Training: 0.642162071284896 Seed: 18983\n",
      "Testing: 0.6538571477344426 Training: 0.6432568450069778 Seed: 18984\n",
      "Testing: 0.6461545775777007 Training: 0.6451621748488268 Seed: 18986\n",
      "Testing: 0.6550380661973362 Training: 0.6429375654284332 Seed: 18987\n",
      "Testing: 0.6461668828906615 Training: 0.6451839880553942 Seed: 18988\n",
      "Testing: 0.651081472940373 Training: 0.643942055743738 Seed: 18989\n",
      "Testing: 0.6606960135494812 Training: 0.6416495728220042 Seed: 18990\n",
      "Testing: 0.648741975263508 Training: 0.6443755868533937 Seed: 18992\n",
      "Testing: 0.6599159258243862 Training: 0.6417358732687489 Seed: 18995\n",
      "Testing: 0.6505874615315943 Training: 0.6441310120711975 Seed: 18996\n",
      "Testing: 0.6584491212481547 Training: 0.6421332862617919 Seed: 18999\n",
      "Testing: 0.6523397115164423 Training: 0.6436639147265004 Seed: 19002\n",
      "Testing: 0.6511934100430637 Training: 0.643924285263318 Seed: 19004\n",
      "Testing: 0.6508076678285236 Training: 0.6437610089137394 Seed: 19005\n",
      "Testing: 0.6462321493199754 Training: 0.644768263747 Seed: 19009\n",
      "Testing: 0.6524166629736591 Training: 0.6437402519122227 Seed: 19010\n",
      "Testing: 0.6544831511466658 Training: 0.6429577071200037 Seed: 19011\n",
      "Testing: 0.6473867597021694 Training: 0.6448634739936787 Seed: 19012\n",
      "Testing: 0.6504757799389329 Training: 0.6439659752790877 Seed: 19013\n",
      "Testing: 0.6489292180157107 Training: 0.6444373151394438 Seed: 19014\n",
      "Testing: 0.6485801018336483 Training: 0.6444718252234345 Seed: 19015\n",
      "Testing: 0.6459430406646636 Training: 0.6452002672044368 Seed: 19018\n",
      "Testing: 0.649980521201994 Training: 0.6442892566378506 Seed: 19021\n",
      "Testing: 0.650349899257683 Training: 0.6437335832283381 Seed: 19022\n",
      "Testing: 0.6472555795563747 Training: 0.6449657669676312 Seed: 19026\n",
      "Testing: 0.6491175839010399 Training: 0.6444584981346795 Seed: 19028\n",
      "Testing: 0.6509389242981964 Training: 0.6440246152213982 Seed: 19037\n",
      "Testing: 0.6521703416602254 Training: 0.6436468556407859 Seed: 19038\n",
      "Testing: 0.6510142244769505 Training: 0.6439136029484918 Seed: 19042\n",
      "Testing: 0.6543530007967789 Training: 0.6432790961682127 Seed: 19043\n",
      "Testing: 0.6521650078194016 Training: 0.6436796198569936 Seed: 19047\n",
      "Testing: 0.6526935316552833 Training: 0.643348440948502 Seed: 19048\n",
      "Testing: 0.6576162654291571 Training: 0.6423487037393252 Seed: 19049\n",
      "Testing: 0.6472233790066082 Training: 0.6448286466616124 Seed: 19050\n",
      "Testing: 0.6459727379553692 Training: 0.6452376969157716 Seed: 19052\n",
      "Testing: 0.6565457993125178 Training: 0.6426116907273881 Seed: 19054\n",
      "Testing: 0.6582562694854903 Training: 0.6421362172792309 Seed: 19055\n",
      "Testing: 0.6484701099083977 Training: 0.6446345553474887 Seed: 19056\n",
      "Testing: 0.6557142462585688 Training: 0.642652959324616 Seed: 19057\n",
      "Testing: 0.6556339362602953 Training: 0.6425953620204283 Seed: 19058\n",
      "Testing: 0.6455682990887992 Training: 0.6443745953903753 Seed: 19060\n",
      "Testing: 0.6523349654656933 Training: 0.6436447815963182 Seed: 19062\n",
      "Testing: 0.6484455197143417 Training: 0.6446778330322644 Seed: 19063\n",
      "Testing: 0.6522531957949755 Training: 0.6435493963808013 Seed: 19064\n",
      "Testing: 0.6501405096939348 Training: 0.6441895771639289 Seed: 19066\n",
      "Testing: 0.645880085222327 Training: 0.6451790234575738 Seed: 19067\n",
      "Testing: 0.6533967044550375 Training: 0.6433061094313413 Seed: 19069\n",
      "Testing: 0.6605299408550583 Training: 0.6415098684641856 Seed: 19071\n",
      "Testing: 0.6632879657741705 Training: 0.64104788956448 Seed: 19073\n",
      "Testing: 0.6593334767942554 Training: 0.642029650741766 Seed: 19074\n",
      "Testing: 0.6578355901093278 Training: 0.6422962547795515 Seed: 19078\n",
      "Testing: 0.6501821126071218 Training: 0.643460498106133 Seed: 19079\n",
      "Testing: 0.6559409181443137 Training: 0.6428128263341062 Seed: 19080\n",
      "Testing: 0.6467066941454646 Training: 0.6448589246089713 Seed: 19081\n",
      "Testing: 0.6601743136700928 Training: 0.6417479947693837 Seed: 19083\n",
      "Testing: 0.6488135903603209 Training: 0.6445482832741809 Seed: 19084\n",
      "Testing: 0.6486320221043319 Training: 0.6446165006350251 Seed: 19085\n",
      "Testing: 0.6638109733695319 Training: 0.6406955422742673 Seed: 19086\n",
      "Testing: 0.6519526236478042 Training: 0.6437372495075582 Seed: 19087\n",
      "Testing: 0.6485562818261459 Training: 0.6446002510704162 Seed: 19093\n",
      "Testing: 0.6520336826609994 Training: 0.6437464891410102 Seed: 19095\n",
      "Testing: 0.6732729915132729 Training: 0.6383321999641511 Seed: 19096\n",
      "Testing: 0.6538414822505894 Training: 0.6432824614598163 Seed: 19102\n",
      "Testing: 0.6571396559073064 Training: 0.6423970294665177 Seed: 19103\n",
      "Testing: 0.6537172017072352 Training: 0.6434002350575656 Seed: 19104\n",
      "Testing: 0.6578734270254429 Training: 0.6419540225673079 Seed: 19107\n",
      "Testing: 0.6623433594618339 Training: 0.6409995422638168 Seed: 19109\n",
      "Testing: 0.6487384856727326 Training: 0.6444906815197768 Seed: 19111\n",
      "Testing: 0.649660514095256 Training: 0.6442431274457923 Seed: 19112\n",
      "Testing: 0.6620882528830567 Training: 0.6413412615969595 Seed: 19114\n",
      "Testing: 0.6514150395490415 Training: 0.6439630045985789 Seed: 19119\n",
      "Testing: 0.6597402206071558 Training: 0.6415780370853708 Seed: 19123\n",
      "Testing: 0.6496411301644779 Training: 0.6442714542229966 Seed: 19127\n",
      "Testing: 0.6502450386717401 Training: 0.6440662651560324 Seed: 19128\n",
      "Testing: 0.6529732528648404 Training: 0.6434768227424874 Seed: 19130\n",
      "Testing: 0.6599761630735428 Training: 0.6417059246365397 Seed: 19131\n",
      "Testing: 0.6502972475236863 Training: 0.6439663179682475 Seed: 19132\n",
      "Testing: 0.6527492760015539 Training: 0.6434091284008316 Seed: 19134\n",
      "Testing: 0.6491084893178786 Training: 0.6438024526386543 Seed: 19135\n",
      "Testing: 0.6546715350559068 Training: 0.6430927141405134 Seed: 19136\n",
      "Testing: 0.6486182763495454 Training: 0.644662682805577 Seed: 19137\n",
      "Testing: 0.6508662950379502 Training: 0.6434466507936865 Seed: 19138\n",
      "Testing: 0.6541179903011227 Training: 0.6431141642090146 Seed: 19142\n",
      "Testing: 0.6543019858323544 Training: 0.6431665241431375 Seed: 19145\n",
      "Testing: 0.6517929767701465 Training: 0.6434367936717797 Seed: 19146\n",
      "Testing: 0.6512560260389593 Training: 0.6438513472116865 Seed: 19147\n",
      "Testing: 0.6506889181420366 Training: 0.6440422874817202 Seed: 19150\n",
      "Testing: 0.6588859593556301 Training: 0.6419533123621273 Seed: 19151\n",
      "Testing: 0.6548420746950007 Training: 0.6428455129245583 Seed: 19154\n",
      "Testing: 0.6454317861513361 Training: 0.6452823224392513 Seed: 19155\n",
      "Testing: 0.6484622390425095 Training: 0.6445189104552356 Seed: 19156\n",
      "Testing: 0.6527956387925443 Training: 0.6435697898007642 Seed: 19157\n",
      "Testing: 0.6500471218099911 Training: 0.6442656624555174 Seed: 19159\n",
      "Testing: 0.6556711043496813 Training: 0.6427849873988756 Seed: 19162\n",
      "Testing: 0.6516075998204992 Training: 0.6438172118135039 Seed: 19164\n",
      "Testing: 0.6528577436604477 Training: 0.643421019509379 Seed: 19165\n",
      "Testing: 0.6462734753153745 Training: 0.6452317036996107 Seed: 19166\n",
      "Testing: 0.6460522244633037 Training: 0.6451466065018142 Seed: 19167\n",
      "Testing: 0.6465482233603645 Training: 0.6450276325121903 Seed: 19168\n",
      "Testing: 0.6613544023285346 Training: 0.6414615381180531 Seed: 19171\n",
      "Testing: 0.673426903585072 Training: 0.6386143271259881 Seed: 19173\n",
      "Testing: 0.6469555198018502 Training: 0.644936494112905 Seed: 19174\n",
      "Testing: 0.6586207975061852 Training: 0.641875031076166 Seed: 19176\n",
      "Testing: 0.6547391012212693 Training: 0.6429224176771926 Seed: 19181\n",
      "Testing: 0.6551498820413006 Training: 0.6428456419450284 Seed: 19183\n",
      "Testing: 0.6481592306526568 Training: 0.6446777976531708 Seed: 19185\n",
      "Testing: 0.6467914573912344 Training: 0.6450516501654762 Seed: 19186\n",
      "Testing: 0.6588544548777412 Training: 0.6418501980175637 Seed: 19189\n",
      "Testing: 0.6734914528291862 Training: 0.6380730266724293 Seed: 19191\n",
      "Testing: 0.653563778610723 Training: 0.6430850192994382 Seed: 19196\n",
      "Testing: 0.6571664878534949 Training: 0.6424622030234933 Seed: 19197\n",
      "Testing: 0.6516066021958322 Training: 0.6438249452175498 Seed: 19198\n",
      "Testing: 0.6556815293628407 Training: 0.6428303962028987 Seed: 19200\n",
      "Testing: 0.6578735317717307 Training: 0.6423697444908151 Seed: 19204\n",
      "Testing: 0.6593344603189704 Training: 0.6419157783384792 Seed: 19206\n",
      "Testing: 0.6571546511904579 Training: 0.6424191014619376 Seed: 19207\n",
      "Testing: 0.6624755607790052 Training: 0.6409136741891039 Seed: 19208\n",
      "Testing: 0.6468246120116212 Training: 0.6449691540706233 Seed: 19209\n",
      "Testing: 0.6529068152705271 Training: 0.6436292525210934 Seed: 19210\n",
      "Testing: 0.6462767452688751 Training: 0.6452317028901693 Seed: 19212\n",
      "Testing: 0.6538249370988566 Training: 0.6432975688546977 Seed: 19213\n",
      "Testing: 0.6461106803492508 Training: 0.6451938115020801 Seed: 19214\n",
      "Testing: 0.6689629147247039 Training: 0.6392430435889334 Seed: 19218\n",
      "Testing: 0.6503750926944007 Training: 0.6439494962491816 Seed: 19220\n",
      "Testing: 0.6497052282140734 Training: 0.6442809526830404 Seed: 19224\n",
      "Testing: 0.6479796578041048 Training: 0.6446025229794954 Seed: 19225\n",
      "Testing: 0.6582646441557527 Training: 0.6421402992395724 Seed: 19230\n",
      "Testing: 0.6471203206909288 Training: 0.6449985960190845 Seed: 19233\n",
      "Testing: 0.6459924215740339 Training: 0.6451805101758887 Seed: 19235\n",
      "Testing: 0.6507810084256442 Training: 0.6438758871126339 Seed: 19238\n",
      "Testing: 0.6550559927759887 Training: 0.6429396559795821 Seed: 19241\n",
      "Testing: 0.6459842429279771 Training: 0.6450177557438158 Seed: 19242\n",
      "Testing: 0.6481955355095889 Training: 0.6443014475929973 Seed: 19250\n",
      "Testing: 0.6562791181934369 Training: 0.6426206785393203 Seed: 19252\n",
      "Testing: 0.6500977637979537 Training: 0.6442185663929171 Seed: 19254\n",
      "Testing: 0.648581058602252 Training: 0.6445900326439793 Seed: 19256\n",
      "Testing: 0.6482903686540233 Training: 0.6446981252451687 Seed: 19259\n",
      "Testing: 0.6619602830956104 Training: 0.6411982720312523 Seed: 19260\n",
      "Testing: 0.6463751545186416 Training: 0.6451338718563655 Seed: 19263\n",
      "Testing: 0.6529973432269045 Training: 0.6434952216713375 Seed: 19264\n",
      "Testing: 0.6622336293703974 Training: 0.6408993266618779 Seed: 19266\n",
      "Testing: 0.6470385252541853 Training: 0.6448851324016304 Seed: 19269\n",
      "Testing: 0.6547568661075468 Training: 0.6430407989717852 Seed: 19270\n",
      "Testing: 0.6470727990918207 Training: 0.6449596858244578 Seed: 19275\n",
      "Testing: 0.6460606499865076 Training: 0.6452484592448484 Seed: 19276\n",
      "Testing: 0.6488052370106765 Training: 0.6445792748273724 Seed: 19277\n",
      "Testing: 0.6568050599291627 Training: 0.6424979006467975 Seed: 19280\n",
      "Testing: 0.6559460923970402 Training: 0.6427746988760139 Seed: 19281\n",
      "Testing: 0.6536691843048338 Training: 0.6432710890010509 Seed: 19283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6533148595301042 Training: 0.6433010914504566 Seed: 19285\n",
      "Testing: 0.658224748803964 Training: 0.6416176887690404 Seed: 19287\n",
      "Testing: 0.6596682886383798 Training: 0.6416511899679056 Seed: 19288\n",
      "Testing: 0.6481065493966247 Training: 0.644564246386446 Seed: 19290\n",
      "Testing: 0.6485000079746015 Training: 0.6445732385954007 Seed: 19293\n",
      "Testing: 0.6644661340833856 Training: 0.6406280310361782 Seed: 19296\n",
      "Testing: 0.6507917088779261 Training: 0.6440855823279041 Seed: 19297\n",
      "Testing: 0.6659232627662759 Training: 0.6401413255001771 Seed: 19301\n",
      "Testing: 0.6601802366191691 Training: 0.6415540799626871 Seed: 19305\n",
      "Testing: 0.6459773987735287 Training: 0.6453069838813671 Seed: 19307\n",
      "Testing: 0.6550542691886994 Training: 0.6429835355268699 Seed: 19310\n",
      "Testing: 0.6515872388698444 Training: 0.6435831007828081 Seed: 19311\n",
      "Testing: 0.650059660101378 Training: 0.6442692044942384 Seed: 19313\n",
      "Testing: 0.6461433816790184 Training: 0.6452391143889645 Seed: 19315\n",
      "Testing: 0.6630187262514249 Training: 0.6410444522817504 Seed: 19316\n",
      "Testing: 0.6509047573496746 Training: 0.643934098696655 Seed: 19317\n",
      "Testing: 0.647191063938259 Training: 0.644943905815382 Seed: 19318\n",
      "Testing: 0.6577574087185591 Training: 0.6422933630094724 Seed: 19319\n",
      "Testing: 0.6520552196078349 Training: 0.643653425607096 Seed: 19320\n",
      "Testing: 0.6628472859697949 Training: 0.6408567683962616 Seed: 19321\n",
      "Testing: 0.6460202792216063 Training: 0.6450914982590803 Seed: 19322\n",
      "Testing: 0.654616593422851 Training: 0.6429374397207606 Seed: 19323\n",
      "Testing: 0.6497763989176304 Training: 0.6442445703452548 Seed: 19324\n",
      "Testing: 0.6654141031120813 Training: 0.6403006653299289 Seed: 19325\n",
      "Testing: 0.656568798930397 Training: 0.6423516504454279 Seed: 19332\n",
      "Testing: 0.664177014113378 Training: 0.6407478847163749 Seed: 19335\n",
      "Testing: 0.6479381474609454 Training: 0.6446589954293962 Seed: 19336\n",
      "Testing: 0.6501497533469849 Training: 0.6441212487084211 Seed: 19338\n",
      "Testing: 0.6492672448390114 Training: 0.6444127078909861 Seed: 19347\n",
      "Testing: 0.649736283749585 Training: 0.6443271627819251 Seed: 19348\n",
      "Testing: 0.6534362234449409 Training: 0.6427177355258756 Seed: 19350\n",
      "Testing: 0.6479918035614134 Training: 0.6445909933277055 Seed: 19351\n",
      "Testing: 0.6467444194961555 Training: 0.6448066272352868 Seed: 19353\n",
      "Testing: 0.6672265058390339 Training: 0.6399273792129968 Seed: 19354\n",
      "Testing: 0.6505722178163771 Training: 0.6440468003169697 Seed: 19356\n",
      "Testing: 0.6478566874642958 Training: 0.644784857519273 Seed: 19357\n",
      "Testing: 0.6506966126278675 Training: 0.6439947932595956 Seed: 19364\n",
      "Testing: 0.6482836122679596 Training: 0.644655239899018 Seed: 19369\n",
      "Testing: 0.6647374927994598 Training: 0.6405704769093976 Seed: 19370\n",
      "Testing: 0.6611687378520759 Training: 0.6413661453402753 Seed: 19377\n",
      "Testing: 0.6475314944978978 Training: 0.644887768613496 Seed: 19378\n",
      "Testing: 0.649464286312991 Training: 0.6443231609889468 Seed: 19379\n",
      "Testing: 0.6567999788447954 Training: 0.6426353308387467 Seed: 19380\n",
      "Testing: 0.6517829488518375 Training: 0.643639396300538 Seed: 19383\n",
      "Testing: 0.6557723263780855 Training: 0.6427641824831332 Seed: 19385\n",
      "Testing: 0.6473213720485081 Training: 0.6448667066666386 Seed: 19390\n",
      "Testing: 0.6457419378657132 Training: 0.6452029130087736 Seed: 19394\n",
      "Testing: 0.654121953218691 Training: 0.6430561932200171 Seed: 19396\n",
      "Testing: 0.6642320445453174 Training: 0.6407144269568561 Seed: 19399\n",
      "Testing: 0.6476112645938119 Training: 0.6448163410045524 Seed: 19400\n",
      "Testing: 0.6459967423337276 Training: 0.6452036690659843 Seed: 19401\n",
      "Testing: 0.6460640420212098 Training: 0.6451783131663995 Seed: 19402\n",
      "Testing: 0.6547106297371892 Training: 0.6431354079577547 Seed: 19405\n",
      "Testing: 0.6594002528848494 Training: 0.6418668112586767 Seed: 19406\n",
      "Testing: 0.6585170230830737 Training: 0.6419923714110876 Seed: 19410\n",
      "Testing: 0.6529322627430187 Training: 0.6434386106895038 Seed: 19415\n",
      "Testing: 0.6553442932198091 Training: 0.6429322604580681 Seed: 19416\n",
      "Testing: 0.649902155841139 Training: 0.6442157814209044 Seed: 19417\n",
      "Testing: 0.6469518087006028 Training: 0.6449784707375429 Seed: 19418\n",
      "Testing: 0.6548780988009889 Training: 0.6428872015536221 Seed: 19419\n",
      "Testing: 0.6513125070240271 Training: 0.6438794309010015 Seed: 19420\n",
      "Testing: 0.6520863397526846 Training: 0.6436480560031206 Seed: 19423\n",
      "Testing: 0.6556157352296622 Training: 0.6428487934985261 Seed: 19424\n",
      "Testing: 0.6607840945669388 Training: 0.6414813443853482 Seed: 19426\n",
      "Testing: 0.6566667827937074 Training: 0.6424617229053777 Seed: 19427\n",
      "Testing: 0.6561354767459273 Training: 0.6426529514428532 Seed: 19428\n",
      "Testing: 0.6606666087941416 Training: 0.6414604351751945 Seed: 19429\n",
      "Testing: 0.6497161571401915 Training: 0.6443173034012251 Seed: 19430\n",
      "Testing: 0.6507298835100498 Training: 0.6441146307979908 Seed: 19431\n",
      "Testing: 0.6508477284366728 Training: 0.643991251649115 Seed: 19433\n",
      "Testing: 0.6588315898642509 Training: 0.6421113663197969 Seed: 19434\n",
      "Testing: 0.6457099100091365 Training: 0.6452890295393175 Seed: 19435\n",
      "Testing: 0.6602690237319843 Training: 0.6418235337608587 Seed: 19439\n",
      "Testing: 0.646297489679863 Training: 0.6451582270095427 Seed: 19442\n",
      "Testing: 0.6509307484422829 Training: 0.6439436321085825 Seed: 19444\n",
      "Testing: 0.6594263654100665 Training: 0.6417721712354655 Seed: 19447\n",
      "Testing: 0.6516838763269316 Training: 0.6437109718618771 Seed: 19448\n",
      "Testing: 0.649384247794091 Training: 0.6443342465336284 Seed: 19450\n",
      "Testing: 0.6712577476954418 Training: 0.638760864914061 Seed: 19452\n",
      "Testing: 0.6537834349394249 Training: 0.6432765811449828 Seed: 19456\n",
      "Testing: 0.6542747950458723 Training: 0.6430319456085023 Seed: 19461\n",
      "Testing: 0.6460761889271384 Training: 0.6451581225552014 Seed: 19462\n",
      "Testing: 0.6584259901729412 Training: 0.6419666049023263 Seed: 19463\n",
      "Testing: 0.6591223653022927 Training: 0.6418664613130985 Seed: 19475\n",
      "Testing: 0.6588828717635222 Training: 0.6419416665439419 Seed: 19477\n",
      "Testing: 0.6536869090904841 Training: 0.6425548359112069 Seed: 19478\n",
      "Testing: 0.6518353960187698 Training: 0.643741559819287 Seed: 19480\n",
      "Testing: 0.648535562249841 Training: 0.6446564071923935 Seed: 19481\n",
      "Testing: 0.6509885732203924 Training: 0.6439835421276991 Seed: 19482\n",
      "Testing: 0.6669124552010501 Training: 0.639901813441886 Seed: 19489\n",
      "Testing: 0.6472764286418202 Training: 0.6448865554388795 Seed: 19496\n",
      "Testing: 0.6505460541874208 Training: 0.6440616523258887 Seed: 19502\n",
      "Testing: 0.6530460733388921 Training: 0.6433819369149754 Seed: 19503\n",
      "Testing: 0.6474165624004085 Training: 0.6448488790446651 Seed: 19511\n",
      "Testing: 0.6464491845667567 Training: 0.644792183679682 Seed: 19514\n",
      "Testing: 0.6530710233145574 Training: 0.6435582805494433 Seed: 19515\n",
      "Testing: 0.6568578128666347 Training: 0.6424111544155863 Seed: 19516\n",
      "Testing: 0.6545632088795255 Training: 0.6430979111837146 Seed: 19525\n",
      "Testing: 0.6531604362973772 Training: 0.6434581896842222 Seed: 19526\n",
      "Testing: 0.6506492744342077 Training: 0.6440074092942668 Seed: 19528\n",
      "Testing: 0.6506464239242169 Training: 0.6440369835550024 Seed: 19529\n",
      "Testing: 0.6579492101672582 Training: 0.6422727240440452 Seed: 19530\n",
      "Testing: 0.6629489519302447 Training: 0.6408782910954313 Seed: 19533\n",
      "Testing: 0.6499753752718652 Training: 0.6442085770746375 Seed: 19537\n",
      "Testing: 0.6567046245928061 Training: 0.6424474490883252 Seed: 19539\n",
      "Testing: 0.6633297012205875 Training: 0.6409546029240292 Seed: 19542\n",
      "Testing: 0.6598914859831105 Training: 0.6417576934738795 Seed: 19546\n",
      "Testing: 0.6552110738315692 Training: 0.6428856316426417 Seed: 19558\n",
      "Testing: 0.6488841663929584 Training: 0.6444814794441873 Seed: 19560\n",
      "Testing: 0.66275454887027 Training: 0.6408933643778074 Seed: 19562\n",
      "Testing: 0.6479975703671506 Training: 0.6446693814679316 Seed: 19564\n",
      "Testing: 0.6460716492249039 Training: 0.6451132837411658 Seed: 19566\n",
      "Testing: 0.6551510862305152 Training: 0.6430266718567421 Seed: 19567\n",
      "Testing: 0.6543090234398952 Training: 0.6429718783600467 Seed: 19571\n",
      "Testing: 0.653211148804257 Training: 0.6434823168060901 Seed: 19573\n",
      "Testing: 0.6487553578428584 Training: 0.6444523364136817 Seed: 19578\n",
      "Testing: 0.6613049806058487 Training: 0.6413940669770234 Seed: 19579\n",
      "Testing: 0.6481555127066226 Training: 0.6446350961297105 Seed: 19582\n",
      "Testing: 0.6481712962103128 Training: 0.6443621971031429 Seed: 19583\n",
      "Testing: 0.65322610666487 Training: 0.6433973834055018 Seed: 19586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.646482810156922 Training: 0.6451498325670146 Seed: 19588\n",
      "Testing: 0.661182522584018 Training: 0.6415379698483823 Seed: 19591\n",
      "Testing: 0.6506757768402385 Training: 0.6439431458528668 Seed: 19595\n",
      "Testing: 0.6517764710874792 Training: 0.6437867658029428 Seed: 19596\n",
      "Testing: 0.6537310286316815 Training: 0.643395925996425 Seed: 19600\n",
      "Testing: 0.6480598369292417 Training: 0.6446922371608523 Seed: 19601\n",
      "Testing: 0.6510915884119572 Training: 0.6438219127641452 Seed: 19602\n",
      "Testing: 0.6585496518745779 Training: 0.6421080458372351 Seed: 19603\n",
      "Testing: 0.6492071270385249 Training: 0.6443670604371627 Seed: 19605\n",
      "Testing: 0.6479372182737159 Training: 0.6444569960997506 Seed: 19606\n",
      "Testing: 0.6521820236363822 Training: 0.6435696686645882 Seed: 19607\n",
      "Testing: 0.6459601545274724 Training: 0.6452251696820752 Seed: 19608\n",
      "Testing: 0.6482003300768131 Training: 0.6446899137561748 Seed: 19609\n",
      "Testing: 0.6605437574342288 Training: 0.6417999301977363 Seed: 19610\n",
      "Testing: 0.648014082211749 Training: 0.6446817699155403 Seed: 19613\n",
      "Testing: 0.6551001495965246 Training: 0.6430151256870584 Seed: 19614\n",
      "Testing: 0.6508504133994675 Training: 0.6440190360514149 Seed: 19616\n",
      "Testing: 0.6532245091113281 Training: 0.6433075462934417 Seed: 19617\n",
      "Testing: 0.6547249062221038 Training: 0.6430744923668774 Seed: 19618\n",
      "Testing: 0.6460769540884362 Training: 0.6452752511391662 Seed: 19622\n",
      "Testing: 0.6675463478856708 Training: 0.6400983283923485 Seed: 19626\n",
      "Testing: 0.6456380867035624 Training: 0.6445205223711136 Seed: 19629\n",
      "Testing: 0.6492377549798085 Training: 0.64420193347604 Seed: 19630\n",
      "Testing: 0.6603430457964891 Training: 0.64133423505837 Seed: 19635\n",
      "Testing: 0.662611109318011 Training: 0.6409917452533971 Seed: 19636\n",
      "Testing: 0.6563721512822244 Training: 0.6426588634968863 Seed: 19639\n",
      "Testing: 0.6490224422908555 Training: 0.644418764659508 Seed: 19640\n",
      "Testing: 0.6475235692266104 Training: 0.6447690982381571 Seed: 19641\n",
      "Testing: 0.6457347631683599 Training: 0.6452668503519682 Seed: 19644\n",
      "Testing: 0.6590791484523033 Training: 0.6419839679177906 Seed: 19645\n",
      "Testing: 0.6482483115838306 Training: 0.6447222926262806 Seed: 19647\n",
      "Testing: 0.6511131333125829 Training: 0.6438667689082513 Seed: 19648\n",
      "Testing: 0.6610951567615533 Training: 0.6414745787099121 Seed: 19649\n",
      "Testing: 0.6531565323058733 Training: 0.6430939910122897 Seed: 19651\n",
      "Testing: 0.6750526636931913 Training: 0.6378677080601101 Seed: 19652\n",
      "Testing: 0.6589405137170765 Training: 0.6418222724018716 Seed: 19653\n",
      "Testing: 0.6466574888666421 Training: 0.6448141877932946 Seed: 19658\n",
      "Testing: 0.6587165648471678 Training: 0.6419626610835389 Seed: 19660\n",
      "Testing: 0.6557662877074392 Training: 0.6427706647726465 Seed: 19664\n",
      "Testing: 0.6462372952361422 Training: 0.6451396469111531 Seed: 19667\n",
      "Testing: 0.645592628631229 Training: 0.6452740494231622 Seed: 19670\n",
      "Testing: 0.6518611093263045 Training: 0.643760001440764 Seed: 19671\n",
      "Testing: 0.6518797824599759 Training: 0.6437715919632643 Seed: 19673\n",
      "Testing: 0.6496539213088135 Training: 0.6443347565870506 Seed: 19675\n",
      "Testing: 0.6667686045386481 Training: 0.6398759683929358 Seed: 19678\n",
      "Testing: 0.6590991825350763 Training: 0.642053453704802 Seed: 19684\n",
      "Testing: 0.6612092262784722 Training: 0.6413890392271706 Seed: 19685\n",
      "Testing: 0.6589130862100051 Training: 0.6419258271244848 Seed: 19686\n",
      "Testing: 0.6649969753304468 Training: 0.6405014416669262 Seed: 19687\n",
      "Testing: 0.645876508769284 Training: 0.6452067714759142 Seed: 19690\n",
      "Testing: 0.6490523950082792 Training: 0.6445065636090908 Seed: 19692\n",
      "Testing: 0.6665670130323239 Training: 0.6400829978690503 Seed: 19694\n",
      "Testing: 0.6677673366778721 Training: 0.6396808560679723 Seed: 19696\n",
      "Testing: 0.6590089711134468 Training: 0.6419433525407636 Seed: 19698\n",
      "Testing: 0.6706249492887028 Training: 0.6389254026615063 Seed: 19699\n",
      "Testing: 0.6623527677223825 Training: 0.6410714426280821 Seed: 19700\n",
      "Testing: 0.6453813958793224 Training: 0.6448690857249929 Seed: 19702\n",
      "Testing: 0.6523171125741802 Training: 0.6436235394487397 Seed: 19704\n",
      "Testing: 0.6511247125748071 Training: 0.6437421699135824 Seed: 19705\n",
      "Testing: 0.66469659372143 Training: 0.6404460394838183 Seed: 19707\n",
      "Testing: 0.6508634289130635 Training: 0.6438739876788286 Seed: 19709\n",
      "Testing: 0.6610246393241344 Training: 0.6414914560719822 Seed: 19710\n",
      "Testing: 0.6539761378704385 Training: 0.643099725566423 Seed: 19711\n",
      "Testing: 0.6647769071939388 Training: 0.6406165685885785 Seed: 19715\n",
      "Testing: 0.6584953229153714 Training: 0.6419542141101665 Seed: 19717\n",
      "Testing: 0.6635994663949785 Training: 0.6410384449560002 Seed: 19719\n",
      "Testing: 0.657084166906191 Training: 0.6424332002560538 Seed: 19721\n",
      "Testing: 0.6487427251236508 Training: 0.644513230493595 Seed: 19722\n",
      "Testing: 0.6622088131258905 Training: 0.6411037484581394 Seed: 19725\n",
      "Testing: 0.6493349286955193 Training: 0.6443604067089412 Seed: 19727\n",
      "Testing: 0.6506073080897331 Training: 0.6437424857231137 Seed: 19728\n",
      "Testing: 0.6567700319666337 Training: 0.6424483257890959 Seed: 19731\n",
      "Testing: 0.6635868630905162 Training: 0.6407882770882494 Seed: 19732\n",
      "Testing: 0.6514072230738865 Training: 0.6438897255703557 Seed: 19733\n",
      "Testing: 0.6616734512321086 Training: 0.6412162888258365 Seed: 19734\n",
      "Testing: 0.6499573411560708 Training: 0.6441760623133754 Seed: 19737\n",
      "Testing: 0.6595320524954121 Training: 0.6417236390114818 Seed: 19740\n",
      "Testing: 0.6597834146082713 Training: 0.64165607663496 Seed: 19741\n",
      "Testing: 0.6599831548294556 Training: 0.6415945812102116 Seed: 19742\n",
      "Testing: 0.6490924804102348 Training: 0.6444850952580052 Seed: 19743\n",
      "Testing: 0.6645072554890007 Training: 0.6404218599130569 Seed: 19745\n",
      "Testing: 0.648642304515128 Training: 0.6445945648105298 Seed: 19746\n",
      "Testing: 0.652440709528581 Training: 0.6435263068346179 Seed: 19748\n",
      "Testing: 0.6470749219099908 Training: 0.644975213313812 Seed: 19750\n",
      "Testing: 0.648895085514624 Training: 0.6444473523916893 Seed: 19751\n",
      "Testing: 0.6532765018657897 Training: 0.6434248287552359 Seed: 19752\n",
      "Testing: 0.6520366032498601 Training: 0.643666558229395 Seed: 19753\n",
      "Testing: 0.6531260804268996 Training: 0.6434526370655339 Seed: 19754\n",
      "Testing: 0.6542845400044135 Training: 0.6431364501531897 Seed: 19756\n",
      "Testing: 0.646981446954814 Training: 0.6447691091321404 Seed: 19758\n",
      "Testing: 0.668699098438674 Training: 0.6395805324553738 Seed: 19759\n",
      "Testing: 0.6531871447370574 Training: 0.6434880214419282 Seed: 19761\n",
      "Testing: 0.6552114369167797 Training: 0.6429787322073021 Seed: 19764\n",
      "Testing: 0.6568821303932081 Training: 0.6422527256310944 Seed: 19765\n",
      "Testing: 0.6502811714126044 Training: 0.6441953842424627 Seed: 19767\n",
      "Testing: 0.6458608992744944 Training: 0.6451566128302502 Seed: 19769\n",
      "Testing: 0.6579400292822037 Training: 0.6423384577904439 Seed: 19771\n",
      "Testing: 0.6489650263992678 Training: 0.6444726267947926 Seed: 19774\n",
      "Testing: 0.6541924038274955 Training: 0.6431595066647042 Seed: 19776\n",
      "Testing: 0.6602434827047927 Training: 0.6416418336334293 Seed: 19779\n",
      "Testing: 0.6542559344463136 Training: 0.6430736231255196 Seed: 19781\n",
      "Testing: 0.649262048840076 Training: 0.6442810517400668 Seed: 19782\n",
      "Testing: 0.6589162060182079 Training: 0.6419056549221158 Seed: 19785\n",
      "Testing: 0.6574372836023565 Training: 0.6423192998258642 Seed: 19786\n",
      "Testing: 0.6612404663925738 Training: 0.6412663752887636 Seed: 19787\n",
      "Testing: 0.6478264874263059 Training: 0.6446080648744829 Seed: 19789\n",
      "Testing: 0.6530990229970233 Training: 0.643430896355941 Seed: 19790\n",
      "Testing: 0.6524433104083198 Training: 0.6436264205084228 Seed: 19791\n",
      "Testing: 0.6520260339121611 Training: 0.6437299363395996 Seed: 19792\n",
      "Testing: 0.6563366540063441 Training: 0.6426139041550283 Seed: 19794\n",
      "Testing: 0.6602829009962781 Training: 0.6416434344003064 Seed: 19799\n",
      "Testing: 0.6574860064540842 Training: 0.6423571402463215 Seed: 19802\n",
      "Testing: 0.6688565708685547 Training: 0.6394266106583447 Seed: 19807\n",
      "Testing: 0.6539089656340205 Training: 0.6432589535575831 Seed: 19810\n",
      "Testing: 0.6472175216358993 Training: 0.6449239854912551 Seed: 19814\n",
      "Testing: 0.6537043186020031 Training: 0.6431146013827451 Seed: 19821\n",
      "Testing: 0.6531663982897898 Training: 0.6434843908138275 Seed: 19822\n",
      "Testing: 0.6461898975074325 Training: 0.6452084720367955 Seed: 19823\n",
      "Testing: 0.6571134551736947 Training: 0.6423652699516695 Seed: 19826\n",
      "Testing: 0.6501438284429938 Training: 0.6440776353654858 Seed: 19827\n",
      "Testing: 0.6644617299642477 Training: 0.6407615417149233 Seed: 19829\n",
      "Testing: 0.6500818704833577 Training: 0.6442350637096117 Seed: 19830\n",
      "Testing: 0.6512785152962864 Training: 0.6437863578514524 Seed: 19832\n",
      "Testing: 0.6678653544726919 Training: 0.6396108209555855 Seed: 19835\n",
      "Testing: 0.6533989805999654 Training: 0.6432096325769725 Seed: 19838\n",
      "Testing: 0.6560955367140003 Training: 0.6424561466041243 Seed: 19847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6471138386422954 Training: 0.6449912091279694 Seed: 19849\n",
      "Testing: 0.6590803599425984 Training: 0.6418772502826973 Seed: 19852\n",
      "Testing: 0.6624090169077456 Training: 0.6406828685897425 Seed: 19855\n",
      "Testing: 0.6634227026270607 Training: 0.6407931300417495 Seed: 19857\n",
      "Testing: 0.659987156478245 Training: 0.6411899590948742 Seed: 19861\n",
      "Testing: 0.6588873285223094 Training: 0.6419013631359396 Seed: 19862\n",
      "Testing: 0.6570494842738699 Training: 0.6424451857427912 Seed: 19863\n",
      "Testing: 0.6540996474627425 Training: 0.6432243541052205 Seed: 19865\n",
      "Testing: 0.6569237976113523 Training: 0.6421936290586381 Seed: 19866\n",
      "Testing: 0.6499732117412493 Training: 0.6443160724609076 Seed: 19868\n",
      "Testing: 0.6637822314071425 Training: 0.6405466462646137 Seed: 19869\n",
      "Testing: 0.6511579084673835 Training: 0.6439465130297432 Seed: 19871\n",
      "Testing: 0.6525223423254543 Training: 0.6435391130413205 Seed: 19873\n",
      "Testing: 0.6513390436202653 Training: 0.6438633546609386 Seed: 19874\n",
      "Testing: 0.6647906215628688 Training: 0.6403893940923198 Seed: 19877\n",
      "Testing: 0.6484309276640476 Training: 0.6445017236827913 Seed: 19878\n",
      "Testing: 0.6543343734189124 Training: 0.643211912239287 Seed: 19882\n",
      "Testing: 0.657659829450122 Training: 0.6421011209375682 Seed: 19889\n",
      "Testing: 0.6474979574335494 Training: 0.6448314781530227 Seed: 19892\n",
      "Testing: 0.6488908222990639 Training: 0.6445247037827516 Seed: 19893\n",
      "Testing: 0.6462880657802822 Training: 0.6451649575882199 Seed: 19894\n",
      "Testing: 0.654415555833367 Training: 0.6431491047543589 Seed: 19897\n",
      "Testing: 0.6573034656328969 Training: 0.6426100447778641 Seed: 19898\n",
      "Testing: 0.6491973969818681 Training: 0.6444216477146921 Seed: 19901\n",
      "Testing: 0.6568193716070976 Training: 0.6425191378799375 Seed: 19902\n",
      "Testing: 0.6563981801174087 Training: 0.6427241209968098 Seed: 19903\n",
      "Testing: 0.6579958010467567 Training: 0.6423444449117428 Seed: 19904\n",
      "Testing: 0.6607674835454541 Training: 0.6416582538894876 Seed: 19906\n",
      "Testing: 0.6493861686359296 Training: 0.6444125315619667 Seed: 19907\n",
      "Testing: 0.6476914836940365 Training: 0.6448338149387624 Seed: 19908\n",
      "Testing: 0.6653347938891828 Training: 0.6403063397035555 Seed: 19910\n",
      "Testing: 0.6609283846940248 Training: 0.6412525672891116 Seed: 19912\n",
      "Testing: 0.6469284387000966 Training: 0.6449121047300217 Seed: 19914\n",
      "Testing: 0.6525818443834771 Training: 0.6436213915451123 Seed: 19916\n",
      "Testing: 0.6514270656839969 Training: 0.6439898852487969 Seed: 19918\n",
      "Testing: 0.6455706555517908 Training: 0.645257668698955 Seed: 19922\n",
      "Testing: 0.6463748958206518 Training: 0.6450376115135735 Seed: 19924\n",
      "Testing: 0.6516333524629017 Training: 0.6436502712730194 Seed: 19926\n",
      "Testing: 0.6560955164578524 Training: 0.6424756345132117 Seed: 19928\n",
      "Testing: 0.6537171221245178 Training: 0.6431422213365878 Seed: 19929\n",
      "Testing: 0.6504037752626327 Training: 0.644201984294682 Seed: 19930\n",
      "Testing: 0.6511344540518759 Training: 0.6438279929259884 Seed: 19936\n",
      "Testing: 0.6485575955440464 Training: 0.6443392099436835 Seed: 19937\n",
      "Testing: 0.6491563935946149 Training: 0.6443134328611213 Seed: 19939\n",
      "Testing: 0.6527506633233555 Training: 0.6436059423520253 Seed: 19940\n",
      "Testing: 0.6483552256845966 Training: 0.6443070547738319 Seed: 19943\n",
      "Testing: 0.6487930119249588 Training: 0.6444559736191642 Seed: 19948\n",
      "Testing: 0.6643073851193798 Training: 0.6405106034055594 Seed: 19949\n",
      "Testing: 0.6592768872379654 Training: 0.6420028230044416 Seed: 19950\n",
      "Testing: 0.6500734390695099 Training: 0.6441272212009432 Seed: 19951\n",
      "Testing: 0.6572969593104756 Training: 0.642246952035846 Seed: 19956\n",
      "Testing: 0.6485727747327 Training: 0.644556525854761 Seed: 19959\n",
      "Testing: 0.6596684638747563 Training: 0.6418581680339094 Seed: 19960\n",
      "Testing: 0.6513062586767409 Training: 0.6436915095255329 Seed: 19964\n",
      "Testing: 0.6508567184306574 Training: 0.644004200673607 Seed: 19971\n",
      "Testing: 0.6505121974635164 Training: 0.6441614508881728 Seed: 19972\n",
      "Testing: 0.6525604356990585 Training: 0.6435755167109561 Seed: 19974\n",
      "Testing: 0.6572147443674702 Training: 0.6422988543178911 Seed: 19977\n",
      "Testing: 0.6523028797099149 Training: 0.6436899380228421 Seed: 19978\n",
      "Testing: 0.651072638591616 Training: 0.6440469034543487 Seed: 19980\n",
      "Testing: 0.6502843353533805 Training: 0.6442558322131221 Seed: 19981\n",
      "Testing: 0.6568285427150908 Training: 0.6425436302556067 Seed: 19982\n",
      "Testing: 0.6501116992880266 Training: 0.6441424325001225 Seed: 19983\n",
      "Testing: 0.648136068378932 Training: 0.6446288962483759 Seed: 19985\n",
      "Testing: 0.654019204562206 Training: 0.6431486825082428 Seed: 19986\n",
      "Testing: 0.6455726647705886 Training: 0.6452856095998584 Seed: 19992\n",
      "Testing: 0.6575747314209863 Training: 0.6423712791075918 Seed: 19995\n",
      "Testing: 0.6525950455126834 Training: 0.6436236738545704 Seed: 19996\n",
      "Testing: 0.6614797903903507 Training: 0.6412008988755131 Seed: 19997\n",
      "Testing: 0.6548471216263755 Training: 0.6429725347729132 Seed: 19998\n",
      "Testing: 0.6509051718515872 Training: 0.6439519331937702 Seed: 19999\n",
      "Testing: 0.6457075589563072 Training: 0.6451941209446328 Seed: 20000\n",
      "Testing: 0.6542929083101251 Training: 0.6432527410365443 Seed: 20003\n",
      "Testing: 0.6490417238979099 Training: 0.6444026760195317 Seed: 20004\n",
      "Testing: 0.6644614073599718 Training: 0.6406619338321653 Seed: 20005\n",
      "Testing: 0.6526733024465674 Training: 0.6435212045513055 Seed: 20008\n",
      "Testing: 0.6533668779829729 Training: 0.6434575424650216 Seed: 20010\n",
      "Testing: 0.6474163598782798 Training: 0.6448080962032298 Seed: 20011\n",
      "Testing: 0.6498165479657763 Training: 0.6442315926598152 Seed: 20013\n",
      "Testing: 0.6458593891159436 Training: 0.6452278099729001 Seed: 20017\n",
      "Testing: 0.6532200455815185 Training: 0.6432933881838563 Seed: 20018\n",
      "Testing: 0.6577488567995842 Training: 0.6422497559554908 Seed: 20019\n",
      "Testing: 0.6495710867913642 Training: 0.6442245824435673 Seed: 20020\n",
      "Testing: 0.6587841252520886 Training: 0.6419012281444514 Seed: 20022\n",
      "Testing: 0.6699383690591008 Training: 0.6390579291094887 Seed: 20025\n",
      "Testing: 0.6517208258899612 Training: 0.643885047973009 Seed: 20029\n",
      "Testing: 0.6508246944555403 Training: 0.6435562705042623 Seed: 20031\n",
      "Testing: 0.6532947549319768 Training: 0.6434768373550441 Seed: 20033\n",
      "Testing: 0.6602627421862927 Training: 0.6415700473900242 Seed: 20035\n",
      "Testing: 0.6504202800520141 Training: 0.6440738956431149 Seed: 20036\n",
      "Testing: 0.6472142880618365 Training: 0.6449215377278538 Seed: 20043\n",
      "Testing: 0.6457707227781975 Training: 0.6451245101826046 Seed: 20048\n",
      "Testing: 0.6529855306438987 Training: 0.6432242515905191 Seed: 20058\n",
      "Testing: 0.6492473047165097 Training: 0.6443251053090217 Seed: 20059\n",
      "Testing: 0.6507779929243738 Training: 0.6439628983292793 Seed: 20063\n",
      "Testing: 0.6491722076263391 Training: 0.6443510749259188 Seed: 20064\n",
      "Testing: 0.6537542812668893 Training: 0.6433414296426754 Seed: 20069\n",
      "Testing: 0.6583095094551923 Training: 0.6420247046795636 Seed: 20071\n",
      "Testing: 0.6477102308282803 Training: 0.6446900102142625 Seed: 20074\n",
      "Testing: 0.6469136694368058 Training: 0.6449053294550611 Seed: 20075\n",
      "Testing: 0.6515636780565821 Training: 0.6438500331360235 Seed: 20078\n",
      "Testing: 0.6541868433368674 Training: 0.6431036783462828 Seed: 20079\n",
      "Testing: 0.6566582662291456 Training: 0.6426194403492449 Seed: 20080\n",
      "Testing: 0.6490339417870765 Training: 0.6443375794561781 Seed: 20081\n",
      "Testing: 0.6465868137921614 Training: 0.645133566914218 Seed: 20083\n",
      "Testing: 0.647738712207347 Training: 0.6447547733848972 Seed: 20084\n",
      "Testing: 0.6538058956261223 Training: 0.6431492809220785 Seed: 20093\n",
      "Testing: 0.6466028304340047 Training: 0.6451215531735179 Seed: 20094\n",
      "Testing: 0.6551390378485528 Training: 0.643007610809777 Seed: 20095\n",
      "Testing: 0.6484077043566303 Training: 0.6446236226805113 Seed: 20096\n",
      "Testing: 0.653182918857732 Training: 0.6435240177981194 Seed: 20099\n",
      "Testing: 0.6543461859301088 Training: 0.6431236323520552 Seed: 20102\n",
      "Testing: 0.6548380841878916 Training: 0.6428940475815941 Seed: 20105\n",
      "Testing: 0.6465451149131226 Training: 0.6451200127312169 Seed: 20110\n",
      "Testing: 0.6479283648376845 Training: 0.6447865610439646 Seed: 20113\n",
      "Testing: 0.6468984169387376 Training: 0.6445537316748283 Seed: 20114\n",
      "Testing: 0.6547386630104252 Training: 0.6429623149004436 Seed: 20115\n",
      "Testing: 0.6580023610314073 Training: 0.6421721237476373 Seed: 20118\n",
      "Testing: 0.6493951476654758 Training: 0.6443915257819338 Seed: 20120\n",
      "Testing: 0.6484165079043669 Training: 0.6445807913486804 Seed: 20121\n",
      "Testing: 0.6544140866753784 Training: 0.6429140439245726 Seed: 20124\n",
      "Testing: 0.6472682538119618 Training: 0.6448235402378045 Seed: 20125\n",
      "Testing: 0.6464290281689127 Training: 0.6449912208352376 Seed: 20127\n",
      "Testing: 0.6510229788111827 Training: 0.6438934409429209 Seed: 20129\n",
      "Testing: 0.646494098320671 Training: 0.6451051317901337 Seed: 20130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6531625286338705 Training: 0.6430978018653631 Seed: 20132\n",
      "Testing: 0.6481267367435547 Training: 0.6447750807078112 Seed: 20134\n",
      "Testing: 0.6462116674337576 Training: 0.6450972594969211 Seed: 20136\n",
      "Testing: 0.6474793855092633 Training: 0.6447178452489556 Seed: 20137\n",
      "Testing: 0.6495188148700275 Training: 0.6444147601326182 Seed: 20139\n",
      "Testing: 0.6542790853589997 Training: 0.6431158801278989 Seed: 20140\n",
      "Testing: 0.6533492429604784 Training: 0.6434643609265404 Seed: 20141\n",
      "Testing: 0.6528264228413161 Training: 0.6435623231944692 Seed: 20143\n",
      "Testing: 0.6556111480732569 Training: 0.6427798920748572 Seed: 20145\n",
      "Testing: 0.6527301002186117 Training: 0.6431322989953927 Seed: 20147\n",
      "Testing: 0.6587495577151534 Training: 0.6421264871018837 Seed: 20149\n",
      "Testing: 0.6549336068382844 Training: 0.6419871253474196 Seed: 20150\n",
      "Testing: 0.6479159766099702 Training: 0.6447042608154346 Seed: 20151\n",
      "Testing: 0.6529839307335052 Training: 0.643312456469876 Seed: 20152\n",
      "Testing: 0.6457732446347226 Training: 0.6452690538203188 Seed: 20154\n",
      "Testing: 0.6497927343413368 Training: 0.6442218193803966 Seed: 20155\n",
      "Testing: 0.6510467950350496 Training: 0.6439832231463767 Seed: 20158\n",
      "Testing: 0.6620012658791087 Training: 0.6409933090337359 Seed: 20160\n",
      "Testing: 0.6474427415408708 Training: 0.6449393725252799 Seed: 20161\n",
      "Testing: 0.6571617978949593 Training: 0.6426599477312027 Seed: 20163\n",
      "Testing: 0.6589749758659008 Training: 0.6422560559990695 Seed: 20166\n",
      "Testing: 0.6567482655378936 Training: 0.6425922128229159 Seed: 20167\n",
      "Testing: 0.6566691326140683 Training: 0.642392762427862 Seed: 20168\n",
      "Testing: 0.6658408536296447 Training: 0.6399135606813608 Seed: 20170\n",
      "Testing: 0.6478988677886646 Training: 0.6446586911545221 Seed: 20171\n",
      "Testing: 0.6459848661945735 Training: 0.6451899380681985 Seed: 20173\n",
      "Testing: 0.6496126835421341 Training: 0.6442831274259747 Seed: 20174\n",
      "Testing: 0.6542429058688408 Training: 0.6432083365530741 Seed: 20176\n",
      "Testing: 0.6653043998713987 Training: 0.6403372656942048 Seed: 20178\n",
      "Testing: 0.6489590555583408 Training: 0.644445060144272 Seed: 20179\n",
      "Testing: 0.6530593443123726 Training: 0.6433727177663708 Seed: 20180\n",
      "Testing: 0.647338055988158 Training: 0.6447735161007438 Seed: 20181\n",
      "Testing: 0.6562346365867553 Training: 0.6426559604237153 Seed: 20185\n",
      "Testing: 0.6498059464914355 Training: 0.6440205581034824 Seed: 20187\n",
      "Testing: 0.6469995428485102 Training: 0.6449771469051824 Seed: 20189\n",
      "Testing: 0.6624765339262505 Training: 0.6409862723890793 Seed: 20190\n",
      "Testing: 0.6486319876244135 Training: 0.6445736784803527 Seed: 20192\n",
      "Testing: 0.645426065292459 Training: 0.6454069177780736 Seed: 20193\n",
      "Testing: 0.6467920607566752 Training: 0.6450311961301755 Seed: 20194\n",
      "Testing: 0.6487590299550952 Training: 0.6445038773307115 Seed: 20199\n",
      "Testing: 0.6535151918418394 Training: 0.6433836122964083 Seed: 20200\n",
      "Testing: 0.6525892235364061 Training: 0.6436115563106388 Seed: 20203\n",
      "Testing: 0.6536202108623033 Training: 0.6430322351093697 Seed: 20207\n",
      "Testing: 0.6538606388650393 Training: 0.643119883877167 Seed: 20208\n",
      "Testing: 0.6457155656668893 Training: 0.6452717563395136 Seed: 20209\n",
      "Testing: 0.6520471267446355 Training: 0.6435495450909459 Seed: 20210\n",
      "Testing: 0.6490910351075255 Training: 0.6444352586054324 Seed: 20212\n",
      "Testing: 0.6482096673403074 Training: 0.6446734042629454 Seed: 20216\n",
      "Testing: 0.6598434737123899 Training: 0.6415148512471031 Seed: 20218\n",
      "Testing: 0.6503759757141458 Training: 0.6440340298583799 Seed: 20220\n",
      "Testing: 0.6501302152828414 Training: 0.6442506642068367 Seed: 20224\n",
      "Testing: 0.6636070959198512 Training: 0.640919526771156 Seed: 20225\n",
      "Testing: 0.6532849680251914 Training: 0.6433072408775207 Seed: 20230\n",
      "Testing: 0.6544005067641613 Training: 0.643087762376461 Seed: 20231\n",
      "Testing: 0.6470861633501381 Training: 0.6448758753220223 Seed: 20232\n",
      "Testing: 0.6504723904210689 Training: 0.6442145006483836 Seed: 20233\n",
      "Testing: 0.6550266681555479 Training: 0.6430040828861168 Seed: 20234\n",
      "Testing: 0.6616413091404703 Training: 0.6413170516718356 Seed: 20235\n",
      "Testing: 0.6590727097281127 Training: 0.6420647413088093 Seed: 20237\n",
      "Testing: 0.6457562577327163 Training: 0.6452904224133873 Seed: 20238\n",
      "Testing: 0.6497648238794427 Training: 0.6442689351941986 Seed: 20239\n",
      "Testing: 0.6481296934256461 Training: 0.6446523385990116 Seed: 20240\n",
      "Testing: 0.6473242634213214 Training: 0.6449121921904614 Seed: 20243\n",
      "Testing: 0.6465218979289269 Training: 0.6451136819827649 Seed: 20244\n",
      "Testing: 0.6652515565048501 Training: 0.6404502594136607 Seed: 20246\n",
      "Testing: 0.6501808955927908 Training: 0.6441435572368829 Seed: 20252\n",
      "Testing: 0.65795822323207 Training: 0.6421843216377312 Seed: 20253\n",
      "Testing: 0.64625748676447 Training: 0.6451303734990577 Seed: 20254\n",
      "Testing: 0.6576380089843308 Training: 0.6422600753527059 Seed: 20255\n",
      "Testing: 0.650287797013503 Training: 0.6441721989696618 Seed: 20256\n",
      "Testing: 0.6548587886623228 Training: 0.6429551948769339 Seed: 20258\n",
      "Testing: 0.657774461886104 Training: 0.642179671846924 Seed: 20260\n",
      "Testing: 0.6635812095555114 Training: 0.6406773352340193 Seed: 20261\n",
      "Testing: 0.6531788316012893 Training: 0.6434743357943947 Seed: 20262\n",
      "Testing: 0.6497438925844747 Training: 0.6441713270779512 Seed: 20264\n",
      "Testing: 0.6578023140880375 Training: 0.6421678277153025 Seed: 20265\n",
      "Testing: 0.6487990212319099 Training: 0.6441910038280043 Seed: 20266\n",
      "Testing: 0.6518762260448484 Training: 0.643633414039604 Seed: 20268\n",
      "Testing: 0.6549212916119318 Training: 0.6425712788665985 Seed: 20270\n",
      "Testing: 0.6477202190624816 Training: 0.6447686787696806 Seed: 20272\n",
      "Testing: 0.6594899356584784 Training: 0.6419381828369068 Seed: 20274\n",
      "Testing: 0.6533396661718023 Training: 0.64334297200466 Seed: 20275\n",
      "Testing: 0.6529318404071104 Training: 0.6433668439810619 Seed: 20282\n",
      "Testing: 0.6460527289640141 Training: 0.6451401861620536 Seed: 20283\n",
      "Testing: 0.659871130904378 Training: 0.6414981974388394 Seed: 20284\n",
      "Testing: 0.6505662893045077 Training: 0.6438245140808683 Seed: 20286\n",
      "Testing: 0.6516281734572915 Training: 0.6436721455919827 Seed: 20288\n",
      "Testing: 0.648065408001831 Training: 0.6447576593502402 Seed: 20289\n",
      "Testing: 0.6484458246414115 Training: 0.6446816990936429 Seed: 20291\n",
      "Testing: 0.6565523768015913 Training: 0.6426101492933463 Seed: 20293\n",
      "Testing: 0.6460551616108011 Training: 0.6452513013456282 Seed: 20295\n",
      "Testing: 0.647545694017158 Training: 0.6447578462433825 Seed: 20296\n",
      "Testing: 0.6498841417512381 Training: 0.6441858836324911 Seed: 20300\n",
      "Testing: 0.658319775890924 Training: 0.6421533025451971 Seed: 20302\n",
      "Testing: 0.6519040705610611 Training: 0.643694171743141 Seed: 20305\n",
      "Testing: 0.6473400143193057 Training: 0.6448551233744286 Seed: 20308\n",
      "Testing: 0.6487870089657934 Training: 0.6445411096494933 Seed: 20313\n",
      "Testing: 0.651961388983654 Training: 0.6436683473377136 Seed: 20316\n",
      "Testing: 0.6702012995599831 Training: 0.6391953783641177 Seed: 20318\n",
      "Testing: 0.6558673577996336 Training: 0.6425066056961309 Seed: 20319\n",
      "Testing: 0.657336483922876 Training: 0.6423110637410252 Seed: 20320\n",
      "Testing: 0.6610686476006703 Training: 0.6414062666240263 Seed: 20321\n",
      "Testing: 0.6587266959702945 Training: 0.6421003347620036 Seed: 20324\n",
      "Testing: 0.6662802116344059 Training: 0.6402056209454584 Seed: 20325\n",
      "Testing: 0.6492022226360059 Training: 0.6443959805660523 Seed: 20329\n",
      "Testing: 0.6663596343941195 Training: 0.6401210416754999 Seed: 20330\n",
      "Testing: 0.6745109866131129 Training: 0.6378548548716667 Seed: 20331\n",
      "Testing: 0.6534476470522874 Training: 0.643462389508921 Seed: 20332\n",
      "Testing: 0.6678404546100117 Training: 0.639646831804283 Seed: 20333\n",
      "Testing: 0.6568451993067115 Training: 0.6424377050824615 Seed: 20335\n",
      "Testing: 0.651579324365386 Training: 0.6437476946667098 Seed: 20337\n",
      "Testing: 0.6621289492089896 Training: 0.6412190222593911 Seed: 20343\n",
      "Testing: 0.6647657916341236 Training: 0.6405698764946717 Seed: 20347\n",
      "Testing: 0.652221422469715 Training: 0.6436227532663856 Seed: 20351\n",
      "Testing: 0.6500721278336046 Training: 0.6441107967851654 Seed: 20353\n",
      "Testing: 0.6632457829002537 Training: 0.6409813765335018 Seed: 20354\n",
      "Testing: 0.6460281571630664 Training: 0.6452273664990522 Seed: 20355\n",
      "Testing: 0.6598457701494824 Training: 0.6418519756718651 Seed: 20356\n",
      "Testing: 0.6501168861709121 Training: 0.6442542557156365 Seed: 20360\n",
      "Testing: 0.6459223034879197 Training: 0.6452968908804663 Seed: 20362\n",
      "Testing: 0.6527616154769467 Training: 0.6434940458322422 Seed: 20363\n",
      "Testing: 0.658225545375978 Training: 0.6418243067621059 Seed: 20365\n",
      "Testing: 0.6600031328443079 Training: 0.6416822310585648 Seed: 20368\n",
      "Testing: 0.6477965703468287 Training: 0.6448069660089671 Seed: 20369\n",
      "Testing: 0.6478489029420678 Training: 0.6448219858093023 Seed: 20371\n",
      "Testing: 0.6528263372416799 Training: 0.643480812797153 Seed: 20372\n",
      "Testing: 0.6593592914712921 Training: 0.6419459246745359 Seed: 20373\n",
      "Testing: 0.6531415759931652 Training: 0.6433937184070062 Seed: 20374\n",
      "Testing: 0.6548900673380604 Training: 0.6430703898256573 Seed: 20376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.657005577670322 Training: 0.6425042223052574 Seed: 20377\n",
      "Testing: 0.6476638243796545 Training: 0.6447662071250727 Seed: 20379\n",
      "Testing: 0.6531261512651478 Training: 0.6433952926227283 Seed: 20384\n",
      "Testing: 0.661236121145274 Training: 0.6414479301988503 Seed: 20385\n",
      "Testing: 0.6482224558316796 Training: 0.644653083581133 Seed: 20386\n",
      "Testing: 0.6465711121645819 Training: 0.6450663135550427 Seed: 20387\n",
      "Testing: 0.6544060645956277 Training: 0.6431061246836755 Seed: 20390\n",
      "Testing: 0.6486164820531476 Training: 0.6442784992557994 Seed: 20394\n",
      "Testing: 0.6550509292107296 Training: 0.6429366000839686 Seed: 20395\n",
      "Testing: 0.6576374055767809 Training: 0.6424861925411396 Seed: 20397\n",
      "Testing: 0.645664259642996 Training: 0.645283092847611 Seed: 20398\n",
      "Testing: 0.648047613995748 Training: 0.6446356990053754 Seed: 20399\n",
      "Testing: 0.6641551906461172 Training: 0.6405719777254659 Seed: 20403\n",
      "Testing: 0.6566392826313744 Training: 0.6426491584794548 Seed: 20404\n",
      "Testing: 0.6538228425415242 Training: 0.6430897149086532 Seed: 20410\n",
      "Testing: 0.655528493876735 Training: 0.642807223537307 Seed: 20411\n",
      "Testing: 0.6466106205141877 Training: 0.6451252319519217 Seed: 20414\n",
      "Testing: 0.6543605813550374 Training: 0.6431379580543921 Seed: 20415\n",
      "Testing: 0.647032852202096 Training: 0.6448989967636403 Seed: 20416\n",
      "Testing: 0.649385881711136 Training: 0.6443959753765943 Seed: 20417\n",
      "Testing: 0.6576570123288514 Training: 0.642355806618505 Seed: 20418\n",
      "Testing: 0.6561446664759419 Training: 0.6426012424939385 Seed: 20420\n",
      "Testing: 0.6756250294790738 Training: 0.6379007733021953 Seed: 20425\n",
      "Testing: 0.647572619998346 Training: 0.6448845738500154 Seed: 20428\n",
      "Testing: 0.6540517940045699 Training: 0.6430504658263955 Seed: 20430\n",
      "Testing: 0.650735156760334 Training: 0.6438189921959674 Seed: 20431\n",
      "Testing: 0.6499791484864021 Training: 0.6441698165166349 Seed: 20435\n",
      "Testing: 0.6466868336659486 Training: 0.6449435151423455 Seed: 20438\n",
      "Testing: 0.6536378796137097 Training: 0.6431988945441527 Seed: 20439\n",
      "Testing: 0.6475295089254942 Training: 0.6448943530813115 Seed: 20441\n",
      "Testing: 0.6558084812130887 Training: 0.6426763664753293 Seed: 20442\n",
      "Testing: 0.6551354298976719 Training: 0.6429671375215973 Seed: 20443\n",
      "Testing: 0.6579291554042962 Training: 0.6420490973822043 Seed: 20444\n",
      "Testing: 0.652312953241503 Training: 0.6434110162392038 Seed: 20446\n",
      "Testing: 0.6544535151076057 Training: 0.6431521042672634 Seed: 20449\n",
      "Testing: 0.6541271222805669 Training: 0.6432616077359017 Seed: 20450\n",
      "Testing: 0.6522971990616329 Training: 0.6436565301927527 Seed: 20451\n",
      "Testing: 0.6601814209482735 Training: 0.641445271313428 Seed: 20453\n",
      "Testing: 0.6549582275041878 Training: 0.6429873371275823 Seed: 20456\n",
      "Testing: 0.662861505043487 Training: 0.6408596379172543 Seed: 20459\n",
      "Testing: 0.6604633220734196 Training: 0.6415086282373879 Seed: 20460\n",
      "Testing: 0.6456173027663514 Training: 0.6452993956551472 Seed: 20463\n",
      "Testing: 0.6611485249697028 Training: 0.6415471426733474 Seed: 20467\n",
      "Testing: 0.6492012465059862 Training: 0.6443450328948748 Seed: 20468\n",
      "Testing: 0.6519188473725884 Training: 0.6437605348913943 Seed: 20469\n",
      "Testing: 0.6562811134673481 Training: 0.6424603774688462 Seed: 20470\n",
      "Testing: 0.6633190073811854 Training: 0.6407469880579812 Seed: 20473\n",
      "Testing: 0.6545495727253414 Training: 0.643062575217501 Seed: 20474\n",
      "Testing: 0.655319914898782 Training: 0.6428637547058482 Seed: 20475\n",
      "Testing: 0.6484469869570713 Training: 0.644374537572683 Seed: 20476\n",
      "Testing: 0.6582028957924898 Training: 0.64216169409342 Seed: 20477\n",
      "Testing: 0.6460748533700016 Training: 0.645222808200798 Seed: 20479\n",
      "Testing: 0.6501900030304834 Training: 0.6441317201017918 Seed: 20480\n",
      "Testing: 0.6538126555579339 Training: 0.6424062701300384 Seed: 20481\n",
      "Testing: 0.666235417807967 Training: 0.6400888292355154 Seed: 20484\n",
      "Testing: 0.6541222258239857 Training: 0.6430119985720191 Seed: 20487\n",
      "Testing: 0.6472706768687542 Training: 0.644833782249813 Seed: 20489\n",
      "Testing: 0.6607642676678229 Training: 0.6414860239106115 Seed: 20490\n",
      "Testing: 0.655611587710829 Training: 0.642606280689198 Seed: 20491\n",
      "Testing: 0.6498046336541001 Training: 0.6443012608272466 Seed: 20496\n",
      "Testing: 0.6532974267889456 Training: 0.643251481328491 Seed: 20498\n",
      "Testing: 0.6564252092616296 Training: 0.6426366555732419 Seed: 20499\n",
      "Testing: 0.648748691846749 Training: 0.6445882109858992 Seed: 20500\n",
      "Testing: 0.6484948471528966 Training: 0.6446225088625821 Seed: 20501\n",
      "Testing: 0.6510898088051853 Training: 0.6439609034307225 Seed: 20502\n",
      "Testing: 0.6559324752464079 Training: 0.6425690538934848 Seed: 20507\n",
      "Testing: 0.6569198210128258 Training: 0.6424051807331407 Seed: 20508\n",
      "Testing: 0.6491402490702184 Training: 0.6443949300797821 Seed: 20510\n",
      "Testing: 0.6495043749981415 Training: 0.6443302935788121 Seed: 20512\n",
      "Testing: 0.6553750653119126 Training: 0.642896135325001 Seed: 20513\n",
      "Testing: 0.6641138557747718 Training: 0.640759279869921 Seed: 20516\n",
      "Testing: 0.650592494797988 Training: 0.6439758995874868 Seed: 20517\n",
      "Testing: 0.6568158043962158 Training: 0.6424845430795556 Seed: 20518\n",
      "Testing: 0.6542693182193486 Training: 0.6430587495989644 Seed: 20522\n",
      "Testing: 0.6543645657754361 Training: 0.6431542862470315 Seed: 20524\n",
      "Testing: 0.6582179283185181 Training: 0.642184177945678 Seed: 20525\n",
      "Testing: 0.6557952283807313 Training: 0.642843435999128 Seed: 20526\n",
      "Testing: 0.6684210720343076 Training: 0.6393293702749823 Seed: 20529\n",
      "Testing: 0.6580581119337701 Training: 0.6422506805220688 Seed: 20530\n",
      "Testing: 0.6491259165907866 Training: 0.6443318201461876 Seed: 20536\n",
      "Testing: 0.6611461886705703 Training: 0.6412961811375208 Seed: 20537\n",
      "Testing: 0.6601509357740398 Training: 0.6414529225304577 Seed: 20544\n",
      "Testing: 0.6639384290758734 Training: 0.6408562720465387 Seed: 20546\n",
      "Testing: 0.6608902901204871 Training: 0.6413701444014489 Seed: 20548\n",
      "Testing: 0.6464128535591567 Training: 0.6451073825239559 Seed: 20550\n",
      "Testing: 0.6502730406852013 Training: 0.6441045519717115 Seed: 20552\n",
      "Testing: 0.6524598521926661 Training: 0.6436243659620853 Seed: 20553\n",
      "Testing: 0.6455350033436851 Training: 0.6452552643041496 Seed: 20554\n",
      "Testing: 0.6502864589080277 Training: 0.6441696097096767 Seed: 20558\n",
      "Testing: 0.648755622280202 Training: 0.6444340215325787 Seed: 20559\n",
      "Testing: 0.6488661913475632 Training: 0.6444702068768389 Seed: 20560\n",
      "Testing: 0.6497210197913704 Training: 0.6443829642838137 Seed: 20563\n",
      "Testing: 0.6647799438586642 Training: 0.6401556042623229 Seed: 20566\n",
      "Testing: 0.6484691741363975 Training: 0.6446316622169531 Seed: 20567\n",
      "Testing: 0.64578981649673 Training: 0.6448198622979837 Seed: 20569\n",
      "Testing: 0.6468662188845686 Training: 0.6441483311050776 Seed: 20571\n",
      "Testing: 0.6476060187410327 Training: 0.64474084631489 Seed: 20572\n",
      "Testing: 0.6474183449638982 Training: 0.6442795418417384 Seed: 20575\n",
      "Testing: 0.6503801649110696 Training: 0.6439326009631576 Seed: 20578\n",
      "Testing: 0.6542528429516136 Training: 0.6431807215667049 Seed: 20582\n",
      "Testing: 0.6490561915477024 Training: 0.6444142850631585 Seed: 20583\n",
      "Testing: 0.6544486958659612 Training: 0.6429746520628963 Seed: 20584\n",
      "Testing: 0.6455652854778862 Training: 0.6453454979323974 Seed: 20590\n",
      "Testing: 0.6603091554424791 Training: 0.6416171252190908 Seed: 20593\n",
      "Testing: 0.648325022665079 Training: 0.6446245384910532 Seed: 20594\n",
      "Testing: 0.6676590415833608 Training: 0.639805756776294 Seed: 20596\n",
      "Testing: 0.6482529522826207 Training: 0.6447012594280559 Seed: 20598\n",
      "Testing: 0.6596771146073372 Training: 0.641276389914347 Seed: 20600\n",
      "Testing: 0.6456992584858333 Training: 0.6451923784051457 Seed: 20601\n",
      "Testing: 0.6464024292795102 Training: 0.6450998675473318 Seed: 20602\n",
      "Testing: 0.6685864003806016 Training: 0.6393083029789253 Seed: 20603\n",
      "Testing: 0.6497079006782385 Training: 0.644300847036722 Seed: 20604\n",
      "Testing: 0.645765506243433 Training: 0.6452277397177035 Seed: 20606\n",
      "Testing: 0.650855048426013 Training: 0.6439097875574685 Seed: 20608\n",
      "Testing: 0.6495483901046505 Training: 0.6443834964459731 Seed: 20612\n",
      "Testing: 0.6543043925690553 Training: 0.6427947519272146 Seed: 20614\n",
      "Testing: 0.6479735560905047 Training: 0.644675170809137 Seed: 20615\n",
      "Testing: 0.6527668615923353 Training: 0.6434671863615612 Seed: 20616\n",
      "Testing: 0.647085997424407 Training: 0.6449408521736114 Seed: 20617\n",
      "Testing: 0.6495957512547756 Training: 0.6441968570170153 Seed: 20618\n",
      "Testing: 0.647864601984916 Training: 0.6445981469674004 Seed: 20619\n",
      "Testing: 0.6568992104186827 Training: 0.642430473347093 Seed: 20623\n",
      "Testing: 0.6477187774397493 Training: 0.6447688460167248 Seed: 20625\n",
      "Testing: 0.6470305562278457 Training: 0.6449643647673735 Seed: 20627\n",
      "Testing: 0.652163897210169 Training: 0.6436645980035806 Seed: 20628\n",
      "Testing: 0.6462337322400233 Training: 0.6451989997234461 Seed: 20629\n",
      "Testing: 0.6489673745908239 Training: 0.6444024795871256 Seed: 20630\n",
      "Testing: 0.6473711250005003 Training: 0.6448941689683052 Seed: 20633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0.6468981007891894 Training: 0.6449010710848038 Seed: 20637\n",
      "Testing: 0.6457753217823412 Training: 0.6451718115646661 Seed: 20639\n",
      "Testing: 0.6537219458382012 Training: 0.6433385266825694 Seed: 20640\n"
     ]
    }
   ],
   "source": [
    "# Lets check the best score possible\n",
    "for i in range(1,20641):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "                                                       label,\n",
    "                                                       test_size = 0.2,\n",
    "                                                        random_state=i)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_score=model.score(X_train, y_train)\n",
    "    test_score=model.score(X_test, y_test)\n",
    "    if test_score > train_score:\n",
    "        print(\"Testing: {} Training: {} Seed: {}\".format(test_score, train_score, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66733.46172571507\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "print(sqrt(mean_squared_error(y_test, model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Perform Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model1 = DecisionTreeClassifier()\n",
    "model1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64333852668256941"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6537219458382012"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66733.46172571507\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "print(sqrt(mean_squared_error(y_test, model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Perform Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=67,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=11, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model2 = RandomForestRegressor(n_estimators=11, max_depth=67)\n",
    "model2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96605427463478111"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80555632607507277"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.score(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
